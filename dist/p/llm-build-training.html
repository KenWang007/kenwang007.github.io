<!DOCTYPE html>
<html lang="zh-CN" class="page-article">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="🏗️ 大模型架构、训练与部署 - 关键词: 架构, 大模型架构, 训练与部署">
    <meta name="keywords" content="架构, 大模型架构, 训练与部署">
    <meta name="author" content="Ken Wang">
    <meta name="robots" content="index, follow">
    
    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://kenwang007.github.io/dist/p/llm-build-training.html">
    <meta property="og:title" content="🏗️ 大模型架构、训练与部署 - Ken的知识库">
    <meta property="og:description" content="🏗️ 大模型架构、训练与部署 - 关键词: 架构, 大模型架构, 训练与部署">
    
    <!-- Twitter -->
    <meta property="twitter:card" content="summary">
    <meta property="twitter:url" content="https://kenwang007.github.io/dist/p/llm-build-training.html">
    <meta property="twitter:title" content="🏗️ 大模型架构、训练与部署 - Ken的知识库">
    <meta property="twitter:description" content="🏗️ 大模型架构、训练与部署 - 关键词: 架构, 大模型架构, 训练与部署">
    
    <!-- Theme Color -->
    <meta name="theme-color" content="#6366f1">
    
    <title>🏗️ 大模型架构、训练与部署 - Ken的知识库</title>
    <link rel="stylesheet" href="/style.css?v=2.1.2">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22 fill=%22%236366f1%22>📚</text></svg>">
    <link rel="canonical" href="https://kenwang007.github.io/dist/p/llm-build-training.html">
    <link rel="manifest" href="/manifest.json">
    
    <!-- Highlight.js for code syntax highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    
    <!-- RSS Feed -->
    <link rel="alternate" type="application/rss+xml" title="Ken的知识库 RSS Feed" href="/rss.xml">
    
    <!-- Breadcrumb Navigation -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "BreadcrumbList",
      "itemListElement": [
        {
          "@type": "ListItem",
          "position": 1,
          "name": "首页",
          "item": "https://kenwang007.github.io/"
        },
        {
          "@type": "ListItem",
          "position": 2,
          "name": "🏗️ 大模型架构、训练与部署",
          "item": "https://kenwang007.github.io/dist/p/llm-build-training.html"
        }
      ]
    }
    </script>
    
    <!-- Article Structured Data -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "🏗️ 大模型架构、训练与部署",
      "description": "🏗️ 大模型架构、训练与部署 - 关键词: 架构, 大模型架构, 训练与部署",
      "author": {
        "@type": "Person",
        "name": "Ken Wang"
      },
      "datePublished": "2026-01-29T20:26:18.610626",
      "dateModified": "2026-01-29T20:26:18.610626",
      "inLanguage": "zh-CN"
    }
    </script>
</head>
<body>
    <!-- 星空背景 -->
    <div class="stars"></div>
    <div class="stars2"></div>
    <div class="stars3"></div>

    <!-- 顶部固定导航 -->
    <header class="top-nav">
        <div class="nav-container">
            <div class="logo">
                <a href="/index.html">
                    <span class="logo-text">📚 Ken的知识库</span>
                </a>
            </div>
            <nav class="main-nav">
                <ul id="nav-menu" class="nav-menu">
                    <!-- 动态生成的导航菜单项 -->
                </ul>
            </nav>
        </div>
    </header>

    <!-- 主内容区域 -->
    <main class="main-content">
        <!-- 左侧固定关键词索引 -->
        <aside class="keyword-sidebar">
            <div class="keyword-header">
                <h3>关键词索引</h3>
            </div>
            <div class="keyword-list" id="keyword-list">
                <!-- 动态生成的关键词 -->
            </div>
        </aside>

        <!-- 中间主内容 -->
        <section class="content-area">
            <div class="content-wrapper">
                <article class="markdown-content">
                    
<h1 id="大模型架构训练与部署">🏗️ 大模型架构、训练与部署</h1>
<h2 id="大模型架构与训练">1. 大模型架构与训练</h2>
<h3 id="transformer-架构详解">1.1 Transformer 架构详解</h3>
<p>Transformer 是现代大语言模型的基础架构，由 Google 在 2017
年的论文《Attention Is All You Need》中提出。</p>
<pre><code>┌─────────────────────────────────────────────────────────┐
│                  Transformer 架构                       │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  ┌─────────────────┐      ┌─────────────────┐          │
│  │    Encoder      │      │    Decoder      │          │
│  │  (理解输入)      │  →   │  (生成输出)      │          │
│  └─────────────────┘      └─────────────────┘          │
│                                                         │
│  每个 Block 包含：                                       │
│  ┌─────────────────────────────────────┐               │
│  │  Multi-Head Self-Attention          │ ← 捕捉依赖关系 │
│  │           ↓                         │               │
│  │  Add &amp; Norm (残差连接 + 层归一化)     │               │
│  │           ↓                         │               │
│  │  Feed Forward Network               │ ← 特征变换    │
│  │           ↓                         │               │
│  │  Add &amp; Norm                         │               │
│  └─────────────────────────────────────┘               │
│                                                         │
└─────────────────────────────────────────────────────────┘</pre>
<h3 id="三种主流架构">1.2 三种主流架构</h3>
<table>
<colgroup>
<col style="width: 27%" />
<col style="width: 27%" />
<col style="width: 16%" />
<col style="width: 27%" />
</colgroup>
<thead>
<tr>
<th>架构类型</th>
<th>代表模型</th>
<th>特点</th>
<th>适用任务</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Encoder-Only</strong></td>
<td>BERT, RoBERTa</td>
<td>双向注意力，理解能力强</td>
<td>文本分类、NER、问答</td>
</tr>
<tr>
<td><strong>Decoder-Only</strong></td>
<td>GPT, LLaMA, Qwen</td>
<td>自回归生成，单向注意力</td>
<td>文本生成、对话</td>
</tr>
<tr>
<td><strong>Encoder-Decoder</strong></td>
<td>T5, BART</td>
<td>编码理解 + 解码生成</td>
<td>翻译、摘要</td>
</tr>
</tbody>
</table>
<pre><code>架构对比：

Encoder-Only (BERT):
  输入: [CLS] 我 爱 北京 [SEP]
        ←────────────────────→  (双向注意力)
  
Decoder-Only (GPT):
  输入: 我 爱 北京
        →→→→→→→→→→  (单向注意力，只看左边)
        
Encoder-Decoder (T5):
  Encoder: [理解输入] ──→ Decoder: [生成输出]</pre>
<h3 id="大模型训练流程">1.3 大模型训练流程</h3>
<pre><code>┌─────────────────────────────────────────────────────────┐
│                大模型训练三阶段                          │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  阶段一：预训练 (Pre-training)                          │
│  ┌─────────────────────────────────────────────────┐   │
│  │  数据：TB级互联网文本（网页、书籍、代码...）         │   │
│  │  目标：Next Token Prediction（预测下一个词）        │   │
│  │  规模：数千GPU，训练数周到数月                       │   │
│  │  产出：Base Model（基座模型）                       │   │
│  └─────────────────────────────────────────────────┘   │
│                          ↓                              │
│  阶段二：监督微调 (Supervised Fine-Tuning, SFT)         │
│  ┌─────────────────────────────────────────────────┐   │
│  │  数据：高质量指令-回答对（10万~100万条）            │   │
│  │  目标：学习遵循指令、生成有帮助的回答              │   │
│  │  产出：SFT Model                                   │   │
│  └─────────────────────────────────────────────────┘   │
│                          ↓                              │
│  阶段三：对齐训练 (Alignment)                           │
│  ┌─────────────────────────────────────────────────┐   │
│  │  方法：RLHF / DPO / RLAIF                          │   │
│  │  目标：与人类偏好对齐，安全、有帮助、诚实           │   │
│  │  产出：Chat Model（可对话的模型）                   │   │
│  └─────────────────────────────────────────────────┘   │
│                                                         │
└─────────────────────────────────────────────────────────┘</pre>
<h3 id="预训练关键技术">1.4 预训练关键技术</h3>
<h4 id="注意力机制优化">1.4.1 注意力机制优化</h4>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th>技术</th>
<th>原理</th>
<th>优势</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Multi-Head Attention</strong></td>
<td>多个注意力头并行计算</td>
<td>捕捉不同子空间的信息</td>
</tr>
<tr>
<td><strong>Flash Attention</strong></td>
<td>IO-aware 精确注意力算法</td>
<td>显存减少，速度提升 2-4x</td>
</tr>
<tr>
<td><strong>GQA (Grouped Query)</strong></td>
<td>KV Cache 分组共享</td>
<td>推理效率提升</td>
</tr>
<tr>
<td><strong>MQA (Multi-Query)</strong></td>
<td>所有头共享 KV</td>
<td>极致推理效率</td>
</tr>
<tr>
<td><strong>Sliding Window</strong></td>
<td>局部注意力窗口</td>
<td>支持更长上下文</td>
</tr>
</tbody>
</table>
<h4 id="位置编码">1.4.2 位置编码</h4>
<pre><code class="language-python"># 主流位置编码方案

位置编码方案对比：
┌────────────────┬─────────────────────────────────────┐
│ 方案           │ 特点                                 │
├────────────────┼─────────────────────────────────────┤
│ 绝对位置编码    │ 简单，但外推能力差                   │
│ (Sinusoidal)   │ 用于原始 Transformer                │
├────────────────┼─────────────────────────────────────┤
│ 可学习位置编码  │ GPT 系列采用                        │
│ (Learned)      │ 灵活但泛化受限于训练长度             │
├────────────────┼─────────────────────────────────────┤
│ RoPE           │ LLaMA/Qwen 采用                     │
│ (旋转位置编码)  │ 外推能力强，支持长文本               │
├────────────────┼─────────────────────────────────────┤
│ ALiBi          │ BLOOM 采用                          │
│ (线性偏置)      │ 无需训练，外推能力好                │
└────────────────┴─────────────────────────────────────┘</pre>
<h4 id="训练优化技术">1.4.3 训练优化技术</h4>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th>技术</th>
<th>作用</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>混合精度训练 (FP16/BF16)</strong></td>
<td>减少显存，加速计算</td>
<td>BF16 数值稳定性更好</td>
</tr>
<tr>
<td><strong>梯度累积</strong></td>
<td>模拟更大 batch size</td>
<td>显存受限时使用</td>
</tr>
<tr>
<td><strong>梯度检查点</strong></td>
<td>用计算换显存</td>
<td>重计算部分激活值</td>
</tr>
<tr>
<td><strong>ZeRO 优化</strong></td>
<td>分布式显存优化</td>
<td>DeepSpeed 三阶段</td>
</tr>
<tr>
<td><strong>张量并行 (TP)</strong></td>
<td>切分模型层</td>
<td>单机多卡</td>
</tr>
<tr>
<td><strong>流水线并行 (PP)</strong></td>
<td>切分模型层序列</td>
<td>多机训练</td>
</tr>
<tr>
<td><strong>数据并行 (DP)</strong></td>
<td>数据分片</td>
<td>最基础的并行</td>
</tr>
</tbody>
</table>
<h3 id="预训练代码示例">1.5 预训练代码示例</h3>
<pre><code class="language-python"># 使用 Hugging Face Transformers 预训练示例
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset

# 1. 加载模型和分词器
model_name = &quot;gpt2&quot;  # 或自定义架构
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# 2. 加载数据集
dataset = load_dataset(&quot;wikitext&quot;, &quot;wikitext-2-raw-v1&quot;)

# 3. 数据预处理
def tokenize_function(examples):
    return tokenizer(examples[&quot;text&quot;], truncation=True, max_length=512)

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# 4. 训练配置
training_args = TrainingArguments(
    output_dir=&quot;./results&quot;,
    num_train_epochs=3,
    per_device_train_batch_size=8,
    gradient_accumulation_steps=4,
    learning_rate=5e-5,
    warmup_steps=500,
    weight_decay=0.01,
    fp16=True,  # 混合精度
    logging_steps=100,
    save_steps=1000,
)

# 5. 数据整理器
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=False  # CLM 任务
)

# 6. 训练
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset[&quot;train&quot;],
    data_collator=data_collator,
)

trainer.train()</pre>
<hr />
<h2 id="大模型对齐与优化技术">2. 大模型对齐与优化技术</h2>
<h3 id="什么是对齐alignment">2.1 什么是对齐（Alignment）？</h3>
<p>对齐是指让模型的输出符合人类的期望和价值观，包括： -
<strong>有帮助（Helpful）</strong>：提供有用、准确的信息 -
<strong>诚实（Honest）</strong>：不编造信息，承认不确定性 -
<strong>无害（Harmless）</strong>：不产生有害、偏见内容</p>
<h3 id="rlhf基于人类反馈的强化学习">2.2
RLHF（基于人类反馈的强化学习）</h3>
<pre><code>┌─────────────────────────────────────────────────────────┐
│                    RLHF 流程                            │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Step 1: 收集人类偏好数据                               │
│  ┌─────────────────────────────────────────────────┐   │
│  │  Prompt → 模型生成多个回答 → 人类标注排序        │   │
│  │  例：回答A &gt; 回答B &gt; 回答C                       │   │
│  └─────────────────────────────────────────────────┘   │
│                          ↓                              │
│  Step 2: 训练奖励模型 (Reward Model)                    │
│  ┌─────────────────────────────────────────────────┐   │
│  │  输入：(prompt, response) → 输出：奖励分数        │   │
│  │  学习预测人类偏好                                 │   │
│  └─────────────────────────────────────────────────┘   │
│                          ↓                              │
│  Step 3: PPO 强化学习优化                              │
│  ┌─────────────────────────────────────────────────┐   │
│  │  策略模型生成回答 → 奖励模型打分 → 更新策略       │   │
│  │  + KL 散度约束（防止偏离原模型太远）              │   │
│  └─────────────────────────────────────────────────┘   │
│                                                         │
└─────────────────────────────────────────────────────────┘</pre>
<h3 id="dpo直接偏好优化">2.3 DPO（直接偏好优化）</h3>
<p>DPO 是 RLHF 的简化替代方案，无需单独训练奖励模型。</p>
<pre><code class="language-python"># DPO 核心思想
&quot;&quot;&quot;
RLHF: SFT Model → Reward Model → PPO → Aligned Model
DPO:  SFT Model → 直接优化 → Aligned Model

DPO 损失函数：
L_DPO = -log σ(β * (log π(y_w|x) - log π(y_l|x) 
                    - log π_ref(y_w|x) + log π_ref(y_l|x)))

其中：
- y_w: 偏好的回答 (winner)
- y_l: 不偏好的回答 (loser)
- π: 当前策略
- π_ref: 参考策略（SFT模型）
- β: 温度参数
&quot;&quot;&quot;</pre>
<table>
<thead>
<tr>
<th>方法</th>
<th>优势</th>
<th>劣势</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>RLHF</strong></td>
<td>效果好，业界验证充分</td>
<td>复杂，需要训练奖励模型</td>
</tr>
<tr>
<td><strong>DPO</strong></td>
<td>简单，无需奖励模型</td>
<td>对数据质量要求高</td>
</tr>
<tr>
<td><strong>RLAIF</strong></td>
<td>用AI代替人类标注</td>
<td>依赖辅助模型质量</td>
</tr>
</tbody>
</table>
<h3 id="提示工程prompt-engineering">2.4 提示工程（Prompt
Engineering）</h3>
<p>提示工程是一种无需训练即可优化模型输出的技术。</p>
<h4 id="核心技巧">2.4.1 核心技巧</h4>
<pre><code>┌─────────────────────────────────────────────────────────┐
│                提示工程技巧大全                          │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  🎭 角色设定 (Role Prompting)                           │
│  &quot;你是一位资深的Python开发专家，擅长代码优化...&quot;          │
│                                                         │
│  📝 指令清晰化                                          │
│  &quot;请用中文回答，分点列出，每点不超过50字&quot;                │
│                                                         │
│  📋 格式约束                                            │
│  &quot;请以JSON格式返回结果，包含name、age、city字段&quot;         │
│                                                         │
│  💡 思维链 (Chain of Thought, CoT)                      │
│  &quot;让我们一步一步思考这个问题...&quot;                         │
│                                                         │
│  📚 少样本学习 (Few-shot Learning)                      │
│  &quot;示例1：输入XX → 输出YY                                │
│   示例2：输入AA → 输出BB                                │
│   现在请处理：输入CC → ?&quot;                               │
│                                                         │
│  🔄 自我一致性 (Self-Consistency)                       │
│  多次采样，投票选择最一致的答案                          │
│                                                         │
│  🌳 思维树 (Tree of Thoughts)                           │
│  探索多条推理路径，评估选择最优                          │
│                                                         │
└─────────────────────────────────────────────────────────┘</pre>
<h4 id="prompt-模板示例">2.4.2 Prompt 模板示例</h4>
<pre><code class="language-python"># 系统级 Prompt 模板
SYSTEM_PROMPT = &quot;&quot;&quot;
你是一个专业的{role}，具有以下特点：
1. {characteristic_1}
2. {characteristic_2}
3. {characteristic_3}

在回答问题时，请遵循以下原则：
- {principle_1}
- {principle_2}
- {principle_3}

输出格式要求：
{output_format}
&quot;&quot;&quot;

# Chain of Thought 模板
COT_PROMPT = &quot;&quot;&quot;
问题：{question}

请按以下步骤思考：
1. 首先，理解问题的核心是什么
2. 然后，分析已知条件
3. 接着，制定解决方案
4. 最后，给出答案

让我们开始：
&quot;&quot;&quot;

# Few-shot 模板
FEW_SHOT_PROMPT = &quot;&quot;&quot;
任务：{task_description}

示例1：
输入：{example_1_input}
输出：{example_1_output}

示例2：
输入：{example_2_input}
输出：{example_2_output}

现在请处理：
输入：{actual_input}
输出：
&quot;&quot;&quot;</pre>
<h4 id="高级-prompt-技术">2.4.3 高级 Prompt 技术</h4>
<table>
<thead>
<tr>
<th>技术</th>
<th>说明</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ReAct</strong></td>
<td>推理+行动交替</td>
<td>Agent 任务</td>
</tr>
<tr>
<td><strong>Reflexion</strong></td>
<td>自我反思改进</td>
<td>复杂推理</td>
</tr>
<tr>
<td><strong>Plan-and-Solve</strong></td>
<td>先规划后执行</td>
<td>多步骤任务</td>
</tr>
<tr>
<td><strong>Least-to-Most</strong></td>
<td>从简到繁分解</td>
<td>复杂问题</td>
</tr>
<tr>
<td><strong>Skeleton-of-Thought</strong></td>
<td>先骨架后填充</td>
<td>长文本生成</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="大模型微调">3. 大模型微调</h2>
<h3 id="微调方法概览">3.1 微调方法概览</h3>
<pre><code>┌─────────────────────────────────────────────────────────┐
│                  微调方法谱系                           │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  按参数更新范围：                                        │
│                                                         │
│  全参数微调 (Full Fine-tuning)                          │
│  ├── 更新所有参数                                       │
│  ├── 效果最好，但成本最高                               │
│  └── 需要大量显存和计算资源                              │
│                                                         │
│  参数高效微调 (PEFT)                                    │
│  ├── LoRA：低秩适配                                     │
│  ├── QLoRA：量化 + LoRA                                 │
│  ├── Prefix Tuning：前缀调优                            │
│  ├── P-Tuning v2：深度提示调优                          │
│  ├── Adapter：适配器层                                  │
│  └── IA3：抑制和放大内部激活                            │
│                                                         │
└─────────────────────────────────────────────────────────┘</pre>
<h3 id="lora-详解">3.2 LoRA 详解</h3>
<p>LoRA (Low-Rank Adaptation) 是最流行的参数高效微调方法。</p>
<pre><code>┌─────────────────────────────────────────────────────────┐
│                    LoRA 原理                            │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  原始权重矩阵 W (d × k)                                  │
│                                                         │
│       ┌───────────────┐                                │
│  x ──→│   W (冻结)     │──→ y                           │
│       └───────────────┘                                │
│              +                                          │
│       ┌─────┐   ┌─────┐                                │
│  x ──→│  A  │──→│  B  │──→ Δy                          │
│       └─────┘   └─────┘                                │
│       (d × r)   (r × k)                                │
│                                                         │
│  最终输出：y&#39; = Wx + BAx                                │
│                                                         │
│  核心思想：                                              │
│  - 冻结原始权重 W                                        │
│  - 新增低秩矩阵 A、B (r &lt;&lt; d, k)                         │
│  - 只训练 A、B，参数量大幅减少                           │
│  - 例：r=8 时，参数量减少 ~10000 倍                      │
│                                                         │
└─────────────────────────────────────────────────────────┘</pre>
<h3 id="微调实践代码">3.3 微调实践代码</h3>
<h4 id="使用-peft-lora-微调">3.3.1 使用 PEFT + LoRA 微调</h4>
<pre><code class="language-python">from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model, TaskType
from trl import SFTTrainer
from datasets import load_dataset

# 1. 加载基座模型
model_name = &quot;meta-llama/Llama-2-7b-hf&quot;
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map=&quot;auto&quot;
)
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# 2. 配置 LoRA
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,                      # 低秩维度
    lora_alpha=32,            # 缩放因子
    lora_dropout=0.1,         # Dropout
    target_modules=[          # 要适配的模块
        &quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;,
        &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;
    ],
)

# 3. 应用 LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# 输出：trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622

# 4. 准备数据集
dataset = load_dataset(&quot;json&quot;, data_files=&quot;train_data.json&quot;)

def format_instruction(example):
    return f&quot;&quot;&quot;### 指令：
{example[&#39;instruction&#39;]}

### 回答：
{example[&#39;output&#39;]}&quot;&quot;&quot;

# 5. 训练配置
training_args = TrainingArguments(
    output_dir=&quot;./lora_output&quot;,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    save_strategy=&quot;epoch&quot;,
    warmup_ratio=0.03,
)

# 6. 训练
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset[&quot;train&quot;],
    formatting_func=format_instruction,
    max_seq_length=512,
)

trainer.train()

# 7. 保存 LoRA 权重
model.save_pretrained(&quot;./lora_weights&quot;)</pre>
<h4 id="使用-qlora-微调显存更省">3.3.2 使用 QLoRA 微调（显存更省）</h4>
<pre><code class="language-python">from transformers import BitsAndBytesConfig
import torch

# 4-bit 量化配置
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

# 加载量化模型
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map=&quot;auto&quot;
)

# 后续与 LoRA 相同...</pre>
<h3 id="微调方法对比">3.4 微调方法对比</h3>
<table>
<thead>
<tr>
<th>方法</th>
<th>可训练参数</th>
<th>显存需求</th>
<th>效果</th>
<th>推荐场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Full Fine-tuning</strong></td>
<td>100%</td>
<td>极高</td>
<td>最好</td>
<td>充足资源</td>
</tr>
<tr>
<td><strong>LoRA</strong></td>
<td>~0.1%</td>
<td>中等</td>
<td>很好</td>
<td>通用场景</td>
</tr>
<tr>
<td><strong>QLoRA</strong></td>
<td>~0.1%</td>
<td>低</td>
<td>较好</td>
<td>显存受限</td>
</tr>
<tr>
<td><strong>Prefix Tuning</strong></td>
<td>~0.01%</td>
<td>低</td>
<td>一般</td>
<td>简单任务</td>
</tr>
<tr>
<td><strong>Adapter</strong></td>
<td>~1%</td>
<td>中低</td>
<td>较好</td>
<td>多任务</td>
</tr>
</tbody>
</table>
<h3 id="微调数据格式">3.5 微调数据格式</h3>
<pre><code class="language-python"># Alpaca 格式（最常用）
{
    &quot;instruction&quot;: &quot;将以下句子翻译成英文&quot;,
    &quot;input&quot;: &quot;今天天气很好&quot;,
    &quot;output&quot;: &quot;The weather is nice today.&quot;
}

# ShareGPT 格式（多轮对话）
{
    &quot;conversations&quot;: [
        {&quot;from&quot;: &quot;human&quot;, &quot;value&quot;: &quot;你好&quot;},
        {&quot;from&quot;: &quot;gpt&quot;, &quot;value&quot;: &quot;你好！有什么我可以帮助你的吗？&quot;},
        {&quot;from&quot;: &quot;human&quot;, &quot;value&quot;: &quot;介绍一下北京&quot;},
        {&quot;from&quot;: &quot;gpt&quot;, &quot;value&quot;: &quot;北京是中国的首都...&quot;}
    ]
}

# OpenAI 格式
{
    &quot;messages&quot;: [
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;你是一个有帮助的助手&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你好&quot;},
        {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;你好！有什么可以帮你的？&quot;}
    ]
}</pre>
<hr />
<h2 id="大模型推理与部署">4. 大模型推理与部署</h2>
<h3 id="推理优化技术">4.1 推理优化技术</h3>
<pre><code>┌─────────────────────────────────────────────────────────┐
│                 推理优化技术栈                          │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  🔢 模型压缩                                            │
│  ├── 量化 (Quantization)                               │
│  │   ├── INT8：精度损失小，推理快 2x                    │
│  │   ├── INT4：精度有损，显存省 4x                      │
│  │   └── GPTQ/AWQ/GGUF：不同量化方案                   │
│  ├── 剪枝 (Pruning)                                    │
│  │   └── 移除不重要的参数                               │
│  └── 蒸馏 (Distillation)                               │
│      └── 大模型知识迁移到小模型                          │
│                                                         │
│  ⚡ 推理加速                                            │
│  ├── KV Cache：缓存注意力键值对                         │
│  ├── Flash Attention：高效注意力计算                   │
│  ├── Continuous Batching：动态批处理                   │
│  ├── Speculative Decoding：推测解码                    │
│  └── PagedAttention：分页注意力 (vLLM)                 │
│                                                         │
│  🖥️ 硬件优化                                           │
│  ├── TensorRT：NVIDIA GPU 优化                         │
│  ├── ONNX Runtime：跨平台加速                          │
│  └── 算子融合：减少内存访问                             │
│                                                         │
└─────────────────────────────────────────────────────────┘</pre>
<h3 id="量化详解">4.2 量化详解</h3>
<table>
<thead>
<tr>
<th>量化格式</th>
<th>位数</th>
<th>显存节省</th>
<th>精度损失</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>FP16</strong></td>
<td>16-bit</td>
<td>2x</td>
<td>几乎无</td>
<td>训练/推理</td>
</tr>
<tr>
<td><strong>BF16</strong></td>
<td>16-bit</td>
<td>2x</td>
<td>几乎无</td>
<td>训练/推理</td>
</tr>
<tr>
<td><strong>INT8</strong></td>
<td>8-bit</td>
<td>4x</td>
<td>很小</td>
<td>推理</td>
</tr>
<tr>
<td><strong>INT4</strong></td>
<td>4-bit</td>
<td>8x</td>
<td>较小</td>
<td>推理</td>
</tr>
<tr>
<td><strong>GPTQ</strong></td>
<td>4-bit</td>
<td>8x</td>
<td>小</td>
<td>GPU 推理</td>
</tr>
<tr>
<td><strong>AWQ</strong></td>
<td>4-bit</td>
<td>8x</td>
<td>更小</td>
<td>GPU 推理</td>
</tr>
<tr>
<td><strong>GGUF</strong></td>
<td>2-8bit</td>
<td>可变</td>
<td>可变</td>
<td>CPU/混合推理</td>
</tr>
</tbody>
</table>
<pre><code class="language-python"># 使用 bitsandbytes 量化加载
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# INT8 量化
model_8bit = AutoModelForCausalLM.from_pretrained(
    &quot;meta-llama/Llama-2-7b-hf&quot;,
    load_in_8bit=True,
    device_map=&quot;auto&quot;
)

# INT4 量化
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=torch.float16,
)
model_4bit = AutoModelForCausalLM.from_pretrained(
    &quot;meta-llama/Llama-2-7b-hf&quot;,
    quantization_config=bnb_config,
    device_map=&quot;auto&quot;
)</pre>
<h3 id="部署方案">4.3 部署方案</h3>
<h4 id="部署框架对比">4.3.1 部署框架对比</h4>
<table>
<thead>
<tr>
<th>框架</th>
<th>特点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>vLLM</strong></td>
<td>PagedAttention，吞吐量高</td>
<td>高并发在线服务</td>
</tr>
<tr>
<td><strong>TGI</strong></td>
<td>HuggingFace 官方，功能全面</td>
<td>生产环境</td>
</tr>
<tr>
<td><strong>Ollama</strong></td>
<td>简单易用，本地部署</td>
<td>个人/开发</td>
</tr>
<tr>
<td><strong>llama.cpp</strong></td>
<td>纯 CPU 推理，跨平台</td>
<td>边缘设备</td>
</tr>
<tr>
<td><strong>TensorRT-LLM</strong></td>
<td>NVIDIA 优化，极致性能</td>
<td>高性能 GPU</td>
</tr>
<tr>
<td><strong>OpenLLM</strong></td>
<td>灵活，支持多模型</td>
<td>模型服务化</td>
</tr>
</tbody>
</table>
<h4 id="vllm-部署示例">4.3.2 vLLM 部署示例</h4>
<pre><code class="language-python"># 安装：pip install vllm

from vllm import LLM, SamplingParams

# 1. 加载模型
llm = LLM(
    model=&quot;meta-llama/Llama-2-7b-chat-hf&quot;,
    tensor_parallel_size=1,  # GPU 数量
    dtype=&quot;float16&quot;,
    max_model_len=4096,
)

# 2. 设置采样参数
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.9,
    max_tokens=512,
)

# 3. 推理
prompts = [&quot;请介绍一下人工智能&quot;, &quot;Python有什么优点？&quot;]
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(output.outputs[0].text)</pre>
<pre><code class="language-bash"># vLLM 启动 API 服务
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --port 8000 \
    --tensor-parallel-size 1</pre>
<h4 id="ollama-本地部署">4.3.3 Ollama 本地部署</h4>
<pre><code class="language-bash"># 安装 Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 下载并运行模型
ollama pull llama3
ollama run llama3

# 使用 API
curl http://localhost:11434/api/generate -d &#39;{
  &quot;model&quot;: &quot;llama3&quot;,
  &quot;prompt&quot;: &quot;什么是大语言模型？&quot;
}&#39;</pre>
<pre><code class="language-python"># Python 调用 Ollama
import ollama

response = ollama.chat(
    model=&#39;llama3&#39;,
    messages=[{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;你好&#39;}]
)
print(response[&#39;message&#39;][&#39;content&#39;])</pre>
<h3 id="部署架构设计">4.4 部署架构设计</h3>
<pre><code>┌─────────────────────────────────────────────────────────┐
│              生产级 LLM 部署架构                         │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  用户请求                                               │
│      ↓                                                  │
│  ┌─────────────┐                                       │
│  │   API 网关   │  ← 认证、限流、路由                    │
│  └──────┬──────┘                                       │
│         │                                              │
│         ↓                                              │
│  ┌─────────────┐                                       │
│  │  负载均衡   │  ← 分发请求到不同实例                   │
│  └──────┬──────┘                                       │
│         │                                              │
│    ┌────┴────┐                                         │
│    ↓         ↓                                         │
│ ┌──────┐ ┌──────┐                                     │
│ │推理1 │ │推理2 │  ← vLLM/TGI 实例                     │
│ │(GPU) │ │(GPU) │                                     │
│ └──────┘ └──────┘                                     │
│    ↑         ↑                                         │
│    └────┬────┘                                         │
│         │                                              │
│  ┌─────────────┐                                       │
│  │  模型存储   │  ← S3/本地/模型仓库                    │
│  └─────────────┘                                       │
│                                                         │
│  监控组件：Prometheus + Grafana                         │
│  日志组件：ELK Stack                                    │
│                                                         │
└─────────────────────────────────────────────────────────┘</pre>
<hr />
<h2 id="其他训练与优化相关知识">5. 其他训练与优化相关知识</h2>
<h3 id="数据工程">5.1 数据工程</h3>
<h4 id="数据质量的重要性">5.1.1 数据质量的重要性</h4>
<pre><code>数据质量金字塔：

        ┌─────────┐
        │ 高质量  │  ← 人工标注、专家校验
        │  数据   │     效果最好，成本最高
        ├─────────┤
        │ 合成数据 │  ← GPT-4 生成、自我指令
        │         │     平衡效果与成本
        ├─────────┤
        │ 网络数据 │  ← CommonCrawl、网页爬取
        │         │     量大质低，需要清洗
        └─────────┘</pre>
<h4 id="数据处理流程">5.1.2 数据处理流程</h4>
<pre><code class="language-python"># 数据处理 Pipeline
data_pipeline = {
    &quot;1. 数据收集&quot;: [
        &quot;爬取网页数据&quot;,
        &quot;收集开源数据集&quot;,
        &quot;生成合成数据&quot;
    ],
    &quot;2. 数据清洗&quot;: [
        &quot;去重（MinHash、SimHash）&quot;,
        &quot;过滤低质量内容&quot;,
        &quot;移除有害/敏感信息&quot;,
        &quot;语言检测与分类&quot;
    ],
    &quot;3. 数据处理&quot;: [
        &quot;分词 Tokenization&quot;,
        &quot;格式标准化&quot;,
        &quot;长度裁剪/填充&quot;
    ],
    &quot;4. 数据增强&quot;: [
        &quot;回译增强&quot;,
        &quot;同义词替换&quot;,
        &quot;模型改写&quot;
    ],
    &quot;5. 质量评估&quot;: [
        &quot;困惑度评估&quot;,
        &quot;人工抽样检查&quot;,
        &quot;多样性分析&quot;
    ]
}</pre>
<h3 id="评估与测试">5.2 评估与测试</h3>
<h4 id="常用评估基准">5.2.1 常用评估基准</h4>
<table>
<thead>
<tr>
<th>基准</th>
<th>评估能力</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>MMLU</strong></td>
<td>多任务知识</td>
<td>57个学科的选择题</td>
</tr>
<tr>
<td><strong>HellaSwag</strong></td>
<td>常识推理</td>
<td>句子补全</td>
</tr>
<tr>
<td><strong>GSM8K</strong></td>
<td>数学推理</td>
<td>小学数学应用题</td>
</tr>
<tr>
<td><strong>HumanEval</strong></td>
<td>代码生成</td>
<td>Python 函数补全</td>
</tr>
<tr>
<td><strong>TruthfulQA</strong></td>
<td>真实性</td>
<td>检测幻觉</td>
</tr>
<tr>
<td><strong>MT-Bench</strong></td>
<td>对话能力</td>
<td>多轮对话评分</td>
</tr>
<tr>
<td><strong>C-Eval</strong></td>
<td>中文知识</td>
<td>中文多任务</td>
</tr>
<tr>
<td><strong>CMMLU</strong></td>
<td>中文 MMLU</td>
<td>中文版 MMLU</td>
</tr>
</tbody>
</table>
<h4 id="评估代码示例">5.2.2 评估代码示例</h4>
<pre><code class="language-python"># 使用 lm-evaluation-harness 评估
# pip install lm-eval

from lm_eval import evaluator
from lm_eval.models.huggingface import HFLM

# 加载模型
model = HFLM(
    pretrained=&quot;meta-llama/Llama-2-7b-hf&quot;,
    device=&quot;cuda&quot;
)

# 运行评估
results = evaluator.simple_evaluate(
    model=model,
    tasks=[&quot;hellaswag&quot;, &quot;mmlu&quot;, &quot;gsm8k&quot;],
    num_fewshot=5,
    batch_size=8
)

print(results[&quot;results&quot;])</pre>
<h3 id="常见问题与解决方案">5.3 常见问题与解决方案</h3>
<table>
<thead>
<tr>
<th>问题</th>
<th>原因</th>
<th>解决方案</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>训练不收敛</strong></td>
<td>学习率不当、数据问题</td>
<td>调整 LR、检查数据</td>
</tr>
<tr>
<td><strong>过拟合</strong></td>
<td>数据量不足、模型过大</td>
<td>数据增强、正则化</td>
</tr>
<tr>
<td><strong>灾难性遗忘</strong></td>
<td>微调破坏原有能力</td>
<td>LoRA、较小学习率</td>
</tr>
<tr>
<td><strong>幻觉</strong></td>
<td>训练数据有噪声</td>
<td>RAG 增强、RLHF</td>
</tr>
<tr>
<td><strong>推理慢</strong></td>
<td>模型大、硬件弱</td>
<td>量化、蒸馏、升级硬件</td>
</tr>
<tr>
<td><strong>显存不足</strong></td>
<td>模型/batch 太大</td>
<td>梯度累积、量化、LoRA</td>
</tr>
</tbody>
</table>
<h3 id="训练资源估算">5.4 训练资源估算</h3>
<pre><code class="language-python"># 显存估算公式（近似）
def estimate_memory(params_billion, precision=&quot;fp16&quot;, training=True):
    &quot;&quot;&quot;
    params_billion: 参数量（十亿）
    precision: fp32/fp16/int8/int4
    training: 是否训练
    &quot;&quot;&quot;
    bytes_per_param = {
        &quot;fp32&quot;: 4,
        &quot;fp16&quot;: 2,
        &quot;bf16&quot;: 2,
        &quot;int8&quot;: 1,
        &quot;int4&quot;: 0.5
    }
    
    # 模型权重
    model_memory = params_billion * bytes_per_param[precision]
    
    if training:
        # 训练时需要额外的优化器状态和梯度
        # AdamW: 参数 + 梯度 + 一阶动量 + 二阶动量 ≈ 4x
        # 加上激活值等，总计约 6-8x
        total_memory = model_memory * 6
    else:
        # 推理时主要是模型权重 + KV Cache
        total_memory = model_memory * 1.2
    
    return f&quot;{total_memory:.1f} GB&quot;

# 示例
print(estimate_memory(7, &quot;fp16&quot;, training=True))   # 约 84 GB
print(estimate_memory(7, &quot;fp16&quot;, training=False))  # 约 16.8 GB
print(estimate_memory(7, &quot;int4&quot;, training=False))  # 约 4.2 GB</pre>
<h3 id="训练工具生态">5.5 训练工具生态</h3>
<pre><code>┌─────────────────────────────────────────────────────────┐
│                  训练工具生态                           │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  🔧 训练框架                                            │
│  ├── Hugging Face Transformers                         │
│  ├── DeepSpeed                                         │
│  ├── Megatron-LM                                       │
│  ├── ColossalAI                                        │
│  └── LLaMA-Factory                                     │
│                                                         │
│  📊 实验管理                                            │
│  ├── Weights &amp; Biases                                  │
│  ├── MLflow                                            │
│  └── TensorBoard                                       │
│                                                         │
│  🗃️ 数据处理                                           │
│  ├── Datasets (HuggingFace)                            │
│  ├── Apache Spark                                      │
│  └── Dask                                              │
│                                                         │
│  ☁️ 云平台                                              │
│  ├── AWS SageMaker                                     │
│  ├── Google Vertex AI                                  │
│  ├── Azure ML                                          │
│  └── AutoDL / 恒源云（国内）                            │
│                                                         │
└─────────────────────────────────────────────────────────┘</pre>
<hr />
<h2 id="总结">6. 总结</h2>
<h3 id="核心要点">6.1 核心要点</h3>
<table>
<thead>
<tr>
<th>主题</th>
<th>关键点</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>架构与训练</strong></td>
<td>Transformer 架构、预训练三阶段、分布式训练</td>
</tr>
<tr>
<td><strong>对齐与优化</strong></td>
<td>RLHF/DPO 对齐、提示工程技巧</td>
</tr>
<tr>
<td><strong>微调技术</strong></td>
<td>LoRA/QLoRA 参数高效微调、数据格式</td>
</tr>
<tr>
<td><strong>推理部署</strong></td>
<td>量化压缩、vLLM/Ollama 部署、架构设计</td>
</tr>
</tbody>
</table>
<h3 id="学习路径">6.2 学习路径</h3>
<pre><code>基础 → 实践 → 深入

1️⃣ 基础理论
   • Transformer 架构原理
   • 注意力机制
   • 预训练目标

2️⃣ 动手实践
   • 使用 HuggingFace 微调模型
   • 尝试 LoRA/QLoRA
   • 本地部署 Ollama

3️⃣ 深入优化
   • 分布式训练
   • 推理优化
   • 生产部署</pre>
<h3 id="参考资源">6.3 参考资源</h3>
<ul>
<li><a href="https://arxiv.org/abs/1706.03762">《Attention Is All You
Need》</a></li>
<li><a href="https://arxiv.org/abs/2106.09685">《LoRA: Low-Rank
Adaptation》</a></li>
<li><a href="https://arxiv.org/abs/2203.02155">《Training language
models to follow instructions with human feedback》</a></li>
<li><a href="https://huggingface.co/docs">Hugging Face 文档</a></li>
<li><a href="https://github.com/vllm-project/vllm">vLLM 项目</a></li>
<li><a
href="https://github.com/hiyouga/LLaMA-Factory">LLaMA-Factory</a></li>
</ul>

                </article>
            </div>
        </section>

        <!-- 右侧热门文章 -->
        <aside class="popular-sidebar">
            <div class="popular-header">
                <h3>热门文章</h3>
            </div>
            <div class="popular-list" id="popular-list">
                <!-- 动态生成的热门文章列表 -->
            </div>
        </aside>
    </main>

    <!-- JavaScript -->
    <script type="module" src="/script.js?v=2.2.0"></script>
    
    <!-- Initialize Highlight.js -->
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // Highlight all code blocks
            document.querySelectorAll('pre code').forEach((block) => {
                hljs.highlightElement(block);
            });
        });
    </script>
</body>
</html>