<!DOCTYPE html>
<html lang="zh-CN" class="page-article">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="🏗️ 大模型架构、训练与部署 - 关键词: 架构, 大模型架构, 训练与部署">
    <meta name="keywords" content="架构, 大模型架构, 训练与部署">
    <meta name="author" content="Ken Wang">
    <meta name="robots" content="index, follow">
    
    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://kenwang007.github.io/dist/p/llm-build-training.html">
    <meta property="og:title" content="🏗️ 大模型架构、训练与部署 - Ken的知识库">
    <meta property="og:description" content="🏗️ 大模型架构、训练与部署 - 关键词: 架构, 大模型架构, 训练与部署">
    
    <!-- Twitter -->
    <meta property="twitter:card" content="summary">
    <meta property="twitter:url" content="https://kenwang007.github.io/dist/p/llm-build-training.html">
    <meta property="twitter:title" content="🏗️ 大模型架构、训练与部署 - Ken的知识库">
    <meta property="twitter:description" content="🏗️ 大模型架构、训练与部署 - 关键词: 架构, 大模型架构, 训练与部署">
    
    <!-- Theme Color -->
    <meta name="theme-color" content="#6366f1">
    
    <title>🏗️ 大模型架构、训练与部署 - Ken的知识库</title>
    <link rel="stylesheet" href="/style.css?v=2.1.2">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22 fill=%22%236366f1%22>📚</text></svg>">
    <link rel="canonical" href="https://kenwang007.github.io/dist/p/llm-build-training.html">
    <link rel="manifest" href="/manifest.json">
    
    <!-- Highlight.js for code syntax highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    
    <!-- RSS Feed -->
    <link rel="alternate" type="application/rss+xml" title="Ken的知识库 RSS Feed" href="/rss.xml">
    
    <!-- Breadcrumb Navigation -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "BreadcrumbList",
      "itemListElement": [
        {
          "@type": "ListItem",
          "position": 1,
          "name": "首页",
          "item": "https://kenwang007.github.io/"
        },
        {
          "@type": "ListItem",
          "position": 2,
          "name": "🏗️ 大模型架构、训练与部署",
          "item": "https://kenwang007.github.io/dist/p/llm-build-training.html"
        }
      ]
    }
    </script>
    
    <!-- Article Structured Data -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "🏗️ 大模型架构、训练与部署",
      "description": "🏗️ 大模型架构、训练与部署 - 关键词: 架构, 大模型架构, 训练与部署",
      "author": {
        "@type": "Person",
        "name": "Ken Wang"
      },
      "datePublished": "2026-01-29T20:26:18.610626",
      "dateModified": "2026-01-29T20:26:18.610626",
      "inLanguage": "zh-CN"
    }
    </script>
</head>
<body>
    <!-- 星空背景 -->
    <div class="stars"></div>
    <div class="stars2"></div>
    <div class="stars3"></div>

    <!-- 顶部固定导航 -->
    <header class="top-nav">
        <div class="nav-container">
            <div class="logo">
                <a href="/index.html">
                    <span class="logo-text">📚 Ken的知识库</span>
                </a>
            </div>
            <nav class="main-nav">
                <ul id="nav-menu" class="nav-menu">
                    <!-- 动态生成的导航菜单项 -->
                </ul>
            </nav>
        </div>
    </header>

    <!-- 主内容区域 -->
    <main class="main-content">
        <!-- 左侧固定关键词索引 -->
        <aside class="keyword-sidebar">
            <div class="keyword-header">
                <h3>关键词索引</h3>
            </div>
            <div class="keyword-list" id="keyword-list">
                <!-- 动态生成的关键词 -->
            </div>
        </aside>

        <!-- 中间主内容 -->
        <section class="content-area">
            <div class="content-wrapper">
                <article class="markdown-content">
                    
<h1 id="大模型架构训练与部署">🏗️ 大模型架构、训练与部署</h1>
<h2 id="大模型架构与训练">1. 大模型架构与训练</h2>
<h3 id="transformer-架构详解">1.1 Transformer 架构详解</h3>
<p>Transformer 是现代大语言模型的基础架构，由 Google 在 2017
年的论文《Attention Is All You Need》中提出。</p>
<pre><code>┌─────────────────────────────────────────────────────────┐
│                  Transformer 架构                       │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  ┌─────────────────┐      ┌─────────────────┐          │
│  │    Encoder      │      │    Decoder      │          │
│  │  (理解输入)      │  →   │  (生成输出)      │          │
│  └─────────────────┘      └─────────────────┘          │
│                                                         │
│  每个 Block 包含：                                       │
│  ┌─────────────────────────────────────┐               │
│  │  Multi-Head Self-Attention          │ ← 捕捉依赖关系 │
│  │           ↓                         │               │
│  │  Add &amp; Norm (残差连接 + 层归一化)     │               │
│  │           ↓                         │               │
│  │  Feed Forward Network               │ ← 特征变换    │
│  │           ↓                         │               │
│  │  Add &amp; Norm                         │               │
│  └─────────────────────────────────────┘               │
│                                                         │
└─────────────────────────────────────────────────────────┘</code></pre>
<h3 id="三种主流架构">1.2 三种主流架构</h3>
<table>
<colgroup>
<col style="width: 27%" />
<col style="width: 27%" />
<col style="width: 16%" />
<col style="width: 27%" />
</colgroup>
<thead>
<tr>
<th>架构类型</th>
<th>代表模型</th>
<th>特点</th>
<th>适用任务</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Encoder-Only</strong></td>
<td>BERT, RoBERTa</td>
<td>双向注意力，理解能力强</td>
<td>文本分类、NER、问答</td>
</tr>
<tr>
<td><strong>Decoder-Only</strong></td>
<td>GPT, LLaMA, Qwen</td>
<td>自回归生成，单向注意力</td>
<td>文本生成、对话</td>
</tr>
<tr>
<td><strong>Encoder-Decoder</strong></td>
<td>T5, BART</td>
<td>编码理解 + 解码生成</td>
<td>翻译、摘要</td>
</tr>
</tbody>
</table>
<pre><code>架构对比：

Encoder-Only (BERT):
  输入: [CLS] 我 爱 北京 [SEP]
        ←────────────────────→  (双向注意力)
  
Decoder-Only (GPT):
  输入: 我 爱 北京
        →→→→→→→→→→  (单向注意力，只看左边)
        
Encoder-Decoder (T5):
  Encoder: [理解输入] ──→ Decoder: [生成输出]</code></pre>
<h3 id="大模型训练流程">1.3 大模型训练流程</h3>
<pre><code>┌─────────────────────────────────────────────────────────┐
│                大模型训练三阶段                          │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  阶段一：预训练 (Pre-training)                          │
│  ┌─────────────────────────────────────────────────┐   │
│  │  数据：TB级互联网文本（网页、书籍、代码...）         │   │
│  │  目标：Next Token Prediction（预测下一个词）        │   │
│  │  规模：数千GPU，训练数周到数月                       │   │
│  │  产出：Base Model（基座模型）                       │   │
│  └─────────────────────────────────────────────────┘   │
│                          ↓                              │
│  阶段二：监督微调 (Supervised Fine-Tuning, SFT)         │
│  ┌─────────────────────────────────────────────────┐   │
│  │  数据：高质量指令-回答对（10万~100万条）            │   │
│  │  目标：学习遵循指令、生成有帮助的回答              │   │
│  │  产出：SFT Model                                   │   │
│  └─────────────────────────────────────────────────┘   │
│                          ↓                              │
│  阶段三：对齐训练 (Alignment)                           │
│  ┌─────────────────────────────────────────────────┐   │
│  │  方法：RLHF / DPO / RLAIF                          │   │
│  │  目标：与人类偏好对齐，安全、有帮助、诚实           │   │
│  │  产出：Chat Model（可对话的模型）                   │   │
│  └─────────────────────────────────────────────────┘   │
│                                                         │
└─────────────────────────────────────────────────────────┘</code></pre>
<h3 id="预训练关键技术">1.4 预训练关键技术</h3>
<h4 id="注意力机制优化">1.4.1 注意力机制优化</h4>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th>技术</th>
<th>原理</th>
<th>优势</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Multi-Head Attention</strong></td>
<td>多个注意力头并行计算</td>
<td>捕捉不同子空间的信息</td>
</tr>
<tr>
<td><strong>Flash Attention</strong></td>
<td>IO-aware 精确注意力算法</td>
<td>显存减少，速度提升 2-4x</td>
</tr>
<tr>
<td><strong>GQA (Grouped Query)</strong></td>
<td>KV Cache 分组共享</td>
<td>推理效率提升</td>
</tr>
<tr>
<td><strong>MQA (Multi-Query)</strong></td>
<td>所有头共享 KV</td>
<td>极致推理效率</td>
</tr>
<tr>
<td><strong>Sliding Window</strong></td>
<td>局部注意力窗口</td>
<td>支持更长上下文</td>
</tr>
</tbody>
</table>
<h4 id="位置编码">1.4.2 位置编码</h4>
<pre><code class="language-python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 主流位置编码方案</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>位置编码方案对比：</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>┌────────────────┬─────────────────────────────────────┐</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>│ 方案           │ 特点                                 │</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>├────────────────┼─────────────────────────────────────┤</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>│ 绝对位置编码    │ 简单，但外推能力差                   │</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>│ (Sinusoidal)   │ 用于原始 Transformer                │</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>├────────────────┼─────────────────────────────────────┤</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>│ 可学习位置编码  │ GPT 系列采用                        │</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>│ (Learned)      │ 灵活但泛化受限于训练长度             │</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>├────────────────┼─────────────────────────────────────┤</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>│ RoPE           │ LLaMA<span class="op">/</span>Qwen 采用                     │</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>│ (旋转位置编码)  │ 外推能力强，支持长文本               │</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>├────────────────┼─────────────────────────────────────┤</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>│ ALiBi          │ BLOOM 采用                          │</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>│ (线性偏置)      │ 无需训练，外推能力好                │</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>└────────────────┴─────────────────────────────────────┘</span></code></pre></div>
<h4 id="训练优化技术">1.4.3 训练优化技术</h4>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th>技术</th>
<th>作用</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>混合精度训练 (FP16/BF16)</strong></td>
<td>减少显存，加速计算</td>
<td>BF16 数值稳定性更好</td>
</tr>
<tr>
<td><strong>梯度累积</strong></td>
<td>模拟更大 batch size</td>
<td>显存受限时使用</td>
</tr>
<tr>
<td><strong>梯度检查点</strong></td>
<td>用计算换显存</td>
<td>重计算部分激活值</td>
</tr>
<tr>
<td><strong>ZeRO 优化</strong></td>
<td>分布式显存优化</td>
<td>DeepSpeed 三阶段</td>
</tr>
<tr>
<td><strong>张量并行 (TP)</strong></td>
<td>切分模型层</td>
<td>单机多卡</td>
</tr>
<tr>
<td><strong>流水线并行 (PP)</strong></td>
<td>切分模型层序列</td>
<td>多机训练</td>
</tr>
<tr>
<td><strong>数据并行 (DP)</strong></td>
<td>数据分片</td>
<td>最基础的并行</td>
</tr>
</tbody>
</table>
<h3 id="预训练代码示例">1.5 预训练代码示例</h3>
<pre><code class="language-python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 使用 Hugging Face Transformers 预训练示例</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> (</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    AutoModelForCausalLM,</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    AutoTokenizer,</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    TrainingArguments,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    Trainer,</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    DataCollatorForLanguageModeling</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. 加载模型和分词器</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">&quot;gpt2&quot;</span>  <span class="co"># 或自定义架构</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(model_name)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. 加载数据集</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(<span class="st">&quot;wikitext&quot;</span>, <span class="st">&quot;wikitext-2-raw-v1&quot;</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. 数据预处理</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_function(examples):</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenizer(examples[<span class="st">&quot;text&quot;</span>], truncation<span class="op">=</span><span class="va">True</span>, max_length<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>tokenized_dataset <span class="op">=</span> dataset.<span class="bu">map</span>(tokenize_function, batched<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. 训练配置</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">&quot;./results&quot;</span>,</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>    gradient_accumulation_steps<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">5e-5</span>,</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>    warmup_steps<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    fp16<span class="op">=</span><span class="va">True</span>,  <span class="co"># 混合精度</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>    logging_steps<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>    save_steps<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. 数据整理器</span></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>data_collator <span class="op">=</span> DataCollatorForLanguageModeling(</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="op">=</span>tokenizer, mlm<span class="op">=</span><span class="va">False</span>  <span class="co"># CLM 任务</span></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a><span class="co"># 6. 训练</span></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>tokenized_dataset[<span class="st">&quot;train&quot;</span>],</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>    data_collator<span class="op">=</span>data_collator,</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>trainer.train()</span></code></pre></div>
<hr />
<h2 id="大模型对齐与优化技术">2. 大模型对齐与优化技术</h2>
<h3 id="什么是对齐alignment">2.1 什么是对齐（Alignment）？</h3>
<p>对齐是指让模型的输出符合人类的期望和价值观，包括： -
<strong>有帮助（Helpful）</strong>：提供有用、准确的信息 -
<strong>诚实（Honest）</strong>：不编造信息，承认不确定性 -
<strong>无害（Harmless）</strong>：不产生有害、偏见内容</p>
<h3 id="rlhf基于人类反馈的强化学习">2.2
RLHF（基于人类反馈的强化学习）</h3>
<pre><code>┌─────────────────────────────────────────────────────────┐
│                    RLHF 流程                            │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Step 1: 收集人类偏好数据                               │
│  ┌─────────────────────────────────────────────────┐   │
│  │  Prompt → 模型生成多个回答 → 人类标注排序        │   │
│  │  例：回答A &gt; 回答B &gt; 回答C                       │   │
│  └─────────────────────────────────────────────────┘   │
│                          ↓                              │
│  Step 2: 训练奖励模型 (Reward Model)                    │
│  ┌─────────────────────────────────────────────────┐   │
│  │  输入：(prompt, response) → 输出：奖励分数        │   │
│  │  学习预测人类偏好                                 │   │
│  └─────────────────────────────────────────────────┘   │
│                          ↓                              │
│  Step 3: PPO 强化学习优化                              │
│  ┌─────────────────────────────────────────────────┐   │
│  │  策略模型生成回答 → 奖励模型打分 → 更新策略       │   │
│  │  + KL 散度约束（防止偏离原模型太远）              │   │
│  └─────────────────────────────────────────────────┘   │
│                                                         │
└─────────────────────────────────────────────────────────┘</code></pre>
<h3 id="dpo直接偏好优化">2.3 DPO（直接偏好优化）</h3>
<p>DPO 是 RLHF 的简化替代方案，无需单独训练奖励模型。</p>
<pre><code class="language-python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># DPO 核心思想</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co">&quot;&quot;&quot;</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co">RLHF: SFT Model → Reward Model → PPO → Aligned Model</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co">DPO:  SFT Model → 直接优化 → Aligned Model</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co">DPO 损失函数：</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co">L_DPO = -log σ(β * (log π(y_w|x) - log π(y_l|x) </span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co">                    - log π_ref(y_w|x) + log π_ref(y_l|x)))</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co">其中：</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co">- y_w: 偏好的回答 (winner)</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co">- y_l: 不偏好的回答 (loser)</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co">- π: 当前策略</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="co">- π_ref: 参考策略（SFT模型）</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co">- β: 温度参数</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co">&quot;&quot;&quot;</span></span></code></pre></div>
<table>
<thead>
<tr>
<th>方法</th>
<th>优势</th>
<th>劣势</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>RLHF</strong></td>
<td>效果好，业界验证充分</td>
<td>复杂，需要训练奖励模型</td>
</tr>
<tr>
<td><strong>DPO</strong></td>
<td>简单，无需奖励模型</td>
<td>对数据质量要求高</td>
</tr>
<tr>
<td><strong>RLAIF</strong></td>
<td>用AI代替人类标注</td>
<td>依赖辅助模型质量</td>
</tr>
</tbody>
</table>
<h3 id="提示工程prompt-engineering">2.4 提示工程（Prompt
Engineering）</h3>
<p>提示工程是一种无需训练即可优化模型输出的技术。</p>
<h4 id="核心技巧">2.4.1 核心技巧</h4>
<pre><code>┌─────────────────────────────────────────────────────────┐
│                提示工程技巧大全                          │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  🎭 角色设定 (Role Prompting)                           │
│  &quot;你是一位资深的Python开发专家，擅长代码优化...&quot;          │
│                                                         │
│  📝 指令清晰化                                          │
│  &quot;请用中文回答，分点列出，每点不超过50字&quot;                │
│                                                         │
│  📋 格式约束                                            │
│  &quot;请以JSON格式返回结果，包含name、age、city字段&quot;         │
│                                                         │
│  💡 思维链 (Chain of Thought, CoT)                      │
│  &quot;让我们一步一步思考这个问题...&quot;                         │
│                                                         │
│  📚 少样本学习 (Few-shot Learning)                      │
│  &quot;示例1：输入XX → 输出YY                                │
│   示例2：输入AA → 输出BB                                │
│   现在请处理：输入CC → ?&quot;                               │
│                                                         │
│  🔄 自我一致性 (Self-Consistency)                       │
│  多次采样，投票选择最一致的答案                          │
│                                                         │
│  🌳 思维树 (Tree of Thoughts)                           │
│  探索多条推理路径，评估选择最优                          │
│                                                         │
└─────────────────────────────────────────────────────────┘</code></pre>
<h4 id="prompt-模板示例">2.4.2 Prompt 模板示例</h4>
<pre><code class="language-python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 系统级 Prompt 模板</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>SYSTEM_PROMPT <span class="op">=</span> <span class="st">&quot;&quot;&quot;</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="st">你是一个专业的</span><span class="sc">{role}</span><span class="st">，具有以下特点：</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="st">1. </span><span class="sc">{characteristic_1}</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="st">2. </span><span class="sc">{characteristic_2}</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="st">3. </span><span class="sc">{characteristic_3}</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="st">在回答问题时，请遵循以下原则：</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="st">- </span><span class="sc">{principle_1}</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="st">- </span><span class="sc">{principle_2}</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="st">- </span><span class="sc">{principle_3}</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="st">输出格式要求：</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="sc">{output_format}</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;&quot;&quot;</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Chain of Thought 模板</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>COT_PROMPT <span class="op">=</span> <span class="st">&quot;&quot;&quot;</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="st">问题：</span><span class="sc">{question}</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="st">请按以下步骤思考：</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a><span class="st">1. 首先，理解问题的核心是什么</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="st">2. 然后，分析已知条件</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="st">3. 接着，制定解决方案</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="st">4. 最后，给出答案</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="st">让我们开始：</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;&quot;&quot;</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Few-shot 模板</span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>FEW_SHOT_PROMPT <span class="op">=</span> <span class="st">&quot;&quot;&quot;</span></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a><span class="st">任务：</span><span class="sc">{task_description}</span></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a><span class="st">示例1：</span></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a><span class="st">输入：</span><span class="sc">{example_1_input}</span></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a><span class="st">输出：</span><span class="sc">{example_1_output}</span></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a><span class="st">示例2：</span></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a><span class="st">输入：</span><span class="sc">{example_2_input}</span></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a><span class="st">输出：</span><span class="sc">{example_2_output}</span></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a><span class="st">现在请处理：</span></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a><span class="st">输入：</span><span class="sc">{actual_input}</span></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a><span class="st">输出：</span></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;&quot;&quot;</span></span></code></pre></div>
<h4 id="高级-prompt-技术">2.4.3 高级 Prompt 技术</h4>
<table>
<thead>
<tr>
<th>技术</th>
<th>说明</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ReAct</strong></td>
<td>推理+行动交替</td>
<td>Agent 任务</td>
</tr>
<tr>
<td><strong>Reflexion</strong></td>
<td>自我反思改进</td>
<td>复杂推理</td>
</tr>
<tr>
<td><strong>Plan-and-Solve</strong></td>
<td>先规划后执行</td>
<td>多步骤任务</td>
</tr>
<tr>
<td><strong>Least-to-Most</strong></td>
<td>从简到繁分解</td>
<td>复杂问题</td>
</tr>
<tr>
<td><strong>Skeleton-of-Thought</strong></td>
<td>先骨架后填充</td>
<td>长文本生成</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="大模型微调">3. 大模型微调</h2>
<h3 id="微调方法概览">3.1 微调方法概览</h3>
<pre><code>┌─────────────────────────────────────────────────────────┐
│                  微调方法谱系                           │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  按参数更新范围：                                        │
│                                                         │
│  全参数微调 (Full Fine-tuning)                          │
│  ├── 更新所有参数                                       │
│  ├── 效果最好，但成本最高                               │
│  └── 需要大量显存和计算资源                              │
│                                                         │
│  参数高效微调 (PEFT)                                    │
│  ├── LoRA：低秩适配                                     │
│  ├── QLoRA：量化 + LoRA                                 │
│  ├── Prefix Tuning：前缀调优                            │
│  ├── P-Tuning v2：深度提示调优                          │
│  ├── Adapter：适配器层                                  │
│  └── IA3：抑制和放大内部激活                            │
│                                                         │
└─────────────────────────────────────────────────────────┘</code></pre>
<h3 id="lora-详解">3.2 LoRA 详解</h3>
<p>LoRA (Low-Rank Adaptation) 是最流行的参数高效微调方法。</p>
<pre><code>┌─────────────────────────────────────────────────────────┐
│                    LoRA 原理                            │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  原始权重矩阵 W (d × k)                                  │
│                                                         │
│       ┌───────────────┐                                │
│  x ──→│   W (冻结)     │──→ y                           │
│       └───────────────┘                                │
│              +                                          │
│       ┌─────┐   ┌─────┐                                │
│  x ──→│  A  │──→│  B  │──→ Δy                          │
│       └─────┘   └─────┘                                │
│       (d × r)   (r × k)                                │
│                                                         │
│  最终输出：y&#39; = Wx + BAx                                │
│                                                         │
│  核心思想：                                              │
│  - 冻结原始权重 W                                        │
│  - 新增低秩矩阵 A、B (r &lt;&lt; d, k)                         │
│  - 只训练 A、B，参数量大幅减少                           │
│  - 例：r=8 时，参数量减少 ~10000 倍                      │
│                                                         │
└─────────────────────────────────────────────────────────┘</code></pre>
<h3 id="微调实践代码">3.3 微调实践代码</h3>
<h4 id="使用-peft-lora-微调">3.3.1 使用 PEFT + LoRA 微调</h4>
<pre><code class="language-python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForCausalLM, AutoTokenizer, TrainingArguments</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> peft <span class="im">import</span> LoraConfig, get_peft_model, TaskType</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> trl <span class="im">import</span> SFTTrainer</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. 加载基座模型</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">&quot;meta-llama/Llama-2-7b-hf&quot;</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    model_name,</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    torch_dtype<span class="op">=</span>torch.float16,</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">&quot;auto&quot;</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>tokenizer.pad_token <span class="op">=</span> tokenizer.eos_token</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. 配置 LoRA</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>lora_config <span class="op">=</span> LoraConfig(</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    task_type<span class="op">=</span>TaskType.CAUSAL_LM,</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    r<span class="op">=</span><span class="dv">8</span>,                      <span class="co"># 低秩维度</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    lora_alpha<span class="op">=</span><span class="dv">32</span>,            <span class="co"># 缩放因子</span></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>    lora_dropout<span class="op">=</span><span class="fl">0.1</span>,         <span class="co"># Dropout</span></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>    target_modules<span class="op">=</span>[          <span class="co"># 要适配的模块</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;q_proj&quot;</span>, <span class="st">&quot;k_proj&quot;</span>, <span class="st">&quot;v_proj&quot;</span>, <span class="st">&quot;o_proj&quot;</span>,</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;gate_proj&quot;</span>, <span class="st">&quot;up_proj&quot;</span>, <span class="st">&quot;down_proj&quot;</span></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. 应用 LoRA</span></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_peft_model(model, lora_config)</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>model.print_trainable_parameters()</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a><span class="co"># 输出：trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622</span></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. 准备数据集</span></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(<span class="st">&quot;json&quot;</span>, data_files<span class="op">=</span><span class="st">&quot;train_data.json&quot;</span>)</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> format_instruction(example):</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="ss">f&quot;&quot;&quot;### 指令：</span></span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a><span class="sc">{</span>example[<span class="st">&#39;instruction&#39;</span>]<span class="sc">}</span></span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a><span class="ss">### 回答：</span></span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a><span class="sc">{</span>example[<span class="st">&#39;output&#39;</span>]<span class="sc">}</span><span class="ss">&quot;&quot;&quot;</span></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. 训练配置</span></span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">&quot;./lora_output&quot;</span>,</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>    gradient_accumulation_steps<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">2e-4</span>,</span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a>    fp16<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a>    logging_steps<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a>    save_strategy<span class="op">=</span><span class="st">&quot;epoch&quot;</span>,</span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a>    warmup_ratio<span class="op">=</span><span class="fl">0.03</span>,</span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a><span class="co"># 6. 训练</span></span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> SFTTrainer(</span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>dataset[<span class="st">&quot;train&quot;</span>],</span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a>    formatting_func<span class="op">=</span>format_instruction,</span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a>    max_seq_length<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a>trainer.train()</span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a><span class="co"># 7. 保存 LoRA 权重</span></span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a>model.save_pretrained(<span class="st">&quot;./lora_weights&quot;</span>)</span></code></pre></div>
<h4 id="使用-qlora-微调显存更省">3.3.2 使用 QLoRA 微调（显存更省）</h4>
<pre><code class="language-python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BitsAndBytesConfig</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 4-bit 量化配置</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>bnb_config <span class="op">=</span> BitsAndBytesConfig(</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    load_in_4bit<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    bnb_4bit_quant_type<span class="op">=</span><span class="st">&quot;nf4&quot;</span>,</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    bnb_4bit_compute_dtype<span class="op">=</span>torch.float16,</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    bnb_4bit_use_double_quant<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载量化模型</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    model_name,</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    quantization_config<span class="op">=</span>bnb_config,</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">&quot;auto&quot;</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 后续与 LoRA 相同...</span></span></code></pre></div>
<h3 id="微调方法对比">3.4 微调方法对比</h3>
<table>
<thead>
<tr>
<th>方法</th>
<th>可训练参数</th>
<th>显存需求</th>
<th>效果</th>
<th>推荐场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Full Fine-tuning</strong></td>
<td>100%</td>
<td>极高</td>
<td>最好</td>
<td>充足资源</td>
</tr>
<tr>
<td><strong>LoRA</strong></td>
<td>~0.1%</td>
<td>中等</td>
<td>很好</td>
<td>通用场景</td>
</tr>
<tr>
<td><strong>QLoRA</strong></td>
<td>~0.1%</td>
<td>低</td>
<td>较好</td>
<td>显存受限</td>
</tr>
<tr>
<td><strong>Prefix Tuning</strong></td>
<td>~0.01%</td>
<td>低</td>
<td>一般</td>
<td>简单任务</td>
</tr>
<tr>
<td><strong>Adapter</strong></td>
<td>~1%</td>
<td>中低</td>
<td>较好</td>
<td>多任务</td>
</tr>
</tbody>
</table>
<h3 id="微调数据格式">3.5 微调数据格式</h3>
<pre><code class="language-python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Alpaca 格式（最常用）</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;instruction&quot;</span>: <span class="st">&quot;将以下句子翻译成英文&quot;</span>,</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;input&quot;</span>: <span class="st">&quot;今天天气很好&quot;</span>,</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;output&quot;</span>: <span class="st">&quot;The weather is nice today.&quot;</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co"># ShareGPT 格式（多轮对话）</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;conversations&quot;</span>: [</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        {<span class="st">&quot;from&quot;</span>: <span class="st">&quot;human&quot;</span>, <span class="st">&quot;value&quot;</span>: <span class="st">&quot;你好&quot;</span>},</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        {<span class="st">&quot;from&quot;</span>: <span class="st">&quot;gpt&quot;</span>, <span class="st">&quot;value&quot;</span>: <span class="st">&quot;你好！有什么我可以帮助你的吗？&quot;</span>},</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        {<span class="st">&quot;from&quot;</span>: <span class="st">&quot;human&quot;</span>, <span class="st">&quot;value&quot;</span>: <span class="st">&quot;介绍一下北京&quot;</span>},</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        {<span class="st">&quot;from&quot;</span>: <span class="st">&quot;gpt&quot;</span>, <span class="st">&quot;value&quot;</span>: <span class="st">&quot;北京是中国的首都...&quot;</span>}</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="co"># OpenAI 格式</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;messages&quot;</span>: [</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>        {<span class="st">&quot;role&quot;</span>: <span class="st">&quot;system&quot;</span>, <span class="st">&quot;content&quot;</span>: <span class="st">&quot;你是一个有帮助的助手&quot;</span>},</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>        {<span class="st">&quot;role&quot;</span>: <span class="st">&quot;user&quot;</span>, <span class="st">&quot;content&quot;</span>: <span class="st">&quot;你好&quot;</span>},</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>        {<span class="st">&quot;role&quot;</span>: <span class="st">&quot;assistant&quot;</span>, <span class="st">&quot;content&quot;</span>: <span class="st">&quot;你好！有什么可以帮你的？&quot;</span>}</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<hr />
<h2 id="大模型推理与部署">4. 大模型推理与部署</h2>
<h3 id="推理优化技术">4.1 推理优化技术</h3>
<pre><code>┌─────────────────────────────────────────────────────────┐
│                 推理优化技术栈                          │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  🔢 模型压缩                                            │
│  ├── 量化 (Quantization)                               │
│  │   ├── INT8：精度损失小，推理快 2x                    │
│  │   ├── INT4：精度有损，显存省 4x                      │
│  │   └── GPTQ/AWQ/GGUF：不同量化方案                   │
│  ├── 剪枝 (Pruning)                                    │
│  │   └── 移除不重要的参数                               │
│  └── 蒸馏 (Distillation)                               │
│      └── 大模型知识迁移到小模型                          │
│                                                         │
│  ⚡ 推理加速                                            │
│  ├── KV Cache：缓存注意力键值对                         │
│  ├── Flash Attention：高效注意力计算                   │
│  ├── Continuous Batching：动态批处理                   │
│  ├── Speculative Decoding：推测解码                    │
│  └── PagedAttention：分页注意力 (vLLM)                 │
│                                                         │
│  🖥️ 硬件优化                                           │
│  ├── TensorRT：NVIDIA GPU 优化                         │
│  ├── ONNX Runtime：跨平台加速                          │
│  └── 算子融合：减少内存访问                             │
│                                                         │
└─────────────────────────────────────────────────────────┘</code></pre>
<h3 id="量化详解">4.2 量化详解</h3>
<table>
<thead>
<tr>
<th>量化格式</th>
<th>位数</th>
<th>显存节省</th>
<th>精度损失</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>FP16</strong></td>
<td>16-bit</td>
<td>2x</td>
<td>几乎无</td>
<td>训练/推理</td>
</tr>
<tr>
<td><strong>BF16</strong></td>
<td>16-bit</td>
<td>2x</td>
<td>几乎无</td>
<td>训练/推理</td>
</tr>
<tr>
<td><strong>INT8</strong></td>
<td>8-bit</td>
<td>4x</td>
<td>很小</td>
<td>推理</td>
</tr>
<tr>
<td><strong>INT4</strong></td>
<td>4-bit</td>
<td>8x</td>
<td>较小</td>
<td>推理</td>
</tr>
<tr>
<td><strong>GPTQ</strong></td>
<td>4-bit</td>
<td>8x</td>
<td>小</td>
<td>GPU 推理</td>
</tr>
<tr>
<td><strong>AWQ</strong></td>
<td>4-bit</td>
<td>8x</td>
<td>更小</td>
<td>GPU 推理</td>
</tr>
<tr>
<td><strong>GGUF</strong></td>
<td>2-8bit</td>
<td>可变</td>
<td>可变</td>
<td>CPU/混合推理</td>
</tr>
</tbody>
</table>
<pre><code class="language-python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 使用 bitsandbytes 量化加载</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForCausalLM, BitsAndBytesConfig</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co"># INT8 量化</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>model_8bit <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;meta-llama/Llama-2-7b-hf&quot;</span>,</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    load_in_8bit<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">&quot;auto&quot;</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="co"># INT4 量化</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>bnb_config <span class="op">=</span> BitsAndBytesConfig(</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    load_in_4bit<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    bnb_4bit_quant_type<span class="op">=</span><span class="st">&quot;nf4&quot;</span>,</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    bnb_4bit_compute_dtype<span class="op">=</span>torch.float16,</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>model_4bit <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;meta-llama/Llama-2-7b-hf&quot;</span>,</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>    quantization_config<span class="op">=</span>bnb_config,</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">&quot;auto&quot;</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<h3 id="部署方案">4.3 部署方案</h3>
<h4 id="部署框架对比">4.3.1 部署框架对比</h4>
<table>
<thead>
<tr>
<th>框架</th>
<th>特点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>vLLM</strong></td>
<td>PagedAttention，吞吐量高</td>
<td>高并发在线服务</td>
</tr>
<tr>
<td><strong>TGI</strong></td>
<td>HuggingFace 官方，功能全面</td>
<td>生产环境</td>
</tr>
<tr>
<td><strong>Ollama</strong></td>
<td>简单易用，本地部署</td>
<td>个人/开发</td>
</tr>
<tr>
<td><strong>llama.cpp</strong></td>
<td>纯 CPU 推理，跨平台</td>
<td>边缘设备</td>
</tr>
<tr>
<td><strong>TensorRT-LLM</strong></td>
<td>NVIDIA 优化，极致性能</td>
<td>高性能 GPU</td>
</tr>
<tr>
<td><strong>OpenLLM</strong></td>
<td>灵活，支持多模型</td>
<td>模型服务化</td>
</tr>
</tbody>
</table>
<h4 id="vllm-部署示例">4.3.2 vLLM 部署示例</h4>
<pre><code class="language-python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 安装：pip install vllm</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> vllm <span class="im">import</span> LLM, SamplingParams</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. 加载模型</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>llm <span class="op">=</span> LLM(</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span><span class="st">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span>,</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    tensor_parallel_size<span class="op">=</span><span class="dv">1</span>,  <span class="co"># GPU 数量</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    dtype<span class="op">=</span><span class="st">&quot;float16&quot;</span>,</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>    max_model_len<span class="op">=</span><span class="dv">4096</span>,</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. 设置采样参数</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>sampling_params <span class="op">=</span> SamplingParams(</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    temperature<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    top_p<span class="op">=</span><span class="fl">0.9</span>,</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>    max_tokens<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. 推理</span></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>prompts <span class="op">=</span> [<span class="st">&quot;请介绍一下人工智能&quot;</span>, <span class="st">&quot;Python有什么优点？&quot;</span>]</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> llm.generate(prompts, sampling_params)</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> output <span class="kw">in</span> outputs:</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(output.outputs[<span class="dv">0</span>].text)</span></code></pre></div>
<pre><code class="language-bash"><code class="sourceCode bash"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># vLLM 启动 API 服务</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> <span class="at">-m</span> vllm.entrypoints.openai.api_server <span class="dt">\</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">--model</span> meta-llama/Llama-2-7b-chat-hf <span class="dt">\</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">--port</span> 8000 <span class="dt">\</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">--tensor-parallel-size</span> 1</span></code></pre></div>
<h4 id="ollama-本地部署">4.3.3 Ollama 本地部署</h4>
<pre><code class="language-bash"><code class="sourceCode bash"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 安装 Ollama</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="ex">curl</span> <span class="at">-fsSL</span> https://ollama.com/install.sh <span class="kw">|</span> <span class="fu">sh</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 下载并运行模型</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> pull llama3</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> run llama3</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 使用 API</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="ex">curl</span> http://localhost:11434/api/generate <span class="at">-d</span> <span class="st">&#39;{</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="st">  &quot;model&quot;: &quot;llama3&quot;,</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="st">  &quot;prompt&quot;: &quot;什么是大语言模型？&quot;</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="st">}&#39;</span></span></code></pre></div>
<pre><code class="language-python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Python 调用 Ollama</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ollama</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.chat(</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span><span class="st">&#39;llama3&#39;</span>,</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    messages<span class="op">=</span>[{<span class="st">&#39;role&#39;</span>: <span class="st">&#39;user&#39;</span>, <span class="st">&#39;content&#39;</span>: <span class="st">&#39;你好&#39;</span>}]</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response[<span class="st">&#39;message&#39;</span>][<span class="st">&#39;content&#39;</span>])</span></code></pre></div>
<h3 id="部署架构设计">4.4 部署架构设计</h3>
<pre><code>┌─────────────────────────────────────────────────────────┐
│              生产级 LLM 部署架构                         │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  用户请求                                               │
│      ↓                                                  │
│  ┌─────────────┐                                       │
│  │   API 网关   │  ← 认证、限流、路由                    │
│  └──────┬──────┘                                       │
│         │                                              │
│         ↓                                              │
│  ┌─────────────┐                                       │
│  │  负载均衡   │  ← 分发请求到不同实例                   │
│  └──────┬──────┘                                       │
│         │                                              │
│    ┌────┴────┐                                         │
│    ↓         ↓                                         │
│ ┌──────┐ ┌──────┐                                     │
│ │推理1 │ │推理2 │  ← vLLM/TGI 实例                     │
│ │(GPU) │ │(GPU) │                                     │
│ └──────┘ └──────┘                                     │
│    ↑         ↑                                         │
│    └────┬────┘                                         │
│         │                                              │
│  ┌─────────────┐                                       │
│  │  模型存储   │  ← S3/本地/模型仓库                    │
│  └─────────────┘                                       │
│                                                         │
│  监控组件：Prometheus + Grafana                         │
│  日志组件：ELK Stack                                    │
│                                                         │
└─────────────────────────────────────────────────────────┘</code></pre>
<hr />
<h2 id="其他训练与优化相关知识">5. 其他训练与优化相关知识</h2>
<h3 id="数据工程">5.1 数据工程</h3>
<h4 id="数据质量的重要性">5.1.1 数据质量的重要性</h4>
<pre><code>数据质量金字塔：

        ┌─────────┐
        │ 高质量  │  ← 人工标注、专家校验
        │  数据   │     效果最好，成本最高
        ├─────────┤
        │ 合成数据 │  ← GPT-4 生成、自我指令
        │         │     平衡效果与成本
        ├─────────┤
        │ 网络数据 │  ← CommonCrawl、网页爬取
        │         │     量大质低，需要清洗
        └─────────┘</code></pre>
<h4 id="数据处理流程">5.1.2 数据处理流程</h4>
<pre><code class="language-python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 数据处理 Pipeline</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>data_pipeline <span class="op">=</span> {</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;1. 数据收集&quot;</span>: [</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;爬取网页数据&quot;</span>,</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;收集开源数据集&quot;</span>,</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;生成合成数据&quot;</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;2. 数据清洗&quot;</span>: [</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;去重（MinHash、SimHash）&quot;</span>,</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;过滤低质量内容&quot;</span>,</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;移除有害/敏感信息&quot;</span>,</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;语言检测与分类&quot;</span></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;3. 数据处理&quot;</span>: [</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;分词 Tokenization&quot;</span>,</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;格式标准化&quot;</span>,</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;长度裁剪/填充&quot;</span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;4. 数据增强&quot;</span>: [</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;回译增强&quot;</span>,</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;同义词替换&quot;</span>,</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;模型改写&quot;</span></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;5. 质量评估&quot;</span>: [</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;困惑度评估&quot;</span>,</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;人工抽样检查&quot;</span>,</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;多样性分析&quot;</span></span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<h3 id="评估与测试">5.2 评估与测试</h3>
<h4 id="常用评估基准">5.2.1 常用评估基准</h4>
<table>
<thead>
<tr>
<th>基准</th>
<th>评估能力</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>MMLU</strong></td>
<td>多任务知识</td>
<td>57个学科的选择题</td>
</tr>
<tr>
<td><strong>HellaSwag</strong></td>
<td>常识推理</td>
<td>句子补全</td>
</tr>
<tr>
<td><strong>GSM8K</strong></td>
<td>数学推理</td>
<td>小学数学应用题</td>
</tr>
<tr>
<td><strong>HumanEval</strong></td>
<td>代码生成</td>
<td>Python 函数补全</td>
</tr>
<tr>
<td><strong>TruthfulQA</strong></td>
<td>真实性</td>
<td>检测幻觉</td>
</tr>
<tr>
<td><strong>MT-Bench</strong></td>
<td>对话能力</td>
<td>多轮对话评分</td>
</tr>
<tr>
<td><strong>C-Eval</strong></td>
<td>中文知识</td>
<td>中文多任务</td>
</tr>
<tr>
<td><strong>CMMLU</strong></td>
<td>中文 MMLU</td>
<td>中文版 MMLU</td>
</tr>
</tbody>
</table>
<h4 id="评估代码示例">5.2.2 评估代码示例</h4>
<pre><code class="language-python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 使用 lm-evaluation-harness 评估</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install lm-eval</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> lm_eval <span class="im">import</span> evaluator</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> lm_eval.models.huggingface <span class="im">import</span> HFLM</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载模型</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> HFLM(</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    pretrained<span class="op">=</span><span class="st">&quot;meta-llama/Llama-2-7b-hf&quot;</span>,</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span><span class="st">&quot;cuda&quot;</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 运行评估</span></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> evaluator.simple_evaluate(</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    tasks<span class="op">=</span>[<span class="st">&quot;hellaswag&quot;</span>, <span class="st">&quot;mmlu&quot;</span>, <span class="st">&quot;gsm8k&quot;</span>],</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>    num_fewshot<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">8</span></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(results[<span class="st">&quot;results&quot;</span>])</span></code></pre></div>
<h3 id="常见问题与解决方案">5.3 常见问题与解决方案</h3>
<table>
<thead>
<tr>
<th>问题</th>
<th>原因</th>
<th>解决方案</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>训练不收敛</strong></td>
<td>学习率不当、数据问题</td>
<td>调整 LR、检查数据</td>
</tr>
<tr>
<td><strong>过拟合</strong></td>
<td>数据量不足、模型过大</td>
<td>数据增强、正则化</td>
</tr>
<tr>
<td><strong>灾难性遗忘</strong></td>
<td>微调破坏原有能力</td>
<td>LoRA、较小学习率</td>
</tr>
<tr>
<td><strong>幻觉</strong></td>
<td>训练数据有噪声</td>
<td>RAG 增强、RLHF</td>
</tr>
<tr>
<td><strong>推理慢</strong></td>
<td>模型大、硬件弱</td>
<td>量化、蒸馏、升级硬件</td>
</tr>
<tr>
<td><strong>显存不足</strong></td>
<td>模型/batch 太大</td>
<td>梯度累积、量化、LoRA</td>
</tr>
</tbody>
</table>
<h3 id="训练资源估算">5.4 训练资源估算</h3>
<pre><code class="language-python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 显存估算公式（近似）</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> estimate_memory(params_billion, precision<span class="op">=</span><span class="st">&quot;fp16&quot;</span>, training<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co">    params_billion: 参数量（十亿）</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="co">    precision: fp32/fp16/int8/int4</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="co">    training: 是否训练</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    bytes_per_param <span class="op">=</span> {</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;fp32&quot;</span>: <span class="dv">4</span>,</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;fp16&quot;</span>: <span class="dv">2</span>,</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;bf16&quot;</span>: <span class="dv">2</span>,</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;int8&quot;</span>: <span class="dv">1</span>,</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;int4&quot;</span>: <span class="fl">0.5</span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 模型权重</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>    model_memory <span class="op">=</span> params_billion <span class="op">*</span> bytes_per_param[precision]</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> training:</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 训练时需要额外的优化器状态和梯度</span></span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># AdamW: 参数 + 梯度 + 一阶动量 + 二阶动量 ≈ 4x</span></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 加上激活值等，总计约 6-8x</span></span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>        total_memory <span class="op">=</span> model_memory <span class="op">*</span> <span class="dv">6</span></span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 推理时主要是模型权重 + KV Cache</span></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>        total_memory <span class="op">=</span> model_memory <span class="op">*</span> <span class="fl">1.2</span></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="ss">f&quot;</span><span class="sc">{</span>total_memory<span class="sc">:.1f}</span><span class="ss"> GB&quot;</span></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a><span class="co"># 示例</span></span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(estimate_memory(<span class="dv">7</span>, <span class="st">&quot;fp16&quot;</span>, training<span class="op">=</span><span class="va">True</span>))   <span class="co"># 约 84 GB</span></span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(estimate_memory(<span class="dv">7</span>, <span class="st">&quot;fp16&quot;</span>, training<span class="op">=</span><span class="va">False</span>))  <span class="co"># 约 16.8 GB</span></span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(estimate_memory(<span class="dv">7</span>, <span class="st">&quot;int4&quot;</span>, training<span class="op">=</span><span class="va">False</span>))  <span class="co"># 约 4.2 GB</span></span></code></pre></div>
<h3 id="训练工具生态">5.5 训练工具生态</h3>
<pre><code>┌─────────────────────────────────────────────────────────┐
│                  训练工具生态                           │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  🔧 训练框架                                            │
│  ├── Hugging Face Transformers                         │
│  ├── DeepSpeed                                         │
│  ├── Megatron-LM                                       │
│  ├── ColossalAI                                        │
│  └── LLaMA-Factory                                     │
│                                                         │
│  📊 实验管理                                            │
│  ├── Weights &amp; Biases                                  │
│  ├── MLflow                                            │
│  └── TensorBoard                                       │
│                                                         │
│  🗃️ 数据处理                                           │
│  ├── Datasets (HuggingFace)                            │
│  ├── Apache Spark                                      │
│  └── Dask                                              │
│                                                         │
│  ☁️ 云平台                                              │
│  ├── AWS SageMaker                                     │
│  ├── Google Vertex AI                                  │
│  ├── Azure ML                                          │
│  └── AutoDL / 恒源云（国内）                            │
│                                                         │
└─────────────────────────────────────────────────────────┘</code></pre>
<hr />
<h2 id="总结">6. 总结</h2>
<h3 id="核心要点">6.1 核心要点</h3>
<table>
<thead>
<tr>
<th>主题</th>
<th>关键点</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>架构与训练</strong></td>
<td>Transformer 架构、预训练三阶段、分布式训练</td>
</tr>
<tr>
<td><strong>对齐与优化</strong></td>
<td>RLHF/DPO 对齐、提示工程技巧</td>
</tr>
<tr>
<td><strong>微调技术</strong></td>
<td>LoRA/QLoRA 参数高效微调、数据格式</td>
</tr>
<tr>
<td><strong>推理部署</strong></td>
<td>量化压缩、vLLM/Ollama 部署、架构设计</td>
</tr>
</tbody>
</table>
<h3 id="学习路径">6.2 学习路径</h3>
<pre><code>基础 → 实践 → 深入

1️⃣ 基础理论
   • Transformer 架构原理
   • 注意力机制
   • 预训练目标

2️⃣ 动手实践
   • 使用 HuggingFace 微调模型
   • 尝试 LoRA/QLoRA
   • 本地部署 Ollama

3️⃣ 深入优化
   • 分布式训练
   • 推理优化
   • 生产部署</code></pre>
<h3 id="参考资源">6.3 参考资源</h3>
<ul>
<li><a href="https://arxiv.org/abs/1706.03762">《Attention Is All You
Need》</a></li>
<li><a href="https://arxiv.org/abs/2106.09685">《LoRA: Low-Rank
Adaptation》</a></li>
<li><a href="https://arxiv.org/abs/2203.02155">《Training language
models to follow instructions with human feedback》</a></li>
<li><a href="https://huggingface.co/docs">Hugging Face 文档</a></li>
<li><a href="https://github.com/vllm-project/vllm">vLLM 项目</a></li>
<li><a
href="https://github.com/hiyouga/LLaMA-Factory">LLaMA-Factory</a></li>
</ul>

                </article>
            </div>
        </section>

        <!-- 右侧热门文章 -->
        <aside class="popular-sidebar">
            <div class="popular-header">
                <h3>热门文章</h3>
            </div>
            <div class="popular-list" id="popular-list">
                <!-- 动态生成的热门文章列表 -->
            </div>
        </aside>
    </main>

    <!-- JavaScript -->
    <script type="module" src="/script.js?v=2.2.0"></script>
    
    <!-- Initialize Highlight.js -->
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // Highlight all code blocks
            document.querySelectorAll('pre code').forEach((block) => {
                hljs.highlightElement(block);
            });
        });
    </script>
</body>
</html>