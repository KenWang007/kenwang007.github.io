#!/usr/bin/env python3
"""
å¯¼èˆªæ•°æ®è‡ªåŠ¨ç”Ÿæˆè„šæœ¬
ç”¨äºæ‰«ænotesç›®å½•ç»“æ„ï¼Œç”Ÿæˆå¯¼èˆªèœå•å’Œåšå®¢æ–‡ç« æ•°æ®
åŒæ—¶å°†Markdownæ–‡ä»¶è½¬æ¢ä¸ºHTMLæ ¼å¼ï¼Œç”Ÿæˆsitemap.xmlå’ŒRSS feed
"""

import os
import json
import re
import subprocess
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Set
from datetime import datetime
import hashlib
import xml.etree.ElementTree as ET
from xml.dom import minidom

# ====== é…ç½® ======
class Config:
    """é…ç½®ç±»"""
    # é¡¹ç›®æ ¹ç›®å½•
    ROOT_DIR = Path(__file__).parent
    # Notesç›®å½•
    NOTES_DIR = ROOT_DIR / "notes"
    # HTMLæ¨¡æ¿æ–‡ä»¶
    TEMPLATE_FILE = ROOT_DIR / "template.html"
    # è¾“å‡ºJSONæ–‡ä»¶
    OUTPUT_FILE = ROOT_DIR / "nav_data.json"
    # Sitemapè¾“å‡ºæ–‡ä»¶
    SITEMAP_FILE = ROOT_DIR / "sitemap.xml"
    # RSS Feedè¾“å‡ºæ–‡ä»¶
    RSS_FILE = ROOT_DIR / "rss.xml"
    
    # ç½‘ç«™é…ç½®
    SITE_URL = "https://kenwang007.github.io"
    SITE_NAME = "Kençš„çŸ¥è¯†åº“"
    SITE_DESCRIPTION = "è®°å½•AIå­¦ä¹ ã€æ¶æ„è®¾è®¡ã€ç¼–ç¨‹æŠ€æœ¯å’Œè¯»ä¹¦å¿ƒå¾—çš„ä¸ªäººçŸ¥è¯†åº“"
    AUTHOR_NAME = "Ken Wang"
    AUTHOR_EMAIL = "ken@example.com"
    
    # å…³é”®è¯æå–é…ç½®
    MAX_KEYWORDS_PER_POST = 5
    MIN_KEYWORD_LENGTH = 2
    
    # æ ¸å¿ƒä¸»é¢˜è¯ï¼ˆè¿™äº›è¯å¦‚æœå‡ºç°ï¼Œåº”è¯¥ä¼˜å…ˆä½œä¸ºå…³é”®è¯ï¼‰
    CORE_TOPICS = {
        'RAG', 'æ£€ç´¢å¢å¼ºç”Ÿæˆ', 'LLM', 'å¤§è¯­è¨€æ¨¡å‹', 'å‘é‡æ•°æ®åº“',
        'AI', 'Python', 'JavaScript', 'TypeScript', 'æ¶æ„', 'è®¾è®¡æ¨¡å¼',
        'å¾®æœåŠ¡', 'Docker', 'Kubernetes', 'æ•°æ®åº“', 'Redis', 'MongoDB',
        'ç®—æ³•', 'æ•°æ®ç»“æ„', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'NLP', 'Ollama',
        'OpenWebUI', 'Cursor', 'Prompt', 'Fine-tuning', 'æç¤ºå·¥ç¨‹'
    }
    
    # è¦ç§»é™¤çš„ä¿®é¥°è¯
    MODIFIER_WORDS = {
        'å…¨é¢', 'è¯¦ç»†', 'æ·±å…¥', 'æœ€æ–°', 'å®Œæ•´', 'ç®€æ˜', 'å¿«é€Ÿ', 
        'å®æˆ˜', 'å…¥é—¨', 'è¿›é˜¶', 'é«˜çº§', 'åŸºç¡€', 'åˆçº§', 'ä¸­çº§',
        'ä»‹ç»', 'æ•™ç¨‹', 'æŒ‡å—', 'å­¦ä¹ ', 'æŠ€æœ¯'
    }
    
    # åœç”¨è¯ï¼ˆåœ¨å…³é”®è¯æå–æ—¶å¿½ç•¥ï¼‰
    STOP_WORDS = {
        'çš„', 'äº†', 'å’Œ', 'æ˜¯', 'åœ¨', 'ä¸', 'æˆ–', 'ç­‰', 'åŠ',
        'è¿™', 'é‚£', 'å…¶', 'æ­¤', 'ä¸º', 'æœ‰', 'å°†', 'å¯', 'èƒ½',
        'How', 'What', 'When', 'Where', 'Why', 'to', 'is', 'in',
        'the', 'a', 'an', 'and', 'or', 'but', 'of', 'at', 'by'
    }

# ====== æ—¥å¿—é…ç½® ======
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


def scan_notes_directory() -> Tuple[List[Dict], List[Dict], List[Dict]]:
    """
    æ‰«ænotesç›®å½•ç»“æ„
    
    Returns:
        Tuple[List[Dict], List[Dict], List[Dict]]: 
            å¯¼èˆªèœå•æ•°æ®ã€åšå®¢æ–‡ç« æ•°æ®ã€ç›®å½•ç»“æ„æ•°æ®
    """
    logger.info("å¼€å§‹æ‰«ænotesç›®å½•...")
    
    # 1. é¦–å…ˆå°†æ‰€æœ‰Markdownæ–‡ä»¶è½¬æ¢ä¸ºHTML
    logger.info("=== å¼€å§‹è½¬æ¢Markdownåˆ°HTML ===")
    convert_all_markdown_files()
    
    # åˆå§‹åŒ–æ•°æ®ç»“æ„
    nav_menu = []
    blog_posts = []
    directory_structure = []
    
    # æ£€æŸ¥notesç›®å½•æ˜¯å¦å­˜åœ¨
    if not Config.NOTES_DIR.exists():
        logger.warning(f"Notesç›®å½•ä¸å­˜åœ¨: {Config.NOTES_DIR}")
        return nav_menu, blog_posts, directory_structure
    
    # æ‰«æä¸€çº§ç›®å½•
    for dir_path in sorted(Config.NOTES_DIR.iterdir()):
        if not dir_path.is_dir():
            continue
            
        dir_name = dir_path.name
        dir_rel_path = str(dir_path.relative_to(Config.ROOT_DIR))
        
        # æ£€æŸ¥ç›®å½•ä¸‹æ˜¯å¦æœ‰.htmlæ–‡ä»¶ï¼ˆåŒ…æ‹¬å­ç›®å½•ï¼‰
        has_html = check_directory_has_html(dir_path)
        
        # æ‰«æå­ç›®å½•ç»“æ„
        subdirs = scan_directory_structure(dir_path)
        
        # åªæœ‰å½“ç›®å½•æœ¬èº«æœ‰.htmlæ–‡ä»¶æˆ–æœ‰åŒ…å«.htmlæ–‡ä»¶çš„å­ç›®å½•æ—¶æ‰æ·»åŠ åˆ°å¯¼èˆª
        if has_html or subdirs:
            nav_menu.append({
                "name": dir_name,
                "path": dir_rel_path
            })
        
        # æ·»åŠ åˆ°ç›®å½•ç»“æ„
        directory_structure.append({
            "path": dir_rel_path,
            "has_html": has_html,
            "subdirs": subdirs
        })
        
        # æ‰«æç›®å½•ä¸‹çš„åšå®¢æ–‡ç« 
        scan_blog_posts(dir_path, blog_posts)
    
    logger.info(f"æ‰«æå®Œæˆ: æ‰¾åˆ° {len(nav_menu)} ä¸ªå¯¼èˆªé¡¹, {len(blog_posts)} ç¯‡æ–‡ç« ")
    return nav_menu, blog_posts, directory_structure

def convert_all_markdown_files() -> None:
    """è½¬æ¢æ‰€æœ‰Markdownæ–‡ä»¶ä¸ºHTML"""
    converted_count = 0
    failed_count = 0
    
    for root, _, files in os.walk(Config.NOTES_DIR):
        for file in files:
            if file.endswith('.md') and file != 'index.md':
                md_file_path = Path(root) / file
                try:
                    if convert_markdown_to_html(md_file_path):
                        converted_count += 1
                except Exception as e:
                    logger.error(f"è½¬æ¢å¤±è´¥: {md_file_path} - {e}")
                    failed_count += 1
    
    logger.info(f"è½¬æ¢å®Œæˆ: {converted_count} ä¸ªæˆåŠŸ, {failed_count} ä¸ªå¤±è´¥")


def check_directory_has_html(directory: Path) -> bool:
    """
    æ£€æŸ¥ç›®å½•ä¸‹æ˜¯å¦æœ‰.htmlæ–‡ä»¶ï¼ˆåŒ…æ‹¬å­ç›®å½•ï¼‰
    
    Args:
        directory: ç›®å½•è·¯å¾„
        
    Returns:
        bool: æ˜¯å¦åŒ…å«HTMLæ–‡ä»¶
    """
    try:
        for root, _, files in os.walk(directory):
            for file in files:
                if file.endswith(".html"):
                    return True
    except (PermissionError, OSError) as e:
        logger.warning(f"æ— æ³•è®¿é—®ç›®å½• {directory}: {e}")
    
    return False


def scan_directory_structure(directory: Path) -> List[Dict]:
    """
    æ‰«æç›®å½•ç»“æ„ï¼Œè¿”å›å­ç›®å½•åˆ—è¡¨ï¼ˆé€’å½’ï¼‰
    
    Args:
        directory: ç›®å½•è·¯å¾„
        
    Returns:
        List[Dict]: å­ç›®å½•åˆ—è¡¨
    """
    subdirs = []
    
    try:
        for item in sorted(directory.iterdir()):
            if not item.is_dir():
                continue
                
            dir_rel_path = str(item.relative_to(Config.ROOT_DIR))
            has_html = check_directory_has_html(item)
            subdirs_structure = scan_directory_structure(item)
            
            # åªæœ‰å½“ç›®å½•æœ¬èº«æœ‰.htmlæ–‡ä»¶æˆ–æœ‰åŒ…å«.htmlæ–‡ä»¶çš„å­ç›®å½•æ—¶æ‰æ·»åŠ 
            if has_html or subdirs_structure:
                subdirs.append({
                    "path": dir_rel_path,
                    "has_html": has_html,
                    "subdirs": subdirs_structure
                })
    except (PermissionError, OSError) as e:
        logger.warning(f"æ— æ³•æ‰«æç›®å½• {directory}: {e}")
    
    return subdirs


def scan_blog_posts(directory: Path, blog_posts: List[Dict]) -> None:
    """
    æ‰«æç›®å½•ä¸‹çš„åšå®¢æ–‡ç« 
    
    Args:
        directory: ç›®å½•è·¯å¾„
        blog_posts: åšå®¢æ–‡ç« åˆ—è¡¨ï¼ˆä¼šè¢«ä¿®æ”¹ï¼‰
    """
    processed_files = set()
    
    try:
        for root, _, files in os.walk(directory):
            for file in files:
                # åªå¤„ç†.htmlæ–‡ä»¶ï¼Œè·³è¿‡index.html
                if not file.endswith('.html') or file.lower() == "index.html":
                    continue
                
                file_path = Path(root) / file
                file_rel_path = str(file_path.relative_to(Config.ROOT_DIR))
                
                # ç¡®ä¿æ¯ä¸ª.htmlæ–‡ä»¶åªå¤„ç†ä¸€æ¬¡
                if file_rel_path in processed_files:
                    continue
                
                try:
                    # æå–æ ‡é¢˜å’Œå…³é”®è¯
                    title, keywords = extract_metadata(file_path)
                    
                    # æ·»åŠ åˆ°åšå®¢æ–‡ç« æ•°æ®
                    blog_posts.append({
                        "title": title or file_path.stem,
                        "path": file_rel_path,
                        "keywords": keywords
                    })
                    
                    processed_files.add(file_rel_path)
                except Exception as e:
                    logger.warning(f"å¤„ç†æ–‡ç« å¤±è´¥ {file_path}: {e}")
    except (PermissionError, OSError) as e:
        logger.error(f"æ‰«æç›®å½•å¤±è´¥ {directory}: {e}")


def extract_metadata(file_path: Path) -> Tuple[Optional[str], List[str]]:
    """
    ä»æ–‡ä»¶ä¸­æå–æ ‡é¢˜å’Œå…³é”®è¯
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        
    Returns:
        Tuple[Optional[str], List[str]]: æ ‡é¢˜å’Œå…³é”®è¯åˆ—è¡¨
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        title = None
        
        # æ£€æµ‹æ–‡ä»¶ç±»å‹å¹¶æå–æ ‡é¢˜
        if file_path.suffix == '.md':
            # Markdownæ ¼å¼ï¼šæå–#æ ‡é¢˜
            title_match = re.search(r'^#\s+(.+)', content, re.MULTILINE)
            if title_match:
                title = title_match.group(1).strip()
        elif file_path.suffix == '.html':
            # HTMLæ ¼å¼ï¼šæå–<h1>æ ‡é¢˜
            title_match = re.search(r'<h1[^>]*>(.+?)</h1>', content, re.IGNORECASE)
            if title_match:
                # ç§»é™¤HTMLæ ‡ç­¾å’Œç‰¹æ®Šå­—ç¬¦
                title = re.sub(r'<[^>]+>', '', title_match.group(1))
                title = title.strip()
        
        # æå–å…³é”®è¯ï¼ˆä¼ å…¥å†…å®¹ä»¥è·å¾—æ›´å¥½çš„ç»“æœï¼‰
        keywords = extract_keywords(title or '', content)
        
        return title, keywords
    except Exception as e:
        logger.warning(f"æå–å…ƒæ•°æ®å¤±è´¥: {file_path} - {e}")
        return file_path.stem, []


def convert_markdown_to_html(md_file_path: Path) -> Optional[Path]:
    """
    å°†Markdownæ–‡ä»¶è½¬æ¢ä¸ºHTMLæ–‡ä»¶
    
    Args:
        md_file_path: Markdownæ–‡ä»¶è·¯å¾„
        
    Returns:
        Optional[Path]: è½¬æ¢åçš„HTMLæ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
    """
    html_file_path = md_file_path.with_suffix('.html')
    temp_html_path = md_file_path.with_suffix('.temp.html')
    
    try:
        # æ£€æŸ¥pandocæ˜¯å¦å¯ç”¨
        try:
            subprocess.run(['pandoc', '--version'], 
                         capture_output=True, 
                         check=True)
        except (subprocess.CalledProcessError, FileNotFoundError):
            logger.error("Pandocæœªå®‰è£…æˆ–ä¸å¯ç”¨ï¼Œè¯·å…ˆå®‰è£…pandoc")
            return None
        
        # è¯»å–Markdownæ–‡ä»¶å†…å®¹
        with open(md_file_path, 'r', encoding='utf-8') as f:
            md_content = f.read()
        
        # æå–æ ‡é¢˜
        title_match = re.search(r'^#\s+(.+)', md_content, re.MULTILINE)
        title = title_match.group(1).strip() if title_match else md_file_path.stem
        
        # ä½¿ç”¨pandocè½¬æ¢Markdownåˆ°HTML
        result = subprocess.run(
            ['pandoc', '-s', str(md_file_path), '-o', str(temp_html_path)],
            capture_output=True,
            text=True,
            timeout=30  # 30ç§’è¶…æ—¶
        )
        
        if result.returncode != 0:
            logger.error(f"Pandocè½¬æ¢å¤±è´¥: {result.stderr}")
            return None
        
        # è¯»å–å¹¶æå–è½¬æ¢åçš„HTMLå†…å®¹
        with open(temp_html_path, 'r', encoding='utf-8') as f:
            temp_html_content = f.read()
        
        # æå–<body>æ ‡ç­¾å†…çš„å†…å®¹
        body_match = re.search(
            r'<body[^>]*>([\s\S]*?)</body>', 
            temp_html_content, 
            re.IGNORECASE
        )
        body_content = body_match.group(1) if body_match else temp_html_content
        
        # è¯»å–æ¨¡æ¿æ–‡ä»¶
        if not Config.TEMPLATE_FILE.exists():
            logger.error(f"æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {Config.TEMPLATE_FILE}")
            return None
            
        with open(Config.TEMPLATE_FILE, 'r', encoding='utf-8') as f:
            template_content = f.read()
        
        # æå–å…³é”®è¯
        keywords = extract_keywords(title, md_content)
        
        # ç”Ÿæˆå…ƒæ•°æ®
        metadata = generate_metadata_for_template(html_file_path, title, keywords)
        
        # æ›¿æ¢æ¨¡æ¿ä¸­çš„æ‰€æœ‰å ä½ç¬¦
        final_html_content = template_content
        for key, value in metadata.items():
            final_html_content = final_html_content.replace(f'{{{{{key}}}}}', str(value))
        
        # æ›¿æ¢å†…å®¹
        final_html_content = final_html_content.replace('{{content}}', body_content)
        
        # å†™å…¥æœ€ç»ˆçš„HTMLæ–‡ä»¶
        with open(html_file_path, 'w', encoding='utf-8') as f:
            f.write(final_html_content)
        
        logger.info(f"âœ“ è½¬æ¢å®Œæˆ: {md_file_path.name} -> {html_file_path.name}")
        return html_file_path
        
    except subprocess.TimeoutExpired:
        logger.error(f"è½¬æ¢è¶…æ—¶: {md_file_path}")
        return None
    except Exception as e:
        logger.error(f"âœ— è½¬æ¢å¤±è´¥: {md_file_path} - {e}")
        return None
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_html_path.exists():
            try:
                temp_html_path.unlink()
            except Exception as e:
                logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")


def extract_keywords(title: str, content: str = "") -> List[str]:
    """
    æ”¹è¿›çš„å…³é”®è¯æå–ç®—æ³•
    
    Args:
        title: æ–‡ç« æ ‡é¢˜
        content: æ–‡ç« å†…å®¹ï¼ˆå¯é€‰ï¼Œç”¨äºæ›´å¥½çš„å…³é”®è¯æå–ï¼‰
        
    Returns:
        List[str]: å…³é”®è¯åˆ—è¡¨
    """
    if not title:
        return []
    
    keywords = []
    seen: Set[str] = set()
    keyword_scores: Dict[str, int] = {}
    
    # ç§»é™¤è¡¨æƒ…ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
    clean_title = re.sub(
        r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF]', 
        '', 
        title
    ).strip()
    
    if not clean_title:
        return []
    
    # 1. æå–æ ¸å¿ƒä¸»é¢˜è¯ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
    for topic in Config.CORE_TOPICS:
        # ä¸åŒºåˆ†å¤§å°å†™åŒ¹é…
        if topic.lower() in clean_title.lower():
            # ä¿ç•™åŸå§‹å¤§å°å†™
            for word in re.findall(r'\b\w+\b', clean_title):
                if word.lower() == topic.lower():
                    if word not in seen:
                        keyword_scores[word] = keyword_scores.get(word, 0) + 10
                        seen.add(word)
                    break
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨é…ç½®ä¸­çš„ç‰ˆæœ¬
                if topic not in seen:
                    keyword_scores[topic] = keyword_scores.get(topic, 0) + 10
                    seen.add(topic)
    
    # 2. æå–è‹±æ–‡æœ¯è¯­ï¼ˆä¸­ç­‰ä¼˜å…ˆçº§ï¼‰
    english_terms = re.findall(r'\b[A-Z][A-Za-z]+\b|\b[a-z]{4,}\b', clean_title)
    for term in english_terms:
        # è¿‡æ»¤åœç”¨è¯
        if term in Config.STOP_WORDS or term.lower() in Config.STOP_WORDS:
            continue
        
        if term not in seen and len(term) >= Config.MIN_KEYWORD_LENGTH:
            # æŠ€æœ¯æœ¯è¯­é€šå¸¸é¦–å­—æ¯å¤§å†™æˆ–å…¨å°å†™
            if term[0].isupper() or len(term) > 4:
                keyword_scores[term] = keyword_scores.get(term, 0) + 5
                seen.add(term)
    
    # 3. æå–ä¸­æ–‡å…³é”®æ¦‚å¿µ
    # ç§»é™¤ä¿®é¥°è¯
    processed_title = clean_title
    for modifier in Config.MODIFIER_WORDS:
        processed_title = re.sub(rf'\b{modifier}\b', '', processed_title)
    processed_title = processed_title.strip()
    
    # æŸ¥æ‰¾æŠ€æœ¯ç›¸å…³æ¨¡å¼
    patterns = [
        (r'([^ï¼Œã€‚ï¼ï¼Ÿ\s]{2,})(?:æŠ€æœ¯|æ¡†æ¶|å·¥å…·|å¹³å°|ç³»ç»Ÿ)', 1),  # æŠ€æœ¯è¯æ±‡
        (r'(?:ä½¿ç”¨|è¿è¡Œ|é…ç½®|å®‰è£…)\s*([^ï¼Œã€‚ï¼ï¼Ÿ\s]{2,})', 1),  # æ“ä½œå¯¹è±¡
        (r'([A-Z][a-z]+(?:[A-Z][a-z]+)*)', 0),  # é©¼å³°å‘½å
    ]
    
    for pattern, group in patterns:
        matches = re.findall(pattern, clean_title)
        for match in matches:
            keyword = match if isinstance(match, str) else match[group]
            keyword = keyword.strip()
            
            # è¿‡æ»¤åœç”¨è¯å’Œå·²è§å…³é”®è¯
            if (keyword and 
                keyword not in Config.STOP_WORDS and
                keyword not in Config.MODIFIER_WORDS and
                len(keyword) >= Config.MIN_KEYWORD_LENGTH and 
                keyword not in seen):
                keyword_scores[keyword] = keyword_scores.get(keyword, 0) + 3
                seen.add(keyword)
    
    # 4. ç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼ˆåŸºäºå¸¸è§åˆ†éš”ç¬¦ï¼‰
    chinese_parts = re.split(r'[ï¼Œã€‚ï¼ï¼Ÿã€\s]+', processed_title)
    for part in chinese_parts:
        # æå–çº¯ä¸­æ–‡è¯æ±‡
        chinese_words = re.findall(r'[\u4e00-\u9fff]{2,}', part)
        for word in chinese_words:
            if (word not in Config.STOP_WORDS and 
                word not in Config.MODIFIER_WORDS and
                len(word) >= Config.MIN_KEYWORD_LENGTH and 
                word not in seen):
                keyword_scores[word] = keyword_scores.get(word, 0) + 2
                seen.add(word)
    
    # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›å‰Nä¸ªå…³é”®è¯
    sorted_keywords = sorted(
        keyword_scores.items(), 
        key=lambda x: x[1], 
        reverse=True
    )
    
    keywords = [kw for kw, score in sorted_keywords[:Config.MAX_KEYWORDS_PER_POST]]
    
    # å¦‚æœå…³é”®è¯å¤ªå°‘ï¼Œæ·»åŠ æ ‡é¢˜ä¸­çš„ä¸»è¦è¯æ±‡
    if len(keywords) < 2:
        words = re.findall(r'[\u4e00-\u9fff]{2,}|[A-Za-z]{3,}', clean_title)
        for word in words:
            if (word not in Config.STOP_WORDS and 
                word not in seen and 
                len(keywords) < Config.MAX_KEYWORDS_PER_POST):
                keywords.append(word)
                seen.add(word)
    
    return keywords[:Config.MAX_KEYWORDS_PER_POST]


def generate_sitemap(blog_posts: List[Dict]) -> None:
    """
    ç”Ÿæˆsitemap.xmlæ–‡ä»¶
    
    Args:
        blog_posts: åšå®¢æ–‡ç« åˆ—è¡¨
    """
    logger.info("å¼€å§‹ç”Ÿæˆsitemap.xml...")
    
    # åˆ›å»ºXMLæ ¹å…ƒç´ 
    urlset = ET.Element('urlset')
    urlset.set('xmlns', 'http://www.sitemaps.org/schemas/sitemap/0.9')
    
    # æ·»åŠ é¦–é¡µ
    url = ET.SubElement(urlset, 'url')
    ET.SubElement(url, 'loc').text = f"{Config.SITE_URL}/"
    ET.SubElement(url, 'changefreq').text = 'daily'
    ET.SubElement(url, 'priority').text = '1.0'
    ET.SubElement(url, 'lastmod').text = datetime.now().strftime('%Y-%m-%d')
    
    # æ·»åŠ æ‰€æœ‰åšå®¢æ–‡ç« 
    for post in blog_posts:
        url = ET.SubElement(urlset, 'url')
        post_url = f"{Config.SITE_URL}/{post['path']}"
        ET.SubElement(url, 'loc').text = post_url
        ET.SubElement(url, 'changefreq').text = 'weekly'
        ET.SubElement(url, 'priority').text = '0.8'
        
        # è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´
        file_path = Config.ROOT_DIR / post['path']
        if file_path.exists():
            mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
            ET.SubElement(url, 'lastmod').text = mod_time.strftime('%Y-%m-%d')
    
    # ç¾åŒ–XMLè¾“å‡º
    xml_str = minidom.parseString(ET.tostring(urlset)).toprettyxml(indent="  ")
    
    # ç§»é™¤ç©ºè¡Œ
    xml_str = '\n'.join([line for line in xml_str.split('\n') if line.strip()])
    
    # ä¿å­˜sitemap.xml
    with open(Config.SITEMAP_FILE, 'w', encoding='utf-8') as f:
        f.write(xml_str)
    
    logger.info(f"âœ… Sitemapç”Ÿæˆå®Œæˆ: {Config.SITEMAP_FILE}")


def generate_rss_feed(blog_posts: List[Dict]) -> None:
    """
    ç”ŸæˆRSS feed
    
    Args:
        blog_posts: åšå®¢æ–‡ç« åˆ—è¡¨
    """
    logger.info("å¼€å§‹ç”ŸæˆRSS feed...")
    
    # åˆ›å»ºRSSæ ¹å…ƒç´ 
    rss = ET.Element('rss')
    rss.set('version', '2.0')
    rss.set('xmlns:atom', 'http://www.w3.org/2005/Atom')
    
    channel = ET.SubElement(rss, 'channel')
    ET.SubElement(channel, 'title').text = Config.SITE_NAME
    ET.SubElement(channel, 'link').text = Config.SITE_URL
    ET.SubElement(channel, 'description').text = Config.SITE_DESCRIPTION
    ET.SubElement(channel, 'language').text = 'zh-CN'
    ET.SubElement(channel, 'lastBuildDate').text = datetime.now().strftime('%a, %d %b %Y %H:%M:%S +0000')
    
    # æ·»åŠ atom:link
    atom_link = ET.SubElement(channel, 'atom:link')
    atom_link.set('href', f"{Config.SITE_URL}/rss.xml")
    atom_link.set('rel', 'self')
    atom_link.set('type', 'application/rss+xml')
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºæ–‡ç« ï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    sorted_posts = sorted(
        blog_posts,
        key=lambda p: Config.ROOT_DIR / p['path'] if (Config.ROOT_DIR / p['path']).exists() else 0,
        reverse=True
    )
    
    # åªåŒ…å«æœ€è¿‘çš„20ç¯‡æ–‡ç« 
    for post in sorted_posts[:20]:
        item = ET.SubElement(channel, 'item')
        ET.SubElement(item, 'title').text = post['title']
        
        post_url = f"{Config.SITE_URL}/{post['path']}"
        ET.SubElement(item, 'link').text = post_url
        ET.SubElement(item, 'guid').text = post_url
        
        # ç”Ÿæˆæè¿°ï¼ˆåŒ…å«å…³é”®è¯ï¼‰
        if post.get('keywords'):
            description = f"å…³é”®è¯: {', '.join(post['keywords'])}"
            ET.SubElement(item, 'description').text = description
        
        # æ·»åŠ åˆ†ç±»ï¼ˆä½¿ç”¨å…³é”®è¯ï¼‰
        for keyword in post.get('keywords', []):
            ET.SubElement(item, 'category').text = keyword
        
        # æ·»åŠ å‘å¸ƒæ—¥æœŸ
        file_path = Config.ROOT_DIR / post['path']
        if file_path.exists():
            pub_date = datetime.fromtimestamp(file_path.stat().st_mtime)
            ET.SubElement(item, 'pubDate').text = pub_date.strftime('%a, %d %b %Y %H:%M:%S +0000')
    
    # ç¾åŒ–XMLè¾“å‡º
    xml_str = minidom.parseString(ET.tostring(rss)).toprettyxml(indent="  ")
    
    # ç§»é™¤ç©ºè¡Œ
    xml_str = '\n'.join([line for line in xml_str.split('\n') if line.strip()])
    
    # ä¿å­˜RSSæ–‡ä»¶
    with open(Config.RSS_FILE, 'w', encoding='utf-8') as f:
        f.write(xml_str)
    
    logger.info(f"âœ… RSS Feedç”Ÿæˆå®Œæˆ: {Config.RSS_FILE}")


def generate_metadata_for_template(file_path: Path, title: str, keywords: List[str]) -> Dict[str, str]:
    """
    ä¸ºæ¨¡æ¿ç”Ÿæˆå…ƒæ•°æ®
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        title: æ ‡é¢˜
        keywords: å…³é”®è¯åˆ—è¡¨
        
    Returns:
        Dict[str, str]: å…ƒæ•°æ®å­—å…¸
    """
    # ç”Ÿæˆæè¿°
    description = f"{title} - "
    if keywords:
        description += f"å…³é”®è¯: {', '.join(keywords[:3])}"
    else:
        description += Config.SITE_DESCRIPTION
    
    # è·å–æ–‡ä»¶ä¿¡æ¯
    stat = file_path.stat()
    created_date = datetime.fromtimestamp(stat.st_ctime)
    modified_date = datetime.fromtimestamp(stat.st_mtime)
    
    # ç›¸å¯¹è·¯å¾„
    rel_path = file_path.relative_to(Config.ROOT_DIR)
    
    return {
        'title': title,
        'description': description[:160],  # é™åˆ¶æè¿°é•¿åº¦
        'keywords': ', '.join(keywords),
        'date': created_date.isoformat(),
        'modified_date': modified_date.isoformat(),
        'path': str(rel_path)
    }


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¯¼èˆªæ•°æ®è‡ªåŠ¨ç”Ÿæˆå·¥å…·')
    parser.add_argument('--no-sitemap', action='store_true', help='ä¸ç”Ÿæˆsitemap.xml')
    parser.add_argument('--no-rss', action='store_true', help='ä¸ç”ŸæˆRSS feed')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡ºæ¨¡å¼')
    
    args = parser.parse_args()
    
    if args.verbose:
        logger.setLevel(logging.DEBUG)
    
    print("=== å¯¼èˆªæ•°æ®è‡ªåŠ¨ç”Ÿæˆå·¥å…· ===")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # æ‰«æç›®å½•ç»“æ„
    nav_menu, blog_posts, directory_structure = scan_notes_directory()
    
    # ç”Ÿæˆå¯¼èˆªæ•°æ®
    nav_data = {
        "nav_menu": nav_menu,
        "blog_posts": blog_posts,
        "directory_structure": directory_structure,
        "generated_at": datetime.now().timestamp()
    }
    
    # ä¿å­˜ä¸ºJSONæ–‡ä»¶
    with open(Config.OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(nav_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"âœ… å¯¼èˆªæ•°æ®å·²ä¿å­˜: {Config.OUTPUT_FILE}")
    
    # ç”Ÿæˆsitemap.xml
    if not args.no_sitemap:
        try:
            generate_sitemap(blog_posts)
        except Exception as e:
            logger.error(f"âŒ Sitemapç”Ÿæˆå¤±è´¥: {e}")
    
    # ç”ŸæˆRSS feed
    if not args.no_rss:
        try:
            generate_rss_feed(blog_posts)
        except Exception as e:
            logger.error(f"âŒ RSSç”Ÿæˆå¤±è´¥: {e}")
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print(f"\n{'='*50}")
    print("ç”Ÿæˆå®Œæˆï¼")
    print(f"{'='*50}")
    print(f"ğŸ“ å¯¼èˆªèœå•æ•°é‡: {len(nav_menu)}")
    print(f"ğŸ“ åšå®¢æ–‡ç« æ•°é‡: {len(blog_posts)}")
    print(f"ğŸ—‚ï¸  ç›®å½•ç»“æ„æ•°é‡: {len(directory_structure)}")
    print(f"\nè¾“å‡ºæ–‡ä»¶:")
    print(f"  â€¢ {Config.OUTPUT_FILE}")
    if not args.no_sitemap:
        print(f"  â€¢ {Config.SITEMAP_FILE}")
    if not args.no_rss:
        print(f"  â€¢ {Config.RSS_FILE}")
    print(f"\nå®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    return nav_data


if __name__ == "__main__":
    main()
