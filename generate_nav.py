#!/usr/bin/env python3
"""
å¯¼èˆªæ•°æ®è‡ªåŠ¨ç”Ÿæˆè„šæœ¬
ç”¨äºæ‰«ænotesç›®å½•ç»“æ„ï¼Œç”Ÿæˆå¯¼èˆªèœå•å’Œåšå®¢æ–‡ç« æ•°æ®
åŒæ—¶å°†Markdownæ–‡ä»¶è½¬æ¢ä¸ºHTMLæ ¼å¼ï¼Œç”Ÿæˆsitemap.xmlå’ŒRSS feed
"""

import os
import json
import re
import subprocess
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Set
from datetime import datetime
import hashlib
import xml.etree.ElementTree as ET
from xml.dom import minidom

# ====== é…ç½® ======
class Config:
    """é…ç½®ç±»"""
    # é¡¹ç›®æ ¹ç›®å½•
    ROOT_DIR = Path(__file__).parent
    # Notesç›®å½•
    NOTES_DIR = ROOT_DIR / "notes"
    # HTMLæ¨¡æ¿æ–‡ä»¶
    TEMPLATE_FILE = ROOT_DIR / "template.html"
    # è¾“å‡ºJSONæ–‡ä»¶
    OUTPUT_FILE = ROOT_DIR / "nav_data.json"
    # Sitemapè¾“å‡ºæ–‡ä»¶
    SITEMAP_FILE = ROOT_DIR / "sitemap.xml"
    # RSS Feedè¾“å‡ºæ–‡ä»¶
    RSS_FILE = ROOT_DIR / "rss.xml"

    # New ASCII-only output roots (under dist/ for cleaner separation)
    DIST_DIR = ROOT_DIR / "dist"
    POSTS_OUT_DIR = DIST_DIR / "p"
    CATEGORIES_OUT_DIR = DIST_DIR / "c"
    
    # ç½‘ç«™é…ç½®
    SITE_URL = "https://kenwang007.github.io"
    SITE_NAME = "Kençš„çŸ¥è¯†åº“"
    SITE_DESCRIPTION = "è®°å½•AIå­¦ä¹ ã€æ¶æ„è®¾è®¡ã€ç¼–ç¨‹æŠ€æœ¯å’Œè¯»ä¹¦å¿ƒå¾—çš„ä¸ªäººçŸ¥è¯†åº“"
    AUTHOR_NAME = "Ken Wang"
    AUTHOR_EMAIL = "ken@example.com"
    
    # å…³é”®è¯æå–é…ç½®
    MAX_KEYWORDS_PER_POST = 5
    MIN_KEYWORD_LENGTH = 2
    
    # æ ¸å¿ƒä¸»é¢˜è¯ï¼ˆè¿™äº›è¯å¦‚æœå‡ºç°ï¼Œåº”è¯¥ä¼˜å…ˆä½œä¸ºå…³é”®è¯ï¼‰
    CORE_TOPICS = {
        'RAG', 'æ£€ç´¢å¢å¼ºç”Ÿæˆ', 'LLM', 'å¤§è¯­è¨€æ¨¡å‹', 'å‘é‡æ•°æ®åº“',
        'AI', 'Python', 'JavaScript', 'TypeScript', 'æ¶æ„', 'è®¾è®¡æ¨¡å¼',
        'å¾®æœåŠ¡', 'Docker', 'Kubernetes', 'æ•°æ®åº“', 'Redis', 'MongoDB',
        'ç®—æ³•', 'æ•°æ®ç»“æ„', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'NLP', 'Ollama',
        'OpenWebUI', 'Cursor', 'Prompt', 'Fine-tuning', 'æç¤ºå·¥ç¨‹'
    }
    
    # è¦ç§»é™¤çš„ä¿®é¥°è¯
    MODIFIER_WORDS = {
        'å…¨é¢', 'è¯¦ç»†', 'æ·±å…¥', 'æœ€æ–°', 'å®Œæ•´', 'ç®€æ˜', 'å¿«é€Ÿ', 
        'å®æˆ˜', 'å…¥é—¨', 'è¿›é˜¶', 'é«˜çº§', 'åŸºç¡€', 'åˆçº§', 'ä¸­çº§',
        'ä»‹ç»', 'æ•™ç¨‹', 'æŒ‡å—', 'å­¦ä¹ ', 'æŠ€æœ¯'
    }
    
    # åœç”¨è¯ï¼ˆåœ¨å…³é”®è¯æå–æ—¶å¿½ç•¥ï¼‰
    STOP_WORDS = {
        'çš„', 'äº†', 'å’Œ', 'æ˜¯', 'åœ¨', 'ä¸', 'æˆ–', 'ç­‰', 'åŠ',
        'è¿™', 'é‚£', 'å…¶', 'æ­¤', 'ä¸º', 'æœ‰', 'å°†', 'å¯', 'èƒ½',
        'How', 'What', 'When', 'Where', 'Why', 'to', 'is', 'in',
        'the', 'a', 'an', 'and', 'or', 'but', 'of', 'at', 'by'
    }

# ====== æ—¥å¿—é…ç½® ======
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

_PANDOC_AVAILABLE: Optional[bool] = None

def pandoc_available() -> bool:
    global _PANDOC_AVAILABLE
    if _PANDOC_AVAILABLE is not None:
        return _PANDOC_AVAILABLE
    try:
        subprocess.run(['pandoc', '--version'], capture_output=True, check=True)
        _PANDOC_AVAILABLE = True
    except (subprocess.CalledProcessError, FileNotFoundError):
        _PANDOC_AVAILABLE = False
    return _PANDOC_AVAILABLE

def extract_markdown_content_from_legacy_html(legacy_html_path: Path) -> str:
    """
    Extract the inner HTML of <article class="markdown-content">...</article> from an existing legacy HTML page.
    Falls back to <body> content if not found.
    """
    try:
        with open(legacy_html_path, 'r', encoding='utf-8') as f:
            html = f.read()
        article_match = re.search(
            r'<article[^>]*class="[^"]*markdown-content[^"]*"[^>]*>([\s\S]*?)</article>',
            html,
            re.IGNORECASE
        )
        if article_match:
            return article_match.group(1).strip()
        body_match = re.search(r'<body[^>]*>([\s\S]*?)</body>', html, re.IGNORECASE)
        if body_match:
            return body_match.group(1).strip()
        return html
    except Exception:
        return ""

def stable_id(text: str, length: int = 12) -> str:
    """Create a stable ASCII id from an arbitrary unicode string."""
    h = hashlib.sha1(text.encode("utf-8")).hexdigest()
    return h[:length]

_FRONT_MATTER_RE = re.compile(r'^\s*---\s*\n([\s\S]*?)\n---\s*\n', re.MULTILINE)

def parse_front_matter(md_text: str) -> Tuple[Dict[str, str], str]:
    """
    Very small YAML front-matter parser.
    Supports top-of-file:
      ---
      key: value
      ---
    Returns (meta, content_without_front_matter).
    """
    m = _FRONT_MATTER_RE.match(md_text)
    if not m:
        return {}, md_text

    raw = m.group(1)
    meta: Dict[str, str] = {}
    for line in raw.splitlines():
        line = line.strip()
        if not line or line.startswith('#'):
            continue
        if ':' not in line:
            continue
        k, v = line.split(':', 1)
        k = k.strip()
        v = v.strip().strip('"').strip("'")
        if k:
            meta[k] = v
    return meta, md_text[m.end():]

def validate_slug(slug: str) -> str:
    slug = (slug or '').strip()
    if not slug:
        raise ValueError("slug is empty")
    if not re.fullmatch(r'[a-z0-9]+(?:-[a-z0-9]+)*', slug):
        raise ValueError(f"invalid slug: {slug} (allowed: a-z0-9 and '-')")
    return slug


def slug_report(md_files: List[Path]) -> int:
    """
    Print a report of post/directory slugs:
    - missing slug
    - invalid slug
    - duplicate slug
    Returns process exit code (0 if OK, 1 if issues found).
    """
    missing_posts: List[Tuple[str, str]] = []   # (rel_md, suggested_slug)
    invalid_posts: List[Tuple[str, str]] = []   # (rel_md, slug)
    dup_posts: Dict[str, List[str]] = {}        # slug -> [rel_md]

    missing_dirs: List[Tuple[str, str]] = []    # (rel_dir_index_md, suggested_slug)
    invalid_dirs: List[Tuple[str, str]] = []    # (rel_dir_index_md, slug)
    dup_dirs: Dict[str, List[str]] = {}         # slug -> [rel_dir_index_md]

    seen_post: Dict[str, List[str]] = {}
    seen_dir: Dict[str, List[str]] = {}

    # Posts
    for md in md_files:
        rel_md = str(md.relative_to(Config.ROOT_DIR))
        text = md.read_text(encoding='utf-8')
        meta, _ = parse_front_matter(text)
        slug = meta.get('slug')
        if not slug:
            missing_posts.append((rel_md, f"post-{stable_id(rel_md)}"))
            continue
        try:
            slug = validate_slug(slug)
            seen_post.setdefault(slug, []).append(rel_md)
        except Exception:
            invalid_posts.append((rel_md, slug))

    for slug, files in seen_post.items():
        if len(files) > 1:
            dup_posts[slug] = files

    # Directories (index.md only)
    for root, _, files in os.walk(Config.NOTES_DIR):
        if "index.md" not in files:
            continue
        idx = Path(root) / "index.md"
        rel_idx = str(idx.relative_to(Config.ROOT_DIR))
        text = idx.read_text(encoding='utf-8')
        meta, _ = parse_front_matter(text)
        slug = meta.get('slug')
        if not slug:
            # Suggest a stable placeholder based on directory path
            rel_dir = str(Path(root).relative_to(Config.ROOT_DIR))
            missing_dirs.append((rel_idx, f"cat-{stable_id(rel_dir)}"))
            continue
        try:
            slug = validate_slug(slug)
            seen_dir.setdefault(slug, []).append(rel_idx)
        except Exception:
            invalid_dirs.append((rel_idx, slug))

    for slug, files in seen_dir.items():
        if len(files) > 1:
            dup_dirs[slug] = files

    issues = (
        len(missing_posts) + len(invalid_posts) + len(dup_posts) +
        len(missing_dirs) + len(invalid_dirs) + len(dup_dirs)
    )

    print("=== Slug Report ===")
    print(f"Posts scanned: {len(md_files)}")
    print()

    if missing_posts:
        print(f"[Posts missing slug] {len(missing_posts)}")
        for rel_md, sug in missing_posts:
            print(f"  - {rel_md}")
            print(f"    suggested: slug: {sug}")
        print()

    if invalid_posts:
        print(f"[Posts invalid slug] {len(invalid_posts)}")
        for rel_md, bad in invalid_posts:
            print(f"  - {rel_md}")
            print(f"    found: slug: {bad}")
            print("    rule: a-z 0-9 and '-' only (lowercase)")
        print()

    if dup_posts:
        print(f"[Posts duplicate slug] {len(dup_posts)}")
        for slug, files in dup_posts.items():
            print(f"  - slug: {slug}")
            for f in files:
                print(f"    * {f}")
        print()

    if missing_dirs:
        print(f"[Dirs missing slug] {len(missing_dirs)} (optional)")
        for rel_idx, sug in missing_dirs:
            print(f"  - {rel_idx}")
            print(f"    suggested: slug: {sug}")
        print()

    if invalid_dirs:
        print(f"[Dirs invalid slug] {len(invalid_dirs)}")
        for rel_idx, bad in invalid_dirs:
            print(f"  - {rel_idx}")
            print(f"    found: slug: {bad}")
            print("    rule: a-z 0-9 and '-' only (lowercase)")
        print()

    if dup_dirs:
        print(f"[Dirs duplicate slug] {len(dup_dirs)}")
        for slug, files in dup_dirs.items():
            print(f"  - slug: {slug}")
            for f in files:
                print(f"    * {f}")
        print()

    if issues == 0:
        print("âœ… No slug issues found.")
        return 0

    print(f"âŒ Found {issues} issue(s).")
    return 1

def collect_markdown_posts() -> List[Path]:
    """Collect all markdown posts under notes/ excluding index.md."""
    md_files: List[Path] = []
    for root, _, files in os.walk(Config.NOTES_DIR):
        for file in files:
            if not file.endswith(".md"):
                continue
            if file == "index.md":
                continue
            md_files.append(Path(root) / file)
    return sorted(md_files)

def build_directory_structure_from_md(md_files: List[Path]) -> List[Dict]:
    """
    Build directory structure tree (only directories that contain posts or index.md or have subdirs with posts).
    Each node has: {name, path(original), id, url, has_posts, subdirs[]}
    """
    # Build a set of directories that should exist as nodes
    dir_set: Set[Path] = set()
    for md in md_files:
        dir_set.add(md.parent)
        # include all parents up to notes
        p = md.parent
        while p != Config.NOTES_DIR and Config.NOTES_DIR in p.parents:
            p = p.parent
            dir_set.add(p)

    # also include directories that have index.md
    for root, dirs, files in os.walk(Config.NOTES_DIR):
        if "index.md" in files:
            dir_set.add(Path(root))

    # directory slug mapping from index.md front matter (manual)
    dir_slug_by_rel: Dict[str, str] = {}
    used_dir_slugs: Set[str] = set()
    for d in list(dir_set):
        index_md = d / "index.md"
        if not index_md.exists():
            continue
        try:
            text = index_md.read_text(encoding='utf-8')
            meta, _ = parse_front_matter(text)
            if meta.get('slug'):
                slug = validate_slug(meta['slug'])
                if slug in used_dir_slugs:
                    raise ValueError(f"duplicate directory slug: {slug}")
                used_dir_slugs.add(slug)
                rel_dir = str(d.relative_to(Config.ROOT_DIR))
                dir_slug_by_rel[rel_dir] = slug
        except Exception as e:
            logger.warning(f"ç›®å½• slug è§£æå¤±è´¥ {index_md}: {e}")

    def node_for_dir(dir_path: Path) -> Dict:
        rel_dir = str(dir_path.relative_to(Config.ROOT_DIR))
        dir_id = stable_id(rel_dir)
        slug = dir_slug_by_rel.get(rel_dir)
        url = f"dist/c/{slug}/index.html" if slug else f"dist/c/{dir_id}/index.html"
        name = dir_path.name
        has_posts = any((p.parent == dir_path) for p in md_files) or (dir_path / "index.md").exists()
        return {
            "name": name,
            "path": rel_dir,
            "id": dir_id,
            "slug": slug,
            "url": url,
            "has_posts": has_posts,
            "subdirs": []
        }

    # Build tree under notes
    def build_children(parent: Path) -> List[Dict]:
        children = []
        for child in sorted(parent.iterdir()):
            if not child.is_dir():
                continue
            if child not in dir_set:
                # But keep if it has any descendant in dir_set
                if not any((d != child and child in d.parents) for d in dir_set):
                    continue
            child_node = node_for_dir(child)
            child_node["subdirs"] = build_children(child)
            # Include node if it has posts or any included subdirs
            if child_node["has_posts"] or child_node["subdirs"]:
                children.append(child_node)
        return children

    roots = []
    for top in sorted(Config.NOTES_DIR.iterdir()):
        if not top.is_dir():
            continue
        if top not in dir_set and not any((d != top and top in d.parents) for d in dir_set):
            continue
        top_node = node_for_dir(top)
        top_node["subdirs"] = build_children(top)
        if top_node["has_posts"] or top_node["subdirs"]:
            roots.append(top_node)
    return roots

def flatten_directories(dirs: List[Dict]) -> List[Dict]:
    """Flatten directory tree into a list."""
    out: List[Dict] = []
    def walk(nodes: List[Dict]):
        for n in nodes:
            out.append(n)
            walk(n.get("subdirs", []))
    walk(dirs)
    return out


def scan_notes_directory() -> Tuple[List[Dict], List[Dict], List[Dict]]:
    """
    æ‰«æ notes ç›®å½•ç»“æ„ï¼ˆASCII-only URL è¾“å‡ºï¼‰

    Returns:
        nav_menu: é¡¶å±‚å¯¼èˆªï¼ˆä½¿ç”¨ /c/<id>/index.htmlï¼‰
        blog_posts: æ–‡ç« åˆ—è¡¨ï¼ˆä½¿ç”¨ /p/<id>.htmlï¼‰
        directory_structure: ç›®å½•æ ‘ï¼ˆæ¯ä¸ªèŠ‚ç‚¹åŒ…å« id/url/path/subdirsï¼‰
    """
    logger.info("å¼€å§‹æ‰«ænotesç›®å½•ï¼ˆASCII-only URLï¼‰...")

    if not Config.NOTES_DIR.exists():
        logger.warning(f"Notesç›®å½•ä¸å­˜åœ¨: {Config.NOTES_DIR}")
        return [], [], []

    md_files = collect_markdown_posts()
    logger.info(f"æ‰¾åˆ° Markdown æ–‡ç« : {len(md_files)}")

    # Build directory structure from Markdown sources
    directory_structure = build_directory_structure_from_md(md_files)
    flat_dirs = flatten_directories(directory_structure)

    # Top nav uses top-level nodes
    nav_menu = [
        {"name": d["name"], "id": d["id"], "url": d["url"], "path": d["path"]}
        for d in directory_structure
    ]

    # Build post records and legacy->new url map (for link rewriting)
    blog_posts: List[Dict] = []
    legacy_to_new: Dict[str, str] = {}
    used_post_slugs: Set[str] = set()

    for md in md_files:
        rel_md = str(md.relative_to(Config.ROOT_DIR))
        # original html path (legacy)
        rel_html_legacy = str(md.with_suffix(".html").relative_to(Config.ROOT_DIR))
        post_id = stable_id(rel_md)

        # Read markdown to support manual slug/title in front matter
        md_text = md.read_text(encoding='utf-8')
        meta, md_text_wo_fm = parse_front_matter(md_text)
        manual_slug = None
        if meta.get('slug'):
            manual_slug = validate_slug(meta['slug'])
            if manual_slug in used_post_slugs:
                raise ValueError(f"duplicate post slug: {manual_slug}")
            used_post_slugs.add(manual_slug)

        post_url = f"dist/p/{manual_slug}.html" if manual_slug else f"dist/p/{post_id}.html"

        # Title / keywords (prefer front matter title if provided)
        title, keywords = extract_metadata(md)
        if meta.get('title'):
            title = meta['title']

        blog_posts.append({
            "title": title or md.stem,
            "id": post_id,
            "slug": manual_slug,
            "url": post_url,
            "original_path": rel_html_legacy,
            "keywords": keywords
        })

        # map both md/html legacy references to new url
        legacy_to_new[rel_html_legacy] = post_url
        legacy_to_new["/" + rel_html_legacy] = post_url
        legacy_to_new[rel_md] = post_url
        legacy_to_new["/" + rel_md] = post_url

    # Directory legacy index mapping (notes/.../index.html -> /c/<id>/index.html)
    for d in flat_dirs:
        legacy_index = f"{d['path']}/index.html"
        legacy_to_new[legacy_index] = d["url"]
        legacy_to_new["/" + legacy_index] = d["url"]

    # Build helper: md -> post record
    id_to_post = {p["id"]: p for p in blog_posts}
    md_to_post: Dict[str, Dict] = {}
    for md in md_files:
        rel_md = str(md.relative_to(Config.ROOT_DIR))
        pid = stable_id(rel_md)
        md_to_post[rel_md] = id_to_post.get(pid)

    converted_ok = 0
    for md in md_files:
        rel_md = str(md.relative_to(Config.ROOT_DIR))
        post = md_to_post.get(rel_md)
        if not post:
            continue
        out_path = Config.ROOT_DIR / post["url"]
        try:
            if convert_markdown_to_html(md, out_path, legacy_to_new):
                converted_ok += 1
        except Exception as e:
            logger.error(f"è½¬æ¢å¤±è´¥: {md} - {e}")

    logger.info(f"æ–‡ç« é¡µç”Ÿæˆå®Œæˆ: {converted_ok}/{len(md_files)}")

    # Generate directory pages
    logger.info("=== å¼€å§‹ç”Ÿæˆç›®å½•é¡µ /c/<id>/index.html ===")
    dir_pages_ok = 0
    for d in flat_dirs:
        try:
            if generate_directory_page(d, legacy_to_new):
                dir_pages_ok += 1
        except Exception as e:
            logger.error(f"ç›®å½•é¡µç”Ÿæˆå¤±è´¥ {d.get('path')}: {e}")
    logger.info(f"ç›®å½•é¡µç”Ÿæˆå®Œæˆ: {dir_pages_ok}/{len(flat_dirs)}")

    logger.info(f"æ‰«æå®Œæˆ: å¯¼èˆª {len(nav_menu)}ï¼Œæ–‡ç«  {len(blog_posts)}ï¼Œç›®å½•èŠ‚ç‚¹ {len(flat_dirs)}")
    return nav_menu, blog_posts, directory_structure

def convert_all_markdown_files() -> None:
    """è½¬æ¢æ‰€æœ‰Markdownæ–‡ä»¶ä¸ºHTML"""
    converted_count = 0
    failed_count = 0
    
    for root, _, files in os.walk(Config.NOTES_DIR):
        for file in files:
            if file.endswith('.md') and file != 'index.md':
                md_file_path = Path(root) / file
                try:
                    if convert_markdown_to_html(md_file_path):
                        converted_count += 1
                except Exception as e:
                    logger.error(f"è½¬æ¢å¤±è´¥: {md_file_path} - {e}")
                    failed_count += 1
    
    logger.info(f"è½¬æ¢å®Œæˆ: {converted_count} ä¸ªæˆåŠŸ, {failed_count} ä¸ªå¤±è´¥")


def check_directory_has_html(directory: Path) -> bool:
    """
    æ£€æŸ¥ç›®å½•ä¸‹æ˜¯å¦æœ‰.htmlæ–‡ä»¶ï¼ˆåŒ…æ‹¬å­ç›®å½•ï¼‰
    
    Args:
        directory: ç›®å½•è·¯å¾„
        
    Returns:
        bool: æ˜¯å¦åŒ…å«HTMLæ–‡ä»¶
    """
    try:
        for root, _, files in os.walk(directory):
            for file in files:
                if file.endswith(".html"):
                    return True
    except (PermissionError, OSError) as e:
        logger.warning(f"æ— æ³•è®¿é—®ç›®å½• {directory}: {e}")
    
    return False


def scan_directory_structure(directory: Path) -> List[Dict]:
    """
    æ‰«æç›®å½•ç»“æ„ï¼Œè¿”å›å­ç›®å½•åˆ—è¡¨ï¼ˆé€’å½’ï¼‰
    
    Args:
        directory: ç›®å½•è·¯å¾„
        
    Returns:
        List[Dict]: å­ç›®å½•åˆ—è¡¨
    """
    subdirs = []
    
    try:
        for item in sorted(directory.iterdir()):
            if not item.is_dir():
                continue
                
            dir_rel_path = str(item.relative_to(Config.ROOT_DIR))
            has_html = check_directory_has_html(item)
            subdirs_structure = scan_directory_structure(item)
            
            # åªæœ‰å½“ç›®å½•æœ¬èº«æœ‰.htmlæ–‡ä»¶æˆ–æœ‰åŒ…å«.htmlæ–‡ä»¶çš„å­ç›®å½•æ—¶æ‰æ·»åŠ 
            if has_html or subdirs_structure:
                subdirs.append({
                    "path": dir_rel_path,
                    "has_html": has_html,
                    "subdirs": subdirs_structure
                })
    except (PermissionError, OSError) as e:
        logger.warning(f"æ— æ³•æ‰«æç›®å½• {directory}: {e}")
    
    return subdirs


def scan_blog_posts(directory: Path, blog_posts: List[Dict]) -> None:
    """
    æ‰«æç›®å½•ä¸‹çš„åšå®¢æ–‡ç« 
    
    Args:
        directory: ç›®å½•è·¯å¾„
        blog_posts: åšå®¢æ–‡ç« åˆ—è¡¨ï¼ˆä¼šè¢«ä¿®æ”¹ï¼‰
    """
    processed_files = set()
    
    try:
        for root, _, files in os.walk(directory):
            for file in files:
                # åªå¤„ç†.htmlæ–‡ä»¶ï¼Œè·³è¿‡index.html
                if not file.endswith('.html') or file.lower() == "index.html":
                    continue
                
                file_path = Path(root) / file
                file_rel_path = str(file_path.relative_to(Config.ROOT_DIR))
                
                # ç¡®ä¿æ¯ä¸ª.htmlæ–‡ä»¶åªå¤„ç†ä¸€æ¬¡
                if file_rel_path in processed_files:
                    continue
                
                try:
                    # æå–æ ‡é¢˜å’Œå…³é”®è¯
                    title, keywords = extract_metadata(file_path)
                    
                    # æ·»åŠ åˆ°åšå®¢æ–‡ç« æ•°æ®
                    blog_posts.append({
                        "title": title or file_path.stem,
                        "path": file_rel_path,
                        "keywords": keywords
                    })
                    
                    processed_files.add(file_rel_path)
                except Exception as e:
                    logger.warning(f"å¤„ç†æ–‡ç« å¤±è´¥ {file_path}: {e}")
    except (PermissionError, OSError) as e:
        logger.error(f"æ‰«æç›®å½•å¤±è´¥ {directory}: {e}")


def extract_metadata(file_path: Path) -> Tuple[Optional[str], List[str]]:
    """
    ä»æ–‡ä»¶ä¸­æå–æ ‡é¢˜å’Œå…³é”®è¯
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        
    Returns:
        Tuple[Optional[str], List[str]]: æ ‡é¢˜å’Œå…³é”®è¯åˆ—è¡¨
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        title = None
        
        # æ£€æµ‹æ–‡ä»¶ç±»å‹å¹¶æå–æ ‡é¢˜
        if file_path.suffix == '.md':
            # Markdownæ ¼å¼ï¼šæå–#æ ‡é¢˜
            title_match = re.search(r'^#\s+(.+)', content, re.MULTILINE)
            if title_match:
                title = title_match.group(1).strip()
        elif file_path.suffix == '.html':
            # HTMLæ ¼å¼ï¼šæå–<h1>æ ‡é¢˜
            title_match = re.search(r'<h1[^>]*>(.+?)</h1>', content, re.IGNORECASE)
            if title_match:
                # ç§»é™¤HTMLæ ‡ç­¾å’Œç‰¹æ®Šå­—ç¬¦
                title = re.sub(r'<[^>]+>', '', title_match.group(1))
                title = title.strip()
        
        # æå–å…³é”®è¯ï¼ˆä¼ å…¥å†…å®¹ä»¥è·å¾—æ›´å¥½çš„ç»“æœï¼‰
        keywords = extract_keywords(title or '', content)
        
        return title, keywords
    except Exception as e:
        logger.warning(f"æå–å…ƒæ•°æ®å¤±è´¥: {file_path} - {e}")
        return file_path.stem, []


def convert_markdown_to_html(md_file_path: Path) -> Optional[Path]:
    """
    å°†Markdownæ–‡ä»¶è½¬æ¢ä¸ºHTMLæ–‡ä»¶
    
    Args:
        md_file_path: Markdownæ–‡ä»¶è·¯å¾„
        
    Returns:
        Optional[Path]: è½¬æ¢åçš„HTMLæ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
    """
    # NOTE: kept for backward compatibility of API signature; new pipeline uses the
    # overloaded function signature below that writes to an ASCII-only output path.
    html_file_path = md_file_path.with_suffix('.html')
    temp_html_path = md_file_path.with_suffix('.temp.html')
    
    try:
        # æ£€æŸ¥pandocæ˜¯å¦å¯ç”¨
        try:
            subprocess.run(['pandoc', '--version'], 
                         capture_output=True, 
                         check=True)
        except (subprocess.CalledProcessError, FileNotFoundError):
            logger.error("Pandocæœªå®‰è£…æˆ–ä¸å¯ç”¨ï¼Œè¯·å…ˆå®‰è£…pandoc")
            return None
        
        # è¯»å–Markdownæ–‡ä»¶å†…å®¹
        with open(md_file_path, 'r', encoding='utf-8') as f:
            md_content = f.read()
        
        # æå–æ ‡é¢˜
        title_match = re.search(r'^#\s+(.+)', md_content, re.MULTILINE)
        title = title_match.group(1).strip() if title_match else md_file_path.stem
        
        # ä½¿ç”¨pandocè½¬æ¢Markdownåˆ°HTML
        result = subprocess.run(
            ['pandoc', '-s', str(md_file_path), '-o', str(temp_html_path)],
            capture_output=True,
            text=True,
            timeout=30  # 30ç§’è¶…æ—¶
        )
        
        if result.returncode != 0:
            logger.error(f"Pandocè½¬æ¢å¤±è´¥: {result.stderr}")
            return None
        
        # è¯»å–å¹¶æå–è½¬æ¢åçš„HTMLå†…å®¹
        with open(temp_html_path, 'r', encoding='utf-8') as f:
            temp_html_content = f.read()
        
        # æå–<body>æ ‡ç­¾å†…çš„å†…å®¹
        body_match = re.search(
            r'<body[^>]*>([\s\S]*?)</body>', 
            temp_html_content, 
            re.IGNORECASE
        )
        body_content = body_match.group(1) if body_match else temp_html_content
        
        # è¯»å–æ¨¡æ¿æ–‡ä»¶
        if not Config.TEMPLATE_FILE.exists():
            logger.error(f"æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {Config.TEMPLATE_FILE}")
            return None
            
        with open(Config.TEMPLATE_FILE, 'r', encoding='utf-8') as f:
            template_content = f.read()
        
        # æå–å…³é”®è¯
        keywords = extract_keywords(title, md_content)
        
        # ç”Ÿæˆå…ƒæ•°æ®ï¼ˆä½¿ç”¨æºmdæ–‡ä»¶è·å–æ–‡ä»¶ä¿¡æ¯ï¼Œå› ä¸ºhtmlæ–‡ä»¶è¿˜ä¸å­˜åœ¨ï¼‰
        metadata = generate_metadata_for_template(html_file_path, title, keywords, source_file=md_file_path)
        
        # æ›¿æ¢æ¨¡æ¿ä¸­çš„æ‰€æœ‰å ä½ç¬¦
        final_html_content = template_content
        for key, value in metadata.items():
            final_html_content = final_html_content.replace(f'{{{{{key}}}}}', str(value))
        
        # æ›¿æ¢å†…å®¹
        final_html_content = final_html_content.replace('{{content}}', body_content)
        
        # å†™å…¥æœ€ç»ˆçš„HTMLæ–‡ä»¶
        with open(html_file_path, 'w', encoding='utf-8') as f:
            f.write(final_html_content)
        
        logger.info(f"âœ“ è½¬æ¢å®Œæˆ: {md_file_path.name} -> {html_file_path.name}")
        return html_file_path
        
    except subprocess.TimeoutExpired:
        logger.error(f"è½¬æ¢è¶…æ—¶: {md_file_path}")
        return None
    except Exception as e:
        logger.error(f"âœ— è½¬æ¢å¤±è´¥: {md_file_path} - {e}")
        return None
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_html_path.exists():
            try:
                temp_html_path.unlink()
            except Exception as e:
                logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")


def rewrite_internal_links(html_fragment: str, current_md: Path, legacy_to_new: Dict[str, str]) -> str:
    """
    Rewrite internal links pointing to notes/*.md or notes/*.html into ASCII-only /p/<id>.html or /c/<id>/index.html.
    """
    # Resolve relative href against current markdown directory
    current_rel_dir = Path(current_md.relative_to(Config.ROOT_DIR)).parent

    def resolve_candidate(href: str) -> Optional[str]:
        # strip query/hash for mapping
        base = href.split('#', 1)[0].split('?', 1)[0]
        if not base:
            return None
        if base.startswith(("http://", "https://", "mailto:", "tel:")):
            return None
        # absolute to site root
        if base.startswith("/"):
            return base.lstrip("/")
        # relative path
        try:
            # Use PurePosix-ish behavior with Path join then normalize
            resolved = (current_rel_dir / base).as_posix()
            # normalize ./ and ../ via Path
            resolved = str(Path(resolved))
            return resolved
        except Exception:
            return None

    # Replace href="..." and href='...'
    href_re = re.compile(r'''href=(["'])([^"']+)\1''', re.IGNORECASE)

    def repl(m):
        quote = m.group(1)
        href = m.group(2)
        candidate = resolve_candidate(href)
        if not candidate:
            return m.group(0)

        # If points to .md, map .md and also .html sibling
        candidates = [candidate]
        if candidate.endswith(".md"):
            candidates.append(candidate[:-3] + ".html")
        if candidate.endswith(".html"):
            candidates.append(candidate[:-5] + ".md")

        for c in candidates:
            if c in legacy_to_new:
                return f'href={quote}/{legacy_to_new[c]}{quote}'
            if ("/" + c) in legacy_to_new:
                return f'href={quote}/{legacy_to_new["/" + c]}{quote}'

        return m.group(0)

    return href_re.sub(repl, html_fragment)


def convert_markdown_to_html(md_file_path: Path, out_html_path: Path, legacy_to_new: Dict[str, str]) -> bool:
    """
    Convert a Markdown file to an HTML page at an explicit output path (ASCII-only URL),
    using the template and rewriting internal links based on legacy_to_new mapping.
    """
    temp_html_path = md_file_path.with_suffix('.temp.html')

    try:
        # Ensure output directories exist
        out_html_path.parent.mkdir(parents=True, exist_ok=True)

        with open(md_file_path, 'r', encoding='utf-8') as f:
            md_content = f.read()
        meta, md_content_wo_fm = parse_front_matter(md_content)

        # Title
        title = meta.get('title')
        if not title:
            title_match = re.search(r'^#\s+(.+)', md_content_wo_fm, re.MULTILINE)
            title = title_match.group(1).strip() if title_match else md_file_path.stem

        body_content = ""
        if pandoc_available():
            # Write a temp md without front matter to avoid leaking slug/title into content
            temp_md_path = md_file_path.with_suffix('.nofm.temp.md')
            try:
                with open(temp_md_path, 'w', encoding='utf-8') as f:
                    f.write(md_content_wo_fm)
            except Exception:
                temp_md_path = md_file_path

            # Convert to temp HTML
            result = subprocess.run(
                ['pandoc', '-s', str(temp_md_path), '-o', str(temp_html_path)],
                capture_output=True,
                text=True,
                timeout=30
            )
            if result.returncode != 0:
                logger.error(f"Pandocè½¬æ¢å¤±è´¥: {result.stderr}")
                body_content = ""
            else:
                with open(temp_html_path, 'r', encoding='utf-8') as f:
                    temp_html_content = f.read()
                body_match = re.search(r'<body[^>]*>([\s\S]*?)</body>', temp_html_content, re.IGNORECASE)
                body_content = body_match.group(1) if body_match else temp_html_content
        else:
            # Fallback: reuse existing legacy html content if present
            legacy_html_path = md_file_path.with_suffix('.html')
            if legacy_html_path.exists():
                body_content = extract_markdown_content_from_legacy_html(legacy_html_path)
            else:
                safe = re.sub(r'&', '&amp;', md_content)
                safe = re.sub(r'<', '&lt;', safe)
                body_content = f"<h1>{title}</h1><pre>{safe}</pre>"

        # Rewrite internal links to ASCII-only urls
        body_content = rewrite_internal_links(body_content, md_file_path, legacy_to_new)

        if not Config.TEMPLATE_FILE.exists():
            logger.error(f"æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {Config.TEMPLATE_FILE}")
            return False

        with open(Config.TEMPLATE_FILE, 'r', encoding='utf-8') as f:
            template_content = f.read()

        keywords = extract_keywords(title, md_content_wo_fm)
        metadata = generate_metadata_for_template(out_html_path, title, keywords, source_file=md_file_path)

        final_html_content = template_content
        for key, value in metadata.items():
            final_html_content = final_html_content.replace(f'{{{{{key}}}}}', str(value))
        final_html_content = final_html_content.replace('{{content}}', body_content)

        with open(out_html_path, 'w', encoding='utf-8') as f:
            f.write(final_html_content)

        logger.info(f"âœ“ ç”Ÿæˆ: {md_file_path.relative_to(Config.ROOT_DIR)} -> {out_html_path.relative_to(Config.ROOT_DIR)}")
        return True
    except subprocess.TimeoutExpired:
        logger.error(f"è½¬æ¢è¶…æ—¶: {md_file_path}")
        return False
    except Exception as e:
        logger.error(f"âœ— è½¬æ¢å¤±è´¥: {md_file_path} - {e}")
        return False
    finally:
        if temp_html_path.exists():
            try:
                temp_html_path.unlink()
            except Exception as e:
                logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
        try:
            temp_md_path = md_file_path.with_suffix('.nofm.temp.md')
            if temp_md_path.exists():
                temp_md_path.unlink()
        except Exception:
            pass


def generate_directory_page(dir_node: Dict, legacy_to_new: Dict[str, str]) -> bool:
    """Generate a directory index page at /c/<id>/index.html."""
    out_path = Config.ROOT_DIR / dir_node["url"]
    out_path.parent.mkdir(parents=True, exist_ok=True)

    # Prefer existing legacy index.html content, then index.md content
    dir_abs = Config.ROOT_DIR / dir_node["path"]
    legacy_index_html = dir_abs / "index.html"
    index_md = dir_abs / "index.md"
    title = dir_node.get("name") or dir_abs.name
    description = f"{title} - {Config.SITE_NAME}"

    body_content = ""
    if legacy_index_html.exists():
        body_content = extract_markdown_content_from_legacy_html(legacy_index_html)
        body_content = rewrite_internal_links(body_content, legacy_index_html, legacy_to_new)

    if index_md.exists() and pandoc_available():
        # Convert index.md to HTML fragment
        temp_html_path = index_md.with_suffix(".temp.html")
        try:
            result = subprocess.run(
                ['pandoc', '-s', str(index_md), '-o', str(temp_html_path)],
                capture_output=True,
                text=True,
                timeout=30
            )
            if result.returncode == 0 and temp_html_path.exists():
                with open(temp_html_path, 'r', encoding='utf-8') as f:
                    temp_html_content = f.read()
                body_match = re.search(r'<body[^>]*>([\s\S]*?)</body>', temp_html_content, re.IGNORECASE)
                body_content = body_match.group(1) if body_match else temp_html_content
                body_content = rewrite_internal_links(body_content, index_md, legacy_to_new)
        except Exception:
            body_content = ""
        finally:
            if temp_html_path.exists():
                try:
                    temp_html_path.unlink()
                except Exception:
                    pass

    if not body_content:
        body_content = f"<h1>{title}</h1><p>{description}</p>"

    # Inject directory id marker for frontend rendering
    body_content = f'<div id="directory-page" data-dir-id="{dir_node["id"]}"></div>\n' + body_content

    with open(Config.TEMPLATE_FILE, 'r', encoding='utf-8') as f:
        template_content = f.read()

    metadata_source = index_md if index_md.exists() else (legacy_index_html if legacy_index_html.exists() else out_path)
    metadata = generate_metadata_for_template(out_path, title, [], source_file=metadata_source)

    final_html_content = template_content
    for key, value in metadata.items():
        final_html_content = final_html_content.replace(f'{{{{{key}}}}}', str(value))
    final_html_content = final_html_content.replace('{{content}}', body_content)

    with open(out_path, 'w', encoding='utf-8') as f:
        f.write(final_html_content)
    return True


def extract_keywords(title: str, content: str = "") -> List[str]:
    """
    æ”¹è¿›çš„å…³é”®è¯æå–ç®—æ³•
    
    Args:
        title: æ–‡ç« æ ‡é¢˜
        content: æ–‡ç« å†…å®¹ï¼ˆå¯é€‰ï¼Œç”¨äºæ›´å¥½çš„å…³é”®è¯æå–ï¼‰
        
    Returns:
        List[str]: å…³é”®è¯åˆ—è¡¨
    """
    if not title:
        return []
    
    keywords = []
    seen: Set[str] = set()
    keyword_scores: Dict[str, int] = {}
    
    # ç§»é™¤è¡¨æƒ…ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
    clean_title = re.sub(
        r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF]', 
        '', 
        title
    ).strip()
    
    if not clean_title:
        return []
    
    # 1. æå–æ ¸å¿ƒä¸»é¢˜è¯ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
    for topic in Config.CORE_TOPICS:
        # ä¸åŒºåˆ†å¤§å°å†™åŒ¹é…
        if topic.lower() in clean_title.lower():
            # ä¿ç•™åŸå§‹å¤§å°å†™
            for word in re.findall(r'\b\w+\b', clean_title):
                if word.lower() == topic.lower():
                    if word not in seen:
                        keyword_scores[word] = keyword_scores.get(word, 0) + 10
                        seen.add(word)
                    break
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨é…ç½®ä¸­çš„ç‰ˆæœ¬
                if topic not in seen:
                    keyword_scores[topic] = keyword_scores.get(topic, 0) + 10
                    seen.add(topic)
    
    # 2. æå–è‹±æ–‡æœ¯è¯­ï¼ˆä¸­ç­‰ä¼˜å…ˆçº§ï¼‰
    english_terms = re.findall(r'\b[A-Z][A-Za-z]+\b|\b[a-z]{4,}\b', clean_title)
    for term in english_terms:
        # è¿‡æ»¤åœç”¨è¯
        if term in Config.STOP_WORDS or term.lower() in Config.STOP_WORDS:
            continue
        
        if term not in seen and len(term) >= Config.MIN_KEYWORD_LENGTH:
            # æŠ€æœ¯æœ¯è¯­é€šå¸¸é¦–å­—æ¯å¤§å†™æˆ–å…¨å°å†™
            if term[0].isupper() or len(term) > 4:
                keyword_scores[term] = keyword_scores.get(term, 0) + 5
                seen.add(term)
    
    # 3. æå–ä¸­æ–‡å…³é”®æ¦‚å¿µ
    # ç§»é™¤ä¿®é¥°è¯
    processed_title = clean_title
    for modifier in Config.MODIFIER_WORDS:
        processed_title = re.sub(rf'\b{modifier}\b', '', processed_title)
    processed_title = processed_title.strip()
    
    # æŸ¥æ‰¾æŠ€æœ¯ç›¸å…³æ¨¡å¼
    patterns = [
        (r'([^ï¼Œã€‚ï¼ï¼Ÿ\s]{2,})(?:æŠ€æœ¯|æ¡†æ¶|å·¥å…·|å¹³å°|ç³»ç»Ÿ)', 1),  # æŠ€æœ¯è¯æ±‡
        (r'(?:ä½¿ç”¨|è¿è¡Œ|é…ç½®|å®‰è£…)\s*([^ï¼Œã€‚ï¼ï¼Ÿ\s]{2,})', 1),  # æ“ä½œå¯¹è±¡
        (r'([A-Z][a-z]+(?:[A-Z][a-z]+)*)', 0),  # é©¼å³°å‘½å
    ]
    
    for pattern, group in patterns:
        matches = re.findall(pattern, clean_title)
        for match in matches:
            keyword = match if isinstance(match, str) else match[group]
            keyword = keyword.strip()
            
            # è¿‡æ»¤åœç”¨è¯å’Œå·²è§å…³é”®è¯
            if (keyword and 
                keyword not in Config.STOP_WORDS and
                keyword not in Config.MODIFIER_WORDS and
                len(keyword) >= Config.MIN_KEYWORD_LENGTH and 
                keyword not in seen):
                keyword_scores[keyword] = keyword_scores.get(keyword, 0) + 3
                seen.add(keyword)
    
    # 4. ç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼ˆåŸºäºå¸¸è§åˆ†éš”ç¬¦ï¼‰
    chinese_parts = re.split(r'[ï¼Œã€‚ï¼ï¼Ÿã€\s]+', processed_title)
    for part in chinese_parts:
        # æå–çº¯ä¸­æ–‡è¯æ±‡
        chinese_words = re.findall(r'[\u4e00-\u9fff]{2,}', part)
        for word in chinese_words:
            if (word not in Config.STOP_WORDS and 
                word not in Config.MODIFIER_WORDS and
                len(word) >= Config.MIN_KEYWORD_LENGTH and 
                word not in seen):
                keyword_scores[word] = keyword_scores.get(word, 0) + 2
                seen.add(word)
    
    # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›å‰Nä¸ªå…³é”®è¯
    sorted_keywords = sorted(
        keyword_scores.items(), 
        key=lambda x: x[1], 
        reverse=True
    )
    
    keywords = [kw for kw, score in sorted_keywords[:Config.MAX_KEYWORDS_PER_POST]]
    
    # å¦‚æœå…³é”®è¯å¤ªå°‘ï¼Œæ·»åŠ æ ‡é¢˜ä¸­çš„ä¸»è¦è¯æ±‡
    if len(keywords) < 2:
        words = re.findall(r'[\u4e00-\u9fff]{2,}|[A-Za-z]{3,}', clean_title)
        for word in words:
            if (word not in Config.STOP_WORDS and 
                word not in seen and 
                len(keywords) < Config.MAX_KEYWORDS_PER_POST):
                keywords.append(word)
                seen.add(word)
    
    return keywords[:Config.MAX_KEYWORDS_PER_POST]


def generate_sitemap(blog_posts: List[Dict]) -> None:
    """
    ç”Ÿæˆsitemap.xmlæ–‡ä»¶
    
    Args:
        blog_posts: åšå®¢æ–‡ç« åˆ—è¡¨
    """
    logger.info("å¼€å§‹ç”Ÿæˆsitemap.xml...")
    
    # åˆ›å»ºXMLæ ¹å…ƒç´ 
    urlset = ET.Element('urlset')
    urlset.set('xmlns', 'http://www.sitemaps.org/schemas/sitemap/0.9')
    
    # æ·»åŠ é¦–é¡µ
    url = ET.SubElement(urlset, 'url')
    ET.SubElement(url, 'loc').text = f"{Config.SITE_URL}/"
    ET.SubElement(url, 'changefreq').text = 'daily'
    ET.SubElement(url, 'priority').text = '1.0'
    ET.SubElement(url, 'lastmod').text = datetime.now().strftime('%Y-%m-%d')
    
    # æ·»åŠ æ‰€æœ‰åšå®¢æ–‡ç« ï¼ˆASCII-only URL ä¼˜å…ˆï¼‰
    for post in blog_posts:
        url = ET.SubElement(urlset, 'url')
        rel = post.get('url') or post.get('path')
        post_url = f"{Config.SITE_URL}/{rel}"
        ET.SubElement(url, 'loc').text = post_url
        ET.SubElement(url, 'changefreq').text = 'weekly'
        ET.SubElement(url, 'priority').text = '0.8'
        
        # è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´
        file_path = Config.ROOT_DIR / rel
        if file_path.exists():
            mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
            ET.SubElement(url, 'lastmod').text = mod_time.strftime('%Y-%m-%d')
    
    # ç¾åŒ–XMLè¾“å‡º
    xml_str = minidom.parseString(ET.tostring(urlset)).toprettyxml(indent="  ")
    
    # ç§»é™¤ç©ºè¡Œ
    xml_str = '\n'.join([line for line in xml_str.split('\n') if line.strip()])
    
    # ä¿å­˜sitemap.xml
    with open(Config.SITEMAP_FILE, 'w', encoding='utf-8') as f:
        f.write(xml_str)
    
    logger.info(f"âœ… Sitemapç”Ÿæˆå®Œæˆ: {Config.SITEMAP_FILE}")


def generate_rss_feed(blog_posts: List[Dict]) -> None:
    """
    ç”ŸæˆRSS feed
    
    Args:
        blog_posts: åšå®¢æ–‡ç« åˆ—è¡¨
    """
    logger.info("å¼€å§‹ç”ŸæˆRSS feed...")
    
    # åˆ›å»ºRSSæ ¹å…ƒç´ 
    rss = ET.Element('rss')
    rss.set('version', '2.0')
    rss.set('xmlns:atom', 'http://www.w3.org/2005/Atom')
    
    channel = ET.SubElement(rss, 'channel')
    ET.SubElement(channel, 'title').text = Config.SITE_NAME
    ET.SubElement(channel, 'link').text = Config.SITE_URL
    ET.SubElement(channel, 'description').text = Config.SITE_DESCRIPTION
    ET.SubElement(channel, 'language').text = 'zh-CN'
    ET.SubElement(channel, 'lastBuildDate').text = datetime.now().strftime('%a, %d %b %Y %H:%M:%S +0000')
    
    # æ·»åŠ atom:link
    atom_link = ET.SubElement(channel, 'atom:link')
    atom_link.set('href', f"{Config.SITE_URL}/rss.xml")
    atom_link.set('rel', 'self')
    atom_link.set('type', 'application/rss+xml')
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºæ–‡ç« ï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    def post_mtime(p: Dict) -> float:
        rel = p.get('url') or p.get('path')
        fp = Config.ROOT_DIR / rel
        return fp.stat().st_mtime if fp.exists() else 0

    sorted_posts = sorted(blog_posts, key=post_mtime, reverse=True)
    
    # åªåŒ…å«æœ€è¿‘çš„20ç¯‡æ–‡ç« 
    for post in sorted_posts[:20]:
        item = ET.SubElement(channel, 'item')
        ET.SubElement(item, 'title').text = post['title']
        
        rel = post.get('url') or post.get('path')
        post_url = f"{Config.SITE_URL}/{rel}"
        ET.SubElement(item, 'link').text = post_url
        ET.SubElement(item, 'guid').text = post_url
        
        # ç”Ÿæˆæè¿°ï¼ˆåŒ…å«å…³é”®è¯ï¼‰
        if post.get('keywords'):
            description = f"å…³é”®è¯: {', '.join(post['keywords'])}"
            ET.SubElement(item, 'description').text = description
        
        # æ·»åŠ åˆ†ç±»ï¼ˆä½¿ç”¨å…³é”®è¯ï¼‰
        for keyword in post.get('keywords', []):
            ET.SubElement(item, 'category').text = keyword
        
        # æ·»åŠ å‘å¸ƒæ—¥æœŸ
        file_path = Config.ROOT_DIR / rel
        if file_path.exists():
            pub_date = datetime.fromtimestamp(file_path.stat().st_mtime)
            ET.SubElement(item, 'pubDate').text = pub_date.strftime('%a, %d %b %Y %H:%M:%S +0000')
    
    # ç¾åŒ–XMLè¾“å‡º
    xml_str = minidom.parseString(ET.tostring(rss)).toprettyxml(indent="  ")
    
    # ç§»é™¤ç©ºè¡Œ
    xml_str = '\n'.join([line for line in xml_str.split('\n') if line.strip()])
    
    # ä¿å­˜RSSæ–‡ä»¶
    with open(Config.RSS_FILE, 'w', encoding='utf-8') as f:
        f.write(xml_str)
    
    logger.info(f"âœ… RSS Feedç”Ÿæˆå®Œæˆ: {Config.RSS_FILE}")


def generate_metadata_for_template(file_path: Path, title: str, keywords: List[str], source_file: Path = None) -> Dict[str, str]:
    """
    ä¸ºæ¨¡æ¿ç”Ÿæˆå…ƒæ•°æ®
    
    Args:
        file_path: ç›®æ ‡æ–‡ä»¶è·¯å¾„
        title: æ ‡é¢˜
        keywords: å…³é”®è¯åˆ—è¡¨
        source_file: æºæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºè·å–æ–‡ä»¶ä¿¡æ¯ï¼Œå¦‚æœç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨ï¼‰
        
    Returns:
        Dict[str, str]: å…ƒæ•°æ®å­—å…¸
    """
    # ç”Ÿæˆæè¿°
    description = f"{title} - "
    if keywords:
        description += f"å…³é”®è¯: {', '.join(keywords[:3])}"
    else:
        description += Config.SITE_DESCRIPTION
    
    # è·å–æ–‡ä»¶ä¿¡æ¯ï¼ˆä¼˜å…ˆä½¿ç”¨æºæ–‡ä»¶ï¼Œå› ä¸ºç›®æ ‡æ–‡ä»¶å¯èƒ½è¿˜ä¸å­˜åœ¨ï¼‰
    stat_file = source_file if source_file and source_file.exists() else file_path
    if stat_file.exists():
        stat = stat_file.stat()
        created_date = datetime.fromtimestamp(stat.st_ctime)
        modified_date = datetime.fromtimestamp(stat.st_mtime)
    else:
        created_date = datetime.now()
        modified_date = datetime.now()
    
    # ç›¸å¯¹è·¯å¾„
    rel_path = file_path.relative_to(Config.ROOT_DIR)
    
    return {
        'title': title,
        'description': description[:160],  # é™åˆ¶æè¿°é•¿åº¦
        'keywords': ', '.join(keywords),
        'date': created_date.isoformat(),
        'modified_date': modified_date.isoformat(),
        'path': str(rel_path)
    }


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¯¼èˆªæ•°æ®è‡ªåŠ¨ç”Ÿæˆå·¥å…·')
    parser.add_argument('--no-sitemap', action='store_true', help='ä¸ç”Ÿæˆsitemap.xml')
    parser.add_argument('--no-rss', action='store_true', help='ä¸ç”ŸæˆRSS feed')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡ºæ¨¡å¼')
    parser.add_argument('--slugs-report', action='store_true', help='è¾“å‡º slug æ£€æŸ¥æŠ¥å‘Šï¼ˆç¼ºå¤±/éæ³•/é‡å¤ï¼‰å¹¶é€€å‡º')
    
    args = parser.parse_args()
    
    if args.verbose:
        logger.setLevel(logging.DEBUG)
    
    print("=== å¯¼èˆªæ•°æ®è‡ªåŠ¨ç”Ÿæˆå·¥å…· ===")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    # Slug report mode (no build)
    if args.slugs_report:
        md_files = collect_markdown_posts()
        return slug_report(md_files)
    
    # æ‰«æç›®å½•ç»“æ„
    nav_menu, blog_posts, directory_structure = scan_notes_directory()
    
    # ç”Ÿæˆå¯¼èˆªæ•°æ®
    nav_data = {
        "nav_menu": nav_menu,
        "blog_posts": blog_posts,
        "directory_structure": directory_structure,
        "generated_at": datetime.now().timestamp()
    }
    
    # ä¿å­˜ä¸ºJSONæ–‡ä»¶
    with open(Config.OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(nav_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"âœ… å¯¼èˆªæ•°æ®å·²ä¿å­˜: {Config.OUTPUT_FILE}")
    
    # ç”Ÿæˆsitemap.xml
    if not args.no_sitemap:
        try:
            generate_sitemap(blog_posts)
        except Exception as e:
            logger.error(f"âŒ Sitemapç”Ÿæˆå¤±è´¥: {e}")
    
    # ç”ŸæˆRSS feed
    if not args.no_rss:
        try:
            generate_rss_feed(blog_posts)
        except Exception as e:
            logger.error(f"âŒ RSSç”Ÿæˆå¤±è´¥: {e}")
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print(f"\n{'='*50}")
    print("ç”Ÿæˆå®Œæˆï¼")
    print(f"{'='*50}")
    print(f"ğŸ“ å¯¼èˆªèœå•æ•°é‡: {len(nav_menu)}")
    print(f"ğŸ“ åšå®¢æ–‡ç« æ•°é‡: {len(blog_posts)}")
    print(f"ğŸ—‚ï¸  ç›®å½•ç»“æ„æ•°é‡: {len(directory_structure)}")
    print(f"\nè¾“å‡ºæ–‡ä»¶:")
    print(f"  â€¢ {Config.OUTPUT_FILE}")
    if not args.no_sitemap:
        print(f"  â€¢ {Config.SITEMAP_FILE}")
    if not args.no_rss:
        print(f"  â€¢ {Config.RSS_FILE}")
    print(f"\nå®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    return 0  # æˆåŠŸè¿”å› 0


if __name__ == "__main__":
    raise SystemExit(main())
