#!/usr/bin/env python3
"""å¯¼èˆªæ•°æ®è‡ªåŠ¨ç”Ÿæˆè„šæœ¬ï¼ˆåŸºäº site_builder æ¨¡å—åŒ–ç®¡é“ï¼‰ã€‚"""

import argparse
import json
import logging
from datetime import datetime
from typing import Any, Dict

from site_builder import config
from site_builder.feeds import generate_rss_feed, generate_sitemap
from site_builder.renderers import generate_directory_page
from site_builder.scanner import collect_markdown_posts, scan_notes_structure, slug_report


logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)


def build_site(args: argparse.Namespace) -> Dict[str, Any]:
    md_files = collect_markdown_posts()
    if args.slugs_report:
        raise SystemExit(slug_report(md_files))

    scan_result = scan_notes_structure(md_files)

    dir_pages_ok = 0
    for directory in scan_result.flat_directories:
        if generate_directory_page(directory, scan_result.legacy_to_new):
            dir_pages_ok += 1
    logger.info("ç›®å½•é¡µç”Ÿæˆå®Œæˆ: %s/%s", dir_pages_ok, len(scan_result.flat_directories))

    nav_data = {
        "nav_menu": scan_result.nav_menu,
        "blog_posts": scan_result.blog_posts,
        "directory_structure": scan_result.directory_structure,
        "generated_at": datetime.now().timestamp(),
    }
    config.OUTPUT_FILE.write_text(json.dumps(nav_data, ensure_ascii=False, indent=2), encoding="utf-8")
    logger.info("âœ… å¯¼èˆªæ•°æ®å·²ä¿å­˜: %s", config.OUTPUT_FILE)

    if not args.no_sitemap:
        generate_sitemap(scan_result.blog_posts)
    if not args.no_rss:
        generate_rss_feed(scan_result.blog_posts)

    return nav_data


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="å¯¼èˆªæ•°æ®è‡ªåŠ¨ç”Ÿæˆå·¥å…·")
    parser.add_argument("--no-sitemap", action="store_true", help="ä¸ç”Ÿæˆsitemap.xml")
    parser.add_argument("--no-rss", action="store_true", help="ä¸ç”ŸæˆRSS feed")
    parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†è¾“å‡ºæ¨¡å¼")
    parser.add_argument("--slugs-report", action="store_true", help="è¾“å‡º slug æ£€æŸ¥æŠ¥å‘Šï¼ˆç¼ºå¤±/éæ³•/é‡å¤ï¼‰å¹¶é€€å‡º")
    return parser.parse_args()


def main() -> int:
    args = parse_args()
    if args.verbose:
        logger.setLevel(logging.DEBUG)

    print("=== å¯¼èˆªæ•°æ®è‡ªåŠ¨ç”Ÿæˆå·¥å…· ===")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    nav_data = build_site(args)

    print("\n" + "=" * 50)
    print("ç”Ÿæˆå®Œæˆï¼")
    print("=" * 50)
    print(f"ğŸ“ å¯¼èˆªèœå•æ•°é‡: {len(nav_data['nav_menu'])}")
    print(f"ğŸ“ åšå®¢æ–‡ç« æ•°é‡: {len(nav_data['blog_posts'])}")
    print(f"ğŸ—‚ï¸  ç›®å½•ç»“æ„æ•°é‡: {len(nav_data['directory_structure'])}")
    print("\nè¾“å‡ºæ–‡ä»¶:")
    print(f"  â€¢ {config.OUTPUT_FILE}")
    if not args.no_sitemap:
        print(f"  â€¢ {config.SITEMAP_FILE}")
    if not args.no_rss:
        print(f"  â€¢ {config.RSS_FILE}")
    print(f"\nå®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
            out.append(n)
            walk(n.get("subdirs", []))
    walk(dirs)
    return out


def scan_notes_directory() -> Tuple[List[Dict], List[Dict], List[Dict]]:
    """

    logger.info(f"æ–‡ç« é¡µç”Ÿæˆå®Œæˆ: {converted_ok}/{len(md_files)}")

    # Generate directory pages
    logger.info("=== å¼€å§‹ç”Ÿæˆç›®å½•é¡µ /c/<id>/index.html ===")
    dir_pages_ok = 0
    for d in flat_dirs:
        try:
            if generate_directory_page(d, legacy_to_new):
    logger.info(f"è½¬æ¢å®Œæˆ: {converted_count} ä¸ªæˆåŠŸ, {failed_count} ä¸ªå¤±è´¥")

    """
    ä»æ–‡ä»¶ä¸­æå–æ ‡é¢˜å’Œå…³é”®è¯
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        
    Returns:
        Tuple[Optional[str], List[str]]: æ ‡é¢˜å’Œå…³é”®è¯åˆ—è¡¨
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        title = None
        
        # æ£€æµ‹æ–‡ä»¶ç±»å‹å¹¶æå–æ ‡é¢˜
        if file_path.suffix == '.md':
            # Markdownæ ¼å¼ï¼šæå–#æ ‡é¢˜
            title_match = re.search(r'^#\s+(.+)', content, re.MULTILINE)
            if title_match:
                title = title_match.group(1).strip()
        elif file_path.suffix == '.html':
            # HTMLæ ¼å¼ï¼šæå–<h1>æ ‡é¢˜
            title_match = re.search(r'<h1[^>]*>(.+?)</h1>', content, re.IGNORECASE)
            if title_match:
                # ç§»é™¤HTMLæ ‡ç­¾å’Œç‰¹æ®Šå­—ç¬¦
                title = re.sub(r'<[^>]+>', '', title_match.group(1))
                title = title.strip()
        
        # æå–å…³é”®è¯ï¼ˆä¼ å…¥å†…å®¹ä»¥è·å¾—æ›´å¥½çš„ç»“æœï¼‰
        keywords = extract_keywords(title or '', content)
        
        return title, keywords
    except Exception as e:
        logger.warning(f"æå–å…ƒæ•°æ®å¤±è´¥: {file_path} - {e}")
        return file_path.stem, []


def convert_markdown_to_html(md_file_path: Path) -> Optional[Path]:
    """
    å°†Markdownæ–‡ä»¶è½¬æ¢ä¸ºHTMLæ–‡ä»¶
    
    Args:
        md_file_path: Markdownæ–‡ä»¶è·¯å¾„
        
    Returns:
        Optional[Path]: è½¬æ¢åçš„HTMLæ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
    """
    # NOTE: kept for backward compatibility of API signature; new pipeline uses the
    # overloaded function signature below that writes to an ASCII-only output path.
    html_file_path = md_file_path.with_suffix('.html')
    temp_html_path = md_file_path.with_suffix('.temp.html')
    
    try:
        # æ£€æŸ¥pandocæ˜¯å¦å¯ç”¨
        try:
            subprocess.run(['pandoc', '--version'], 
                         capture_output=True, 
                         check=True)
        except (subprocess.CalledProcessError, FileNotFoundError):
            logger.error("Pandocæœªå®‰è£…æˆ–ä¸å¯ç”¨ï¼Œè¯·å…ˆå®‰è£…pandoc")
            return None
        
        # è¯»å–Markdownæ–‡ä»¶å†…å®¹
        with open(md_file_path, 'r', encoding='utf-8') as f:
            md_content = f.read()
        
        # æå–æ ‡é¢˜
        title_match = re.search(r'^#\s+(.+)', md_content, re.MULTILINE)
        title = title_match.group(1).strip() if title_match else md_file_path.stem
        
        # ä½¿ç”¨pandocè½¬æ¢Markdownåˆ°HTML
        result = subprocess.run(
            ['pandoc', '-s', str(md_file_path), '-o', str(temp_html_path)],
            capture_output=True,
            text=True,
            timeout=30  # 30ç§’è¶…æ—¶
        )
        
        if result.returncode != 0:
            logger.error(f"Pandocè½¬æ¢å¤±è´¥: {result.stderr}")
            return None
        
        # è¯»å–å¹¶æå–è½¬æ¢åçš„HTMLå†…å®¹
        with open(temp_html_path, 'r', encoding='utf-8') as f:
            temp_html_content = f.read()
        
        # æå–<body>æ ‡ç­¾å†…çš„å†…å®¹
        body_match = re.search(
            r'<body[^>]*>([\s\S]*?)</body>', 
            temp_html_content, 
            re.IGNORECASE
        )
        body_content = body_match.group(1) if body_match else temp_html_content
        
        # è¯»å–æ¨¡æ¿æ–‡ä»¶
        if not Config.TEMPLATE_FILE.exists():
            logger.error(f"æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {Config.TEMPLATE_FILE}")
            return None
            
        with open(Config.TEMPLATE_FILE, 'r', encoding='utf-8') as f:
            template_content = f.read()
        
        # æå–å…³é”®è¯
        keywords = extract_keywords(title, md_content)
        
        # ç”Ÿæˆå…ƒæ•°æ®ï¼ˆä½¿ç”¨æºmdæ–‡ä»¶è·å–æ–‡ä»¶ä¿¡æ¯ï¼Œå› ä¸ºhtmlæ–‡ä»¶è¿˜ä¸å­˜åœ¨ï¼‰
        metadata = generate_metadata_for_template(html_file_path, title, keywords, source_file=md_file_path)
        
        # æ›¿æ¢æ¨¡æ¿ä¸­çš„æ‰€æœ‰å ä½ç¬¦
        final_html_content = template_content
        for key, value in metadata.items():
            final_html_content = final_html_content.replace(f'{{{{{key}}}}}', str(value))
        
        # æ›¿æ¢å†…å®¹
        final_html_content = final_html_content.replace('{{content}}', body_content)
        
        # å†™å…¥æœ€ç»ˆçš„HTMLæ–‡ä»¶
        with open(html_file_path, 'w', encoding='utf-8') as f:
            f.write(final_html_content)
        
        logger.info(f"âœ“ è½¬æ¢å®Œæˆ: {md_file_path.name} -> {html_file_path.name}")
        return html_file_path
        
    except subprocess.TimeoutExpired:
        logger.error(f"è½¬æ¢è¶…æ—¶: {md_file_path}")
        return None
    except Exception as e:
        logger.error(f"âœ— è½¬æ¢å¤±è´¥: {md_file_path} - {e}")
        return None
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_html_path.exists():
            try:
                temp_html_path.unlink()
            except Exception as e:
                logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")


def rewrite_internal_links(html_fragment: str, current_md: Path, legacy_to_new: Dict[str, str]) -> str:
    """
    Rewrite internal links pointing to notes/*.md or notes/*.html into ASCII-only /p/<id>.html or /c/<id>/index.html.
    """
    # Resolve relative href against current markdown directory
    current_rel_dir = Path(current_md.relative_to(Config.ROOT_DIR)).parent

    def resolve_candidate(href: str) -> Optional[str]:
        # strip query/hash for mapping
        base = href.split('#', 1)[0].split('?', 1)[0]
        if not base:
            return None
        if base.startswith(("http://", "https://", "mailto:", "tel:")):
            return None
        # absolute to site root
        if base.startswith("/"):
            return base.lstrip("/")
        # relative path
        try:
            # Use PurePosix-ish behavior with Path join then normalize
            resolved = (current_rel_dir / base).as_posix()
            # normalize ./ and ../ via Path
            resolved = str(Path(resolved))
            return resolved
        except Exception:
            return None

    # Replace href="..." and href='...'
    href_re = re.compile(r'''href=(["'])([^"']+)\1''', re.IGNORECASE)

    def repl(m):
        quote = m.group(1)
        href = m.group(2)
        candidate = resolve_candidate(href)
        if not candidate:
            return m.group(0)

        # If points to .md, map .md and also .html sibling
        candidates = [candidate]
        if candidate.endswith(".md"):
            candidates.append(candidate[:-3] + ".html")
        if candidate.endswith(".html"):
            candidates.append(candidate[:-5] + ".md")

        for c in candidates:
            if c in legacy_to_new:
                return f'href={quote}/{legacy_to_new[c]}{quote}'
            if ("/" + c) in legacy_to_new:
                return f'href={quote}/{legacy_to_new["/" + c]}{quote}'

        return m.group(0)

    return href_re.sub(repl, html_fragment)


def convert_markdown_to_html(md_file_path: Path, out_html_path: Path, legacy_to_new: Dict[str, str]) -> bool:
    """
    Convert a Markdown file to an HTML page at an explicit output path (ASCII-only URL),
    using the template and rewriting internal links based on legacy_to_new mapping.
    """
    temp_html_path = md_file_path.with_suffix('.temp.html')

    try:
        # Ensure output directories exist
        out_html_path.parent.mkdir(parents=True, exist_ok=True)

        with open(md_file_path, 'r', encoding='utf-8') as f:
            md_content = f.read()
        meta, md_content_wo_fm = parse_front_matter(md_content)

        # Title
        title = meta.get('title')
        if not title:
            title_match = re.search(r'^#\s+(.+)', md_content_wo_fm, re.MULTILINE)
            title = title_match.group(1).strip() if title_match else md_file_path.stem

        body_content = ""
        if pandoc_available():
            # Write a temp md without front matter to avoid leaking slug/title into content
            temp_md_path = md_file_path.with_suffix('.nofm.temp.md')
            try:
                with open(temp_md_path, 'w', encoding='utf-8') as f:
                    f.write(md_content_wo_fm)
            except Exception:
                temp_md_path = md_file_path

            # Convert to temp HTML
            result = subprocess.run(
                ['pandoc', '-s', str(temp_md_path), '-o', str(temp_html_path)],
                capture_output=True,
                text=True,
                timeout=30
            )
            if result.returncode != 0:
                logger.error(f"Pandocè½¬æ¢å¤±è´¥: {result.stderr}")
                body_content = ""
            else:
                with open(temp_html_path, 'r', encoding='utf-8') as f:
                    temp_html_content = f.read()
                body_match = re.search(r'<body[^>]*>([\s\S]*?)</body>', temp_html_content, re.IGNORECASE)
                body_content = body_match.group(1) if body_match else temp_html_content
        else:
            # Fallback: reuse existing legacy html content if present
            legacy_html_path = md_file_path.with_suffix('.html')
            if legacy_html_path.exists():
                body_content = extract_markdown_content_from_legacy_html(legacy_html_path)
            else:
                safe = re.sub(r'&', '&amp;', md_content)
                safe = re.sub(r'<', '&lt;', safe)
                body_content = f"<h1>{title}</h1><pre>{safe}</pre>"

        # Rewrite internal links to ASCII-only urls
        body_content = rewrite_internal_links(body_content, md_file_path, legacy_to_new)

        if not Config.TEMPLATE_FILE.exists():
            logger.error(f"æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {Config.TEMPLATE_FILE}")
            return False

        with open(Config.TEMPLATE_FILE, 'r', encoding='utf-8') as f:
            template_content = f.read()

        keywords = extract_keywords(title, md_content_wo_fm)
        metadata = generate_metadata_for_template(out_html_path, title, keywords, source_file=md_file_path)

        final_html_content = template_content
        for key, value in metadata.items():
            final_html_content = final_html_content.replace(f'{{{{{key}}}}}', str(value))
        final_html_content = final_html_content.replace('{{content}}', body_content)

        with open(out_html_path, 'w', encoding='utf-8') as f:
            f.write(final_html_content)

        logger.info(f"âœ“ ç”Ÿæˆ: {md_file_path.relative_to(Config.ROOT_DIR)} -> {out_html_path.relative_to(Config.ROOT_DIR)}")
        return True
    except subprocess.TimeoutExpired:
        logger.error(f"è½¬æ¢è¶…æ—¶: {md_file_path}")
        return False
    except Exception as e:
        logger.error(f"âœ— è½¬æ¢å¤±è´¥: {md_file_path} - {e}")
        return False
    finally:
        if temp_html_path.exists():
            try:
                temp_html_path.unlink()
            except Exception as e:
                logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
        try:
            temp_md_path = md_file_path.with_suffix('.nofm.temp.md')
            if temp_md_path.exists():
                temp_md_path.unlink()
        except Exception:
            pass


def generate_directory_page(dir_node: Dict, legacy_to_new: Dict[str, str]) -> bool:
    """Generate a directory index page at /c/<id>/index.html."""
    out_path = Config.ROOT_DIR / dir_node["url"]
    out_path.parent.mkdir(parents=True, exist_ok=True)

    # Prefer existing legacy index.html content, then index.md content
    dir_abs = Config.ROOT_DIR / dir_node["path"]
    legacy_index_html = dir_abs / "index.html"
    index_md = dir_abs / "index.md"
    title = dir_node.get("name") or dir_abs.name
    description = f"{title} - {Config.SITE_NAME}"

    body_content = ""
    if legacy_index_html.exists():
        body_content = extract_markdown_content_from_legacy_html(legacy_index_html)
        body_content = rewrite_internal_links(body_content, legacy_index_html, legacy_to_new)

    if index_md.exists() and pandoc_available():
        # Convert index.md to HTML fragment
        temp_html_path = index_md.with_suffix(".temp.html")
        try:
            result = subprocess.run(
                ['pandoc', '-s', str(index_md), '-o', str(temp_html_path)],
                capture_output=True,
                text=True,
                timeout=30
            )
            if result.returncode == 0 and temp_html_path.exists():
                with open(temp_html_path, 'r', encoding='utf-8') as f:
                    temp_html_content = f.read()
                body_match = re.search(r'<body[^>]*>([\s\S]*?)</body>', temp_html_content, re.IGNORECASE)
                body_content = body_match.group(1) if body_match else temp_html_content
                body_content = rewrite_internal_links(body_content, index_md, legacy_to_new)
        except Exception:
            body_content = ""
        finally:
            if temp_html_path.exists():
                try:
                    temp_html_path.unlink()
                except Exception:
                    pass

    if not body_content:
        body_content = f"<h1>{title}</h1><p>{description}</p>"

    # Inject directory id marker for frontend rendering
    body_content = f'<div id="directory-page" data-dir-id="{dir_node["id"]}"></div>\n' + body_content

    with open(Config.TEMPLATE_FILE, 'r', encoding='utf-8') as f:
        template_content = f.read()

    metadata_source = index_md if index_md.exists() else (legacy_index_html if legacy_index_html.exists() else out_path)
    metadata = generate_metadata_for_template(out_path, title, [], source_file=metadata_source)

    final_html_content = template_content
    for key, value in metadata.items():
        final_html_content = final_html_content.replace(f'{{{{{key}}}}}', str(value))
    final_html_content = final_html_content.replace('{{content}}', body_content)

    with open(out_path, 'w', encoding='utf-8') as f:
        f.write(final_html_content)
    return True


def extract_keywords(title: str, content: str = "") -> List[str]:
    """
    æ”¹è¿›çš„å…³é”®è¯æå–ç®—æ³•
    
    Args:
        title: æ–‡ç« æ ‡é¢˜
        content: æ–‡ç« å†…å®¹ï¼ˆå¯é€‰ï¼Œç”¨äºæ›´å¥½çš„å…³é”®è¯æå–ï¼‰
        
    Returns:
        List[str]: å…³é”®è¯åˆ—è¡¨
    """
    if not title:
        return []
    
    keywords = []
    seen: Set[str] = set()
    keyword_scores: Dict[str, int] = {}
    
    # ç§»é™¤è¡¨æƒ…ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
    clean_title = re.sub(
        r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF]', 
        '', 
        title
    ).strip()
    
    if not clean_title:
        return []
    
    # 1. æå–æ ¸å¿ƒä¸»é¢˜è¯ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
    for topic in Config.CORE_TOPICS:
        # ä¸åŒºåˆ†å¤§å°å†™åŒ¹é…
        if topic.lower() in clean_title.lower():
            # ä¿ç•™åŸå§‹å¤§å°å†™
            for word in re.findall(r'\b\w+\b', clean_title):
                if word.lower() == topic.lower():
                    if word not in seen:
                        keyword_scores[word] = keyword_scores.get(word, 0) + 10
                        seen.add(word)
                    break
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨é…ç½®ä¸­çš„ç‰ˆæœ¬
                if topic not in seen:
                    keyword_scores[topic] = keyword_scores.get(topic, 0) + 10
                    seen.add(topic)
    
    # 2. æå–è‹±æ–‡æœ¯è¯­ï¼ˆä¸­ç­‰ä¼˜å…ˆçº§ï¼‰
    english_terms = re.findall(r'\b[A-Z][A-Za-z]+\b|\b[a-z]{4,}\b', clean_title)
    for term in english_terms:
        # è¿‡æ»¤åœç”¨è¯
        if term in Config.STOP_WORDS or term.lower() in Config.STOP_WORDS:
            continue
        
        if term not in seen and len(term) >= Config.MIN_KEYWORD_LENGTH:
            # æŠ€æœ¯æœ¯è¯­é€šå¸¸é¦–å­—æ¯å¤§å†™æˆ–å…¨å°å†™
            if term[0].isupper() or len(term) > 4:
                keyword_scores[term] = keyword_scores.get(term, 0) + 5
                seen.add(term)
    
    # 3. æå–ä¸­æ–‡å…³é”®æ¦‚å¿µ
    # ç§»é™¤ä¿®é¥°è¯
    processed_title = clean_title
    for modifier in Config.MODIFIER_WORDS:
        processed_title = re.sub(rf'\b{modifier}\b', '', processed_title)
    processed_title = processed_title.strip()
    
    # æŸ¥æ‰¾æŠ€æœ¯ç›¸å…³æ¨¡å¼
    patterns = [
        (r'([^ï¼Œã€‚ï¼ï¼Ÿ\s]{2,})(?:æŠ€æœ¯|æ¡†æ¶|å·¥å…·|å¹³å°|ç³»ç»Ÿ)', 1),  # æŠ€æœ¯è¯æ±‡
        (r'(?:ä½¿ç”¨|è¿è¡Œ|é…ç½®|å®‰è£…)\s*([^ï¼Œã€‚ï¼ï¼Ÿ\s]{2,})', 1),  # æ“ä½œå¯¹è±¡
        (r'([A-Z][a-z]+(?:[A-Z][a-z]+)*)', 0),  # é©¼å³°å‘½å
    ]
    
    for pattern, group in patterns:
        matches = re.findall(pattern, clean_title)
        for match in matches:
            keyword = match if isinstance(match, str) else match[group]
            keyword = keyword.strip()
            
            # è¿‡æ»¤åœç”¨è¯å’Œå·²è§å…³é”®è¯
            if (keyword and 
                keyword not in Config.STOP_WORDS and
                keyword not in Config.MODIFIER_WORDS and
                len(keyword) >= Config.MIN_KEYWORD_LENGTH and 
                keyword not in seen):
                keyword_scores[keyword] = keyword_scores.get(keyword, 0) + 3
                seen.add(keyword)
    
    # 4. ç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼ˆåŸºäºå¸¸è§åˆ†éš”ç¬¦ï¼‰
    chinese_parts = re.split(r'[ï¼Œã€‚ï¼ï¼Ÿã€\s]+', processed_title)
    for part in chinese_parts:
        # æå–çº¯ä¸­æ–‡è¯æ±‡
        chinese_words = re.findall(r'[\u4e00-\u9fff]{2,}', part)
        for word in chinese_words:
            if (word not in Config.STOP_WORDS and 
                word not in Config.MODIFIER_WORDS and
                len(word) >= Config.MIN_KEYWORD_LENGTH and 
                word not in seen):
                keyword_scores[word] = keyword_scores.get(word, 0) + 2
                seen.add(word)
    
    # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›å‰Nä¸ªå…³é”®è¯
    sorted_keywords = sorted(
        keyword_scores.items(), 
        key=lambda x: x[1], 
        reverse=True
    )
    
    keywords = [kw for kw, score in sorted_keywords[:Config.MAX_KEYWORDS_PER_POST]]
    
    # å¦‚æœå…³é”®è¯å¤ªå°‘ï¼Œæ·»åŠ æ ‡é¢˜ä¸­çš„ä¸»è¦è¯æ±‡
    if len(keywords) < 2:
        words = re.findall(r'[\u4e00-\u9fff]{2,}|[A-Za-z]{3,}', clean_title)
        for word in words:
            if (word not in Config.STOP_WORDS and 
                word not in seen and 
                len(keywords) < Config.MAX_KEYWORDS_PER_POST):
                keywords.append(word)
                seen.add(word)
    
    return keywords[:Config.MAX_KEYWORDS_PER_POST]


def generate_sitemap(blog_posts: List[Dict]) -> None:
    """
    ç”Ÿæˆsitemap.xmlæ–‡ä»¶
    
    Args:
        blog_posts: åšå®¢æ–‡ç« åˆ—è¡¨
    """
    logger.info("å¼€å§‹ç”Ÿæˆsitemap.xml...")
    
    # åˆ›å»ºXMLæ ¹å…ƒç´ 
    urlset = ET.Element('urlset')
    urlset.set('xmlns', 'http://www.sitemaps.org/schemas/sitemap/0.9')
    
    # æ·»åŠ é¦–é¡µ
    url = ET.SubElement(urlset, 'url')
    ET.SubElement(url, 'loc').text = f"{Config.SITE_URL}/"
    ET.SubElement(url, 'changefreq').text = 'daily'
    ET.SubElement(url, 'priority').text = '1.0'
    ET.SubElement(url, 'lastmod').text = datetime.now().strftime('%Y-%m-%d')
    
    # æ·»åŠ æ‰€æœ‰åšå®¢æ–‡ç« ï¼ˆASCII-only URL ä¼˜å…ˆï¼‰
    for post in blog_posts:
        url = ET.SubElement(urlset, 'url')
        rel = post.get('url') or post.get('path')
        post_url = f"{Config.SITE_URL}/{rel}"
        ET.SubElement(url, 'loc').text = post_url
        ET.SubElement(url, 'changefreq').text = 'weekly'
        ET.SubElement(url, 'priority').text = '0.8'
        
        # è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´
        file_path = Config.ROOT_DIR / rel
        if file_path.exists():
            mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
            ET.SubElement(url, 'lastmod').text = mod_time.strftime('%Y-%m-%d')
    
    # ç¾åŒ–XMLè¾“å‡º
    xml_str = minidom.parseString(ET.tostring(urlset)).toprettyxml(indent="  ")
    
    # ç§»é™¤ç©ºè¡Œ
    xml_str = '\n'.join([line for line in xml_str.split('\n') if line.strip()])
    
    # ä¿å­˜sitemap.xml
    with open(Config.SITEMAP_FILE, 'w', encoding='utf-8') as f:
        f.write(xml_str)
    
    logger.info(f"âœ… Sitemapç”Ÿæˆå®Œæˆ: {Config.SITEMAP_FILE}")


def generate_rss_feed(blog_posts: List[Dict]) -> None:
    """
    ç”ŸæˆRSS feed
    
    Args:
        blog_posts: åšå®¢æ–‡ç« åˆ—è¡¨
    """
    logger.info("å¼€å§‹ç”ŸæˆRSS feed...")
    
    # åˆ›å»ºRSSæ ¹å…ƒç´ 
    rss = ET.Element('rss')
    rss.set('version', '2.0')
    rss.set('xmlns:atom', 'http://www.w3.org/2005/Atom')
    
    channel = ET.SubElement(rss, 'channel')
    ET.SubElement(channel, 'title').text = Config.SITE_NAME
    ET.SubElement(channel, 'link').text = Config.SITE_URL
    ET.SubElement(channel, 'description').text = Config.SITE_DESCRIPTION
    ET.SubElement(channel, 'language').text = 'zh-CN'
    ET.SubElement(channel, 'lastBuildDate').text = datetime.now().strftime('%a, %d %b %Y %H:%M:%S +0000')
    
    # æ·»åŠ atom:link
    atom_link = ET.SubElement(channel, 'atom:link')
    atom_link.set('href', f"{Config.SITE_URL}/rss.xml")
    atom_link.set('rel', 'self')
    atom_link.set('type', 'application/rss+xml')
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºæ–‡ç« ï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    def post_mtime(p: Dict) -> float:
        rel = p.get('url') or p.get('path')
        fp = Config.ROOT_DIR / rel
        return fp.stat().st_mtime if fp.exists() else 0

    sorted_posts = sorted(blog_posts, key=post_mtime, reverse=True)
    
    # åªåŒ…å«æœ€è¿‘çš„20ç¯‡æ–‡ç« 
    for post in sorted_posts[:20]:
        item = ET.SubElement(channel, 'item')
        ET.SubElement(item, 'title').text = post['title']
        
        rel = post.get('url') or post.get('path')
        post_url = f"{Config.SITE_URL}/{rel}"
        ET.SubElement(item, 'link').text = post_url
        ET.SubElement(item, 'guid').text = post_url
        
        # ç”Ÿæˆæè¿°ï¼ˆåŒ…å«å…³é”®è¯ï¼‰
        if post.get('keywords'):
            description = f"å…³é”®è¯: {', '.join(post['keywords'])}"
            ET.SubElement(item, 'description').text = description
        
        # æ·»åŠ åˆ†ç±»ï¼ˆä½¿ç”¨å…³é”®è¯ï¼‰
        for keyword in post.get('keywords', []):
            ET.SubElement(item, 'category').text = keyword
        
        # æ·»åŠ å‘å¸ƒæ—¥æœŸ
        file_path = Config.ROOT_DIR / rel
        if file_path.exists():
            pub_date = datetime.fromtimestamp(file_path.stat().st_mtime)
            ET.SubElement(item, 'pubDate').text = pub_date.strftime('%a, %d %b %Y %H:%M:%S +0000')
    
    # ç¾åŒ–XMLè¾“å‡º
    xml_str = minidom.parseString(ET.tostring(rss)).toprettyxml(indent="  ")
    
    # ç§»é™¤ç©ºè¡Œ
    xml_str = '\n'.join([line for line in xml_str.split('\n') if line.strip()])
    
    # ä¿å­˜RSSæ–‡ä»¶
    with open(Config.RSS_FILE, 'w', encoding='utf-8') as f:
        f.write(xml_str)
    
    logger.info(f"âœ… RSS Feedç”Ÿæˆå®Œæˆ: {Config.RSS_FILE}")


def generate_metadata_for_template(file_path: Path, title: str, keywords: List[str], source_file: Path = None) -> Dict[str, str]:
    """
    ä¸ºæ¨¡æ¿ç”Ÿæˆå…ƒæ•°æ®
    
    Args:
        file_path: ç›®æ ‡æ–‡ä»¶è·¯å¾„
        title: æ ‡é¢˜
        keywords: å…³é”®è¯åˆ—è¡¨
        source_file: æºæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºè·å–æ–‡ä»¶ä¿¡æ¯ï¼Œå¦‚æœç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨ï¼‰
        
    Returns:
        Dict[str, str]: å…ƒæ•°æ®å­—å…¸
    """
    # ç”Ÿæˆæè¿°
    description = f"{title} - "
    if keywords:
        description += f"å…³é”®è¯: {', '.join(keywords[:3])}"
    else:
        description += Config.SITE_DESCRIPTION
    
    # è·å–æ–‡ä»¶ä¿¡æ¯ï¼ˆä¼˜å…ˆä½¿ç”¨æºæ–‡ä»¶ï¼Œå› ä¸ºç›®æ ‡æ–‡ä»¶å¯èƒ½è¿˜ä¸å­˜åœ¨ï¼‰
    stat_file = source_file if source_file and source_file.exists() else file_path
    if stat_file.exists():
        stat = stat_file.stat()
        created_date = datetime.fromtimestamp(stat.st_ctime)
        modified_date = datetime.fromtimestamp(stat.st_mtime)
    else:
        created_date = datetime.now()
        modified_date = datetime.now()
    
    # ç›¸å¯¹è·¯å¾„
    rel_path = file_path.relative_to(Config.ROOT_DIR)
    
    return {
        'title': title,
        'description': description[:160],  # é™åˆ¶æè¿°é•¿åº¦
        'keywords': ', '.join(keywords),
        'date': created_date.isoformat(),
        'modified_date': modified_date.isoformat(),
        'path': str(rel_path)
    }


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¯¼èˆªæ•°æ®è‡ªåŠ¨ç”Ÿæˆå·¥å…·')
    parser.add_argument('--no-sitemap', action='store_true', help='ä¸ç”Ÿæˆsitemap.xml')
    parser.add_argument('--no-rss', action='store_true', help='ä¸ç”ŸæˆRSS feed')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡ºæ¨¡å¼')
    parser.add_argument('--slugs-report', action='store_true', help='è¾“å‡º slug æ£€æŸ¥æŠ¥å‘Šï¼ˆç¼ºå¤±/éæ³•/é‡å¤ï¼‰å¹¶é€€å‡º')
    
    args = parser.parse_args()
    
    if args.verbose:
        logger.setLevel(logging.DEBUG)
    
    print("=== å¯¼èˆªæ•°æ®è‡ªåŠ¨ç”Ÿæˆå·¥å…· ===")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    # Slug report mode (no build)
    if args.slugs_report:
        md_files = collect_markdown_posts()
        return slug_report(md_files)
    
    # æ‰«æç›®å½•ç»“æ„
    nav_menu, blog_posts, directory_structure = scan_notes_directory()
    
    # ç”Ÿæˆå¯¼èˆªæ•°æ®
    nav_data = {
        "nav_menu": nav_menu,
        "blog_posts": blog_posts,
        "directory_structure": directory_structure,
        "generated_at": datetime.now().timestamp()
    }
    
    # ä¿å­˜ä¸ºJSONæ–‡ä»¶
    with open(Config.OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(nav_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"âœ… å¯¼èˆªæ•°æ®å·²ä¿å­˜: {Config.OUTPUT_FILE}")
    
    # ç”Ÿæˆsitemap.xml
    if not args.no_sitemap:
        try:
            generate_sitemap(blog_posts)
        except Exception as e:
            logger.error(f"âŒ Sitemapç”Ÿæˆå¤±è´¥: {e}")
    
    # ç”ŸæˆRSS feed
    if not args.no_rss:
        try:
            generate_rss_feed(blog_posts)
        except Exception as e:
            logger.error(f"âŒ RSSç”Ÿæˆå¤±è´¥: {e}")
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print(f"\n{'='*50}")
    print("ç”Ÿæˆå®Œæˆï¼")
    print(f"{'='*50}")
    print(f"ğŸ“ å¯¼èˆªèœå•æ•°é‡: {len(nav_menu)}")
    print(f"ğŸ“ åšå®¢æ–‡ç« æ•°é‡: {len(blog_posts)}")
    print(f"ğŸ—‚ï¸  ç›®å½•ç»“æ„æ•°é‡: {len(directory_structure)}")
    print(f"\nè¾“å‡ºæ–‡ä»¶:")
    print(f"  â€¢ {Config.OUTPUT_FILE}")
    if not args.no_sitemap:
        print(f"  â€¢ {Config.SITEMAP_FILE}")
    if not args.no_rss:
        print(f"  â€¢ {Config.RSS_FILE}")
    print(f"\nå®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    return 0  # æˆåŠŸè¿”å› 0


if __name__ == "__main__":
    raise SystemExit(main())
